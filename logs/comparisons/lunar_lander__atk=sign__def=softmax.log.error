config = {
    "evaluation": {
        "enabled": True, 
        "number_of_episodes_before_eval": 10, 
        "number_of_epsiodes_during_eval": 10, 
        "final_eval": {
            "number_of_steps": 125000, 
            "number_of_episodes": None, 
        }, 
    }, 
    "verbose": True, 
    "number_of_processes": 10, 
    "number_of_malicious_processes": 3, 
    "expected_number_of_malicious_processes": 3, 
    "logarithmic_scale_reference": {
        0: 0.1, 
        1: 0.08, 
        2: 0.063, 
        3: 0.05, 
        4: 0.04, 
        5: 0.032, 
        6: 0.025, 
        7: 0.02, 
        8: 0.016, 
        9: 0.012, 
        10: 0.01, 
        11: 0.008, 
        12: 0.0063, 
        13: 0.005, 
        14: 0.004, 
        15: 0.0032, 
        16: 0.0025, 
        17: 0.002, 
        18: 0.0016, 
        19: 0.0012, 
        20: 0.001, 
        21: 0.0008, 
        22: 0.00063, 
        23: 0.0005, 
        24: 0.0004, 
        25: 0.00032, 
        26: 0.00025, 
        27: 0.0002, 
        28: 0.00016, 
        29: 0.00012, 
        30: 1e-05, 
        31: 8e-06, 
        32: 6.3e-06, 
        33: 5e-06, 
        34: 4e-06, 
        35: 3.2e-06, 
        36: 2.5e-06, 
        37: 2e-06, 
        38: 1.6e-06, 
        39: 1.2e-06, 
    }, 
    "defense_method": "softmax", 
    "attack_method": "sign", 
    "use_frozen_random_seed": False, 
    "random_seeds": {0: 0, }, 
    "value_trend_lookback_size": 10, 
    "env_config": {
        "permaban_threshold": 1000, 
        "is_atari": False, 
        "gradient_size": 34821, 
        "env_name": "LunarLander-v2", 
        "learning_rate": 0.0009, 
        "beta": 2e-05, 
        "t_max": 10, 
        "activation": 1, 
        "hidden_size": 128, 
        "variance_scaling_factor": 1, 
    }, 
    "early_stopping": {
        "lowerbound_for_max_recent_change": 0, 
        "min_number_of_episodes": 30, 
        "thresholds": {}, 
    }, 
    "tuning": {
        "number_of_trials": 300, 
        "phase_1": {
            "categorical_options": {"activation": {0: 0, 1: 1, 2: 2, }, }, 
            "sequential_options": {
                "t_max": {0: 3, 1: 5, 2: 10, 3: 20, 4: 30, 5: 50, }, 
                "hidden_size": {0: 16, 1: 32, 2: 64, 3: 128, }, 
                "learning_rate": {
                    0: 0.1, 
                    1: 0.08, 
                    2: 0.063, 
                    3: 0.05, 
                    4: 0.04, 
                    5: 0.032, 
                    6: 0.025, 
                    7: 0.02, 
                    8: 0.016, 
                    9: 0.012, 
                    10: 0.01, 
                    11: 0.008, 
                    12: 0.0063, 
                    13: 0.005, 
                    14: 0.004, 
                    15: 0.0032, 
                    16: 0.0025, 
                    17: 0.002, 
                    18: 0.0016, 
                    19: 0.0012, 
                    20: 0.001, 
                    21: 0.0008, 
                    22: 0.00063, 
                    23: 0.0005, 
                    24: 0.0004, 
                    25: 0.00032, 
                    26: 0.00025, 
                    27: 0.0002, 
                    28: 0.00016, 
                    29: 0.00012, 
                    30: 1e-05, 
                    31: 8e-06, 
                    32: 6.3e-06, 
                    33: 5e-06, 
                    34: 4e-06, 
                    35: 3.2e-06, 
                    36: 2.5e-06, 
                    37: 2e-06, 
                    38: 1.6e-06, 
                    39: 1.2e-06, 
                }, 
                "beta": {
                    0: 0.1, 
                    1: 0.08, 
                    2: 0.063, 
                    3: 0.05, 
                    4: 0.04, 
                    5: 0.032, 
                    6: 0.025, 
                    7: 0.02, 
                    8: 0.016, 
                    9: 0.012, 
                    10: 0.01, 
                    11: 0.008, 
                    12: 0.0063, 
                    13: 0.005, 
                    14: 0.004, 
                    15: 0.0032, 
                    16: 0.0025, 
                    17: 0.002, 
                    18: 0.0016, 
                    19: 0.0012, 
                    20: 0.001, 
                    21: 0.0008, 
                    22: 0.00063, 
                    23: 0.0005, 
                    24: 0.0004, 
                    25: 0.00032, 
                    26: 0.00025, 
                    27: 0.0002, 
                    28: 0.00016, 
                    29: 0.00012, 
                    30: 1e-05, 
                    31: 8e-06, 
                    32: 6.3e-06, 
                    33: 5e-06, 
                    34: 4e-06, 
                    35: 3.2e-06, 
                    36: 2.5e-06, 
                    37: 2e-06, 
                    38: 1.6e-06, 
                    39: 1.2e-06, 
                }, 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": {0: 1, 1: 3, 2: 5, 3: 7, 4: 9, }, 
                    "exponent": {
                        0: -1, 
                        1: -2, 
                        2: -3, 
                        3: -4, 
                        4: -5, 
                        5: -6, 
                        6: -7, 
                        7: -8, 
                    }, 
                }, 
                "beta": {
                    "base": {0: 1, 1: 3, 2: 5, 3: 7, 4: 9, }, 
                    "exponent": {
                        0: -1, 
                        1: -2, 
                        2: -3, 
                        3: -4, 
                        4: -5, 
                        5: -6, 
                        6: -7, 
                        7: -8, 
                    }, 
                }, 
            }, 
        }, 
    }, 
    "training": {"episode_count": 40000, }, 
}
args = {
    "processes": 10, 
    "env": "LunarLander-v2", 
    "seed": 1996049233, 
    "outdir": "results", 
    "t_max": 10, 
    "beta": 2e-05, 
    "profile": False, 
    "steps": 40000, 
    "max_frames": (108000, ), 
    "lr": 0.0009, 
    "demo": False, 
    "load_pretrained": False, 
    "pretrained_type": "best", 
    "load": "", 
    "log_level": 20, 
    "render": False, 
    "monitor": False, 
    "permaban_threshold": 1000, 
    "malicious": 3, 
    "mal_type": "sign", 
    "rew_scale": 1.0, 
    "hidden_size": 128, 
    "activation": 1, 
}
[starting train_a3c()]
[starting middle_training_function()]

[about to call async_.run_async()]
    [starting process0 (run_func())]
    [starting process1 (run_func())]
        [starting inner_training_loop()]
    [starting process2 (run_func())]
        [starting inner_training_loop()]
        agent0.updated = False
        agent0.updated = False
        agent0.updated = False
        agent0.updated = False
        agent1.updated = False
        agent1.updated = False
        agent0.updated = False
        agent1.updated = False
        agent1.updated = False
        agent0.updated = False
        [starting inner_training_loop()]
        agent0.updated = False
        agent0.updated = False
        agent1.updated = False
    [starting process3 (run_func())]
        agent1.updated = False
        agent1.updated = False
        agent1.updated = False
        agent0.updated = False
        agent2.updated = False
        agent2.updated = False
        [starting inner_training_loop()]
        agent2.updated = False
        agent2.updated = False
    [starting process4 (run_func())]
        agent2.updated = False
        agent2.updated = False
        agent2.updated = False
        agent2.updated = False
        agent1.updated = False
        agent2.updated = False
        agent3.updated = False
        agent3.updated = False
        agent3.updated = False
        agent3.updated = False
        agent3.updated = False
        agent3.updated = False
        agent3.updated = False
        agent3.updated = False
        [starting inner_training_loop()]
        agent3.updated = False
        agent4.updated = False
    [starting process5 (run_func())]
        agent4.updated = False
        agent4.updated = False
        agent4.updated = False
    [starting process6 (run_func())]
        [starting inner_training_loop()]
        [starting inner_training_loop()]
    [starting process8 (run_func())]
        agent4.updated = False
        agent4.updated = False
        agent4.updated = False
        agent4.updated = False
        agent4.updated = False
        agent5.updated = False
        [starting inner_training_loop()]
        agent5.updated = False
        agent5.updated = False
        agent5.updated = False
        agent5.updated = False
    [starting process7 (run_func())]
        agent5.updated = False
        agent5.updated = False
        agent6.updated = False
        agent5.updated = False
        agent6.updated = False
        agent6.updated = False
        agent6.updated = False
        agent5.updated = False
        agent6.updated = False
        agent6.updated = False
        agent8.updated = False
        [starting inner_training_loop()]
        agent6.updated = False
        agent8.updated = False
    [starting process9 (run_func())]
        agent8.updated = False
        agent8.updated = False
        agent8.updated = False
        agent8.updated = False
        agent6.updated = False
        agent6.updated = False
        agent8.updated = False
        agent8.updated = False
        agent7.updated = False
        agent8.updated = False
        [starting inner_training_loop()]
        agent7.updated = False
        agent7.updated = False
        agent7.updated = False
        agent7.updated = False
        agent7.updated = False
        agent7.updated = False
        agent7.updated = False
        agent9.updated = False
        agent7.updated = False
        agent9.updated = False
        agent9.updated = False
        agent9.updated = False
        agent9.updated = False
        agent9.updated = False
        agent9.updated = False
        agent9.updated = False
        agent9.updated = False
        agent1.updated = True
        agent.gradient.shape = (34821,)
        agent2.updated = True
        agent.gradient.shape = (34821,)
        agent3.updated = True
        agent.gradient.shape = (34821,)
        agent0.updated = True
        agent.gradient.shape = (34821,)
        agent4.updated = True
        agent.gradient.shape = (34821,)
        agent8.updated = True
        agent.gradient.shape = (34821,)
        agent6.updated = True
        agent.gradient.shape = (34821,)
        agent9.updated = True
        agent.gradient.shape = (34821,)
        agent7.updated = True
        agent.gradient.shape = (34821,)
        agent5.updated = True
        agent.gradient.shape = (34821,)
        non zero process_gradients = 9.031848597110939
        non zero process_gradients = 9.244364033198357
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926452428132448
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
            started individual_updates_ready_barrier()
            process_gradient_sum = [5.448070526123047, -42.663509368896484, 12.941123962402344, -1.2930121421813965, -9.720123291015625, 48.99091339111328, -29.65499496459961, 4.217505931854248, 49.37557601928711, -26.852994918823242]
            gradients_np = [-1.11589546e-03  1.70444567e-02 -5.14199538e-03 ...  1.99760222e+00
              4.69964385e-01  1.09936857e+01]
            all_grads.sum(axis=1) = [5.44807124376905, -42.66350985296208, 12.941121076645004, -1.2930070496654338, -9.720127967662847, 48.99091579330934, -29.65499750316792, 4.217511881272912, 49.37557143705203, -26.85299360393617]
            starting early_stopping_check()
            finished early_stopping_check()
            number_of_updates.value 0
            starting choose_action()
            process_accumulated_distance = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
            raw weights = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
            exaggerated weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
            output = [9, 2, 1]
            {"step": 0, "successfully_filtered_avg": 0.0, "successfully_filtered": 0, "malicious_log_weight": [0.0, 0.0, 0.0], "non_malicious_log_weight": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "processes": {"0": {"is_malicious": 1, "filtered": 0, "log_weight": 0.0, "weight": 1}, "1": {"is_malicious": 0, "filtered": 0, "log_weight": 0.0, "weight": 1}, "2": {"is_malicious": 0, "filtered": 0, "log_weight": 0.0, "weight": 1}, "3": {"is_malicious": 1, "filtered": 0, "log_weight": 0.0, "weight": 1}, "4": {"is_malicious": 0, "filtered": 0, "log_weight": 0.0, "weight": 1}, "5": {"is_malicious": 0, "filtered": 0, "log_weight": 0.0, "weight": 1}, "6": {"is_malicious": 0, "filtered": 0, "log_weight": 0.0, "weight": 1}, "7": {"is_malicious": 0, "filtered": 0, "log_weight": 0.0, "weight": 1}, "8": {"is_malicious": 0, "filtered": 0, "log_weight": 0.0, "weight": 1}, "9": {"is_malicious": 1, "filtered": 0, "log_weight": 0.0, "weight": 1}}, "process_temp_banned_count": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "q_vals": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
            finished choose_action(), who_to_ban=[9, 2, 1]
            finished individual_updates_ready_barrier()
        agent8.add_update()
        adding update, process.is_malicious = False
        agent6.add_update()
        adding update, process.is_malicious = False
        agent4.add_update()
        adding update, process.is_malicious = False
        agent7.add_update()
        adding update, process.is_malicious = False
        agent3.add_update()
        adding update, process.is_malicious = True
        agent5.add_update()
        adding update, process.is_malicious = False
        agent0.add_update()
        adding update, process.is_malicious = True
        agent2.updated = False
        agent8.updated = False
        agent1.updated = False
        agent0.updated = False
        agent2.updated = False
        agent1.updated = False
        agent0.updated = False
        agent8.updated = False
        agent2.updated = False
        agent0.updated = False
        agent2.updated = False
        agent0.updated = False
        agent3.updated = False
        agent2.updated = False
        agent0.updated = False
        agent3.updated = False
        agent2.updated = False
        agent0.updated = False
        agent3.updated = False
        agent2.updated = False
        agent0.updated = False
        agent3.updated = False
        agent2.updated = False
        agent0.updated = False
        agent3.updated = False
        agent8.updated = False
        agent2.updated = False
        agent0.updated = False
        agent3.updated = False
        agent8.updated = False
        agent8.updated = False
        agent7.updated = False
        agent8.updated = False
        agent5.updated = False
        agent7.updated = False
        agent8.updated = False
        agent5.updated = False
        agent7.updated = False
        agent8.updated = False
        agent5.updated = False
        agent7.updated = False
        agent8.updated = False
        agent5.updated = False
        agent7.updated = False
        agent5.updated = False
        agent7.updated = False
        agent5.updated = False
        agent7.updated = False
        agent5.updated = False
        agent7.updated = False
        agent5.updated = False
        agent7.updated = False
        agent5.updated = False
        agent4.updated = False
        agent6.updated = False
        agent4.updated = False
        agent6.updated = False
        agent0.updated = True
        agent.gradient.shape = (34821,)
        agent6.updated = False
        agent4.updated = False
        agent4.updated = False
        agent6.updated = False
        agent4.updated = False
        agent6.updated = False
        agent4.updated = False
        agent6.updated = False
        agent6.updated = False
        agent4.updated = False
        agent8.updated = True
        agent.gradient.shape = (34821,)
        agent6.updated = False
        agent4.updated = False
        agent9.updated = False
        agent9.updated = False
        agent9.updated = False
        agent9.updated = False
        agent9.updated = False
        agent1.updated = False
        agent9.updated = False
        agent9.updated = False
        agent1.updated = False
        agent9.updated = False
        agent1.updated = False
        agent1.updated = False
        agent9.updated = False
        agent6.updated = False
        agent1.updated = False
        agent3.updated = False
        agent1.updated = False
        agent3.updated = False
        agent1.updated = False
        agent3.updated = False
        agent1.updated = True
        agent4.updated = False
        agent6.updated = True
        agent.gradient.shape = (34821,)
        agent.gradient.shape = (34821,)
        agent5.updated = True
        agent.gradient.shape = (34821,)
        agent7.updated = True
        agent.gradient.shape = (34821,)
        agent2.updated = True
        agent.gradient.shape = (34821,)
        agent3.updated = True
        agent.gradient.shape = (34821,)
        agent9.updated = True
        agent.gradient.shape = (34821,)
        agent4.updated = True
        agent.gradient.shape = (34821,)
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
        non zero process_gradients = 9.926481146434623
            started individual_updates_ready_barrier()
            process_gradient_sum = [82.92644500732422, 71.26207733154297, 18.925687789916992, 57.03013610839844, -36.25714874267578, 66.05715942382812, 29.462135314941406, -2.444509506225586, 53.477447509765625, 56.21521759033203]
            gradients_np = [ 7.31978100e-03 -1.64566338e-01  5.64887114e-02 ... -4.77166116e-01
             -2.87855339e+00  1.44190397e+01]
            all_grads.sum(axis=1) = [82.92643714727598, 71.26208996114588, 18.925684307384472, 57.03013960241492, -36.257146849602016, 66.0571442795972, 29.462123463609714, -2.4445130013325524, 53.4774486560259, 56.21522948883643]
            starting early_stopping_check()
            finished early_stopping_check()
            number_of_updates.value 1
            starting reward_func()
            all_grads
                each.sum() = 0.0
                each.sum() = 0.0
                each.sum() = 0.0
                each.sum() = -142.5753631591797
                each.sum() = 0.0
                each.sum() = 0.0
                each.sum() = 0.0
                each.sum() = 0.0
                each.sum() = 0.0
                each.sum() = 0.0
                math.log(mean_process_distance) = 2.4079455380377173, is_malicious: True
                math.log(mean_process_distance) = 2.4079455380377173, is_malicious: False
                math.log(mean_process_distance) = 2.4079455380377173, is_malicious: False
                math.log(mean_process_distance) = 4.605170115373936, is_malicious: True
                math.log(mean_process_distance) = 2.4079455380377173, is_malicious: False
                math.log(mean_process_distance) = 2.4079455380377173, is_malicious: False
                math.log(mean_process_distance) = 2.4079455380377173, is_malicious: False
                math.log(mean_process_distance) = 2.4079455380377173, is_malicious: False
                math.log(mean_process_distance) = 2.4079455380377173, is_malicious: False
                math.log(mean_process_distance) = 2.4079455380377173, is_malicious: True
                accumulated_distances = [11.11111032650942, 11.11111032650942, 11.11111032650942, 99.99999293858478, 11.11111032650942, 11.11111032650942, 11.11111032650942, 11.11111032650942, 11.11111032650942, 11.11111032650942]
                error = cannot unpack non-iterable float object
                exited at individual_updates_ready_barrier.wait(): 3, error = cannot unpack non-iterable float object
        exited at individual_updates_ready_barrier.wait(): 5, error = 
None
        exited at individual_updates_ready_barrier.wait(): 4, error = 
None
None
        exited at individual_updates_ready_barrier.wait(): 0, error = 
None
        exited at individual_updates_ready_barrier.wait(): 8, error = 
None
        exited at individual_updates_ready_barrier.wait(): 7, error = 
None
        exited at individual_updates_ready_barrier.wait(): 6, error = 
None
        exited at individual_updates_ready_barrier.wait(): 2, error = 
None
        exited at individual_updates_ready_barrier.wait(): 9, error = 
None
        exited at individual_updates_ready_barrier.wait(): 1, error = 
None
[done calling async_.run_async()]
Traceback (most recent call last):
  File "/home/jeffhykin/repos/bizav/examples/atari/reproduction/a3c/comparison.py", line 79, in main
    fitness_values = [ float(train_a3c.outer_training_function(args)) for each in range(runs_for_comparison) ]
  File "/home/jeffhykin/repos/bizav/examples/atari/reproduction/a3c/comparison.py", line 79, in <listcomp>
    fitness_values = [ float(train_a3c.outer_training_function(args)) for each in range(runs_for_comparison) ]
  File "/home/jeffhykin/repos/bizav/examples/atari/reproduction/a3c/train_a3c.py", line 281, in outer_training_function
    eval_stats = experiments.eval_performance(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/evaluator.py", line 285, in eval_performance
    scores, lengths = run_evaluation_episodes(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/evaluator.py", line 90, in run_evaluation_episodes
    return _run_episodes(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/evaluator.py", line 37, in _run_episodes
    a = agent.act(obs)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 304, in act
    return self._act_eval(obs)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 377, in _act_eval
    statevar = self.batch_states([obs], self.device, self.phi)
  File "/home/jeffhykin/repos/bizav/pfrl/utils/batch_states.py", line 33, in batch_states
    collated_features = default_collate(features)
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 139, in default_collate
    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \
KeyboardInterrupt
