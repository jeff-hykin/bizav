(project):
    # a central place for filepaths
    (path_to):
        this_file: "./info.yaml"
        hyperparams_folder: "./hyp_opt"
        ac3_start: ./examples/atari/reproduction/a3c/train_a3c.py

    # git-ignored path (chooses profile)
    (local_data): ./local_data.ignore.yaml

    (profiles):
        (default):
            verbose: True
            number_of_processes: 10
            number_of_malicious_processes: 3
            attack_method: 'sign' # 'sign', 'act', or 'noise'
                                  # 'sign' being the reverse of a good agent (* 2.5)
                                  # 'act' being from a random policy (very dumb agent, but gradient in correct direction)
                                  # 'noise' being simply return noise instead of a gradient
            random_seeds: [ 0 ]
            value_trend_lookback_size: 10
            early_stopping:
                lowerbound_for_max_recent_change: 0 # any change is acceptable
                min_number_of_episodes: 30
                thresholds: {}
            
        CARTPOLE:
            env_config:
                env_name: CartPole-v1
                training_episode_count: 100_000
                learning_rate: 0.001
                beta: 0.00002
                t_max: 5
                activation: 1
                hidden_size: 64
                permaban_threshold: 500
                variance_scaling_factor: 1
        CHEETAH:
            verbose: False
            env_config:
                env_name: "HalfCheetah-v2"
                training_episode_count: 4_000 # ~40,000,000 timesteps (as sum of all agents)
                learning_rate: 0.00003
                beta: 0.0000003
                t_max: 3
                activation: 1
                hidden_size: 2
                variance_scaling_factor: 1 #FIXME: not optimized at all
                permaban_threshold: 500    #FIXME: not optimized at all
            early_stopping:
                lowerbound_for_max_recent_change: 0.5 # if changes in average episode reward are this stable, it needs to just end
                min_number_of_episodes: 10
                thresholds: 
                    # epsiode_number: min_average_episode_value
                    10:  -800
                    100: -700
                    200: -600 #  by epsiode 200, get -600 or better
                    # 1000: -500 # probably too agressive
        
        LUNAR_LANDER:
            env_config:
                env_name: "LunarLander-v2"
                training_episode_count: 100_000