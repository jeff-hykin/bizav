(project):
    # a central place for filepaths
    (path_to):
        this_file: "./info.yaml"
        hyperparams_folder: "./hyp_opt"
        ac3_start: ./examples/atari/reproduction/a3c/train_a3c.py

    # git-ignored path (chooses profile)
    (local_data): ./local_data.ignore.yaml

    (profiles):
        (default):
            verbose: True
            number_of_processes: 10
            number_of_malicious_processes: 3
            logarithmic_scale_reference: [ 0.1, 0.08, 0.063, 0.05, 0.04, 0.032, 0.025, 0.02, 0.016, 0.012, 0.01, 0.008, 0.0063, 0.005, 0.004, 0.0032, 0.0025, 0.002, 0.0016, 0.0012, 0.001, 0.0008, 0.00063, 0.0005, 0.0004, 0.00032, 0.00025, 0.0002, 0.00016, 0.00012, 0.00001, 0.000008, 0.0000063, 0.000005, 0.000004, 0.0000032, 0.0000025, 0.000002, 0.0000016, 0.0000012, ]
            attack_method: 'sign' # 'sign', 'act', or 'noise'
                                  # 'sign' being the reverse of a good agent (* 2.5)
                                  # 'act' being from a random policy (very dumb agent, but gradient in correct direction)
                                  # 'noise' being simply return noise instead of a gradient
            random_seeds: [ 0 ]
            value_trend_lookback_size: 10
            early_stopping:
                lowerbound_for_max_recent_change: 0 # any change is acceptable
                min_number_of_episodes: 30
                thresholds: {}
            
        CARTPOLE:
            env_config:
                env_name: CartPole-v1
                training_episode_count: 100_000
                learning_rate: 0.001
                beta: 0.00002
                t_max: 5
                activation: 1
                hidden_size: 64
                permaban_threshold: 500
                variance_scaling_factor: 1
        CHEETAH:
            verbose: False
            env_config:
                env_name: "HalfCheetah-v2"
                training_episode_count: 8_000 # 4,000 => ~40,000,000 timesteps (as sum of all agents)
                learning_rate: 0.0003
                beta: 0.0000001
                t_max: 0
                activation: 1
                hidden_size: 2
                variance_scaling_factor: 1 #FIXME: not optimized at all
                permaban_threshold: 500    #FIXME: not optimized at all
            
            tuning:
                phase_1:
                    categorical_options:
                        activation: [0, 1, 2]
                    sequential_options:
                        t_max: [3, 5, 10, 20, 30, 50]
                        hidden_size: [16, 32, 64, 128]
                        # learning_rate: [ 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.03, 0.04, 0.001, 0.002, 0.003, 0.004, 0.0001, 0.0002, 0.0003, 0.0004, 0.00001, 0.00002, 0.00003, 0.00004, 0.000001, 0.000002, 0.000003, 0.000004, 0.0000001, 0.0000002, 0.0000003, 0.0000004, 0.00000001, 0.00000002, 0.00000003, 0.00000004, ]
                        # beta:          [ 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.03, 0.04, 0.001, 0.002, 0.003, 0.004, 0.0001, 0.0002, 0.0003, 0.0004, 0.00001, 0.00002, 0.00003, 0.00004, 0.000001, 0.000002, 0.000003, 0.000004, 0.0000001, 0.0000002, 0.0000003, 0.0000004, 0.00000001, 0.00000002, 0.00000003, 0.00000004, ]
                    sequential_exponential_options:
                        learning_rate:
                            base: [ 1, 2, 3, 4, ]
                            exponent: [ -1, -2, -3, -4, -5, -6, -7, -8, ]
                        beta:
                            base: [ 1, 2, 3, 4, ]
                            exponent: [ -1, -2, -3, -4, -5, -6, -7, -8, ]
                phase_2:
                    sequential_options:
                        learning_rate: [ 0.0004, 0.00032, 0.00025 ]
                        variance_scaling_factor: [ 0.01, 1, 100, 1000 ]
                        permaban_threshold: [ 100, 500, 1000, 2000, 10_000 ]
            
            early_stopping:
                lowerbound_for_max_recent_change: 0.1 # if changes in average episode reward are this stable, it needs to just end
                min_number_of_episodes: 10
                thresholds: 
                    # epsiode_number: min_average_episode_value
                    10:  -1000
                    100: -900
                    200: -800 #  by epsiode 200, get -600 or better
                    # 1000: -500 # probably too agressive
        
        LUNAR_LANDER:
            env_config:
                env_name: "LunarLander-v2"
                training_episode_count: 100_000