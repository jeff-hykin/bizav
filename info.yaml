(project):
    # a central place for filepaths
    (path_to):
        this_file: "./info.yaml"
        study_counter_file: "./study_counter.json"
        hyperparams_folder: "./hyp_opt"
        ac3_start: ./examples/atari/reproduction/a3c/train_a3c.py
        study_checkpoints: ./study_checkpoints.ignore/
        comparison_log_folder: ./logs/comparisons/

    # git-ignored path (chooses profile)
    (local_data): ./local_data.ignore.yaml
    
    todo:
        - get hyperparams:
            Cheetah: no biz, by 20,000 episodes expect to have an episode score of 1,500, by 40,000 episodes expect be at 2,000
        - pin down the variance_scaling_factor, permaban_threshold
        - add back the evalutation metrics:
            - run using the 7 processes when evaluating
            - record wall time
        - run sanity check on cartpole with random noise attack and random action attack (and make sure its better than the neurips attack)
        - tests to run:
            - cheetah oracle ~ 110,000 episodes
            - cheetah sign  without ucb
            - cheetah act   without ucb
            - cheetah noise without ucb
            - cheetah sign  neurips paper
            - cheetah act   neurips paper
            - cheetah noise neurips paper
            - cheetah sign  with ucb
            - cheetah act   with ucb
            - cheetah noise with ucb 
            - lunar lander oracle 40,000 episodes
            - lunar lander sign  without ucb
            - lunar lander act   without ucb
            - lunar lander noise without ucb
            - lunar lander sign  neurips paper
            - lunar lander act   neurips paper
            - lunar lander noise neurips paper
            - lunar lander sign  with ucb
            - lunar lander act   with ucb
            - lunar lander noise with ucb 
            - cartpole oracle 21,000 episodes
            - cartpole sign  without ucb
            - cartpole act   without ucb
            - cartpole noise without ucb
            - cartpole sign  neurips paper
            - cartpole act   neurips paper
            - cartpole noise neurips paper
            - cartpole sign  with ucb
            - cartpole act   with ucb
            - cartpole noise with ucb 
            
    (profiles):
        (default):
            evaluation:
                enabled: False
                number_of_episodes_before_eval: 130
                number_of_epsiodes_during_eval: 10 # this value matches the prev paper
                final_eval:
                    # one (and only one) of [ final_number_of_steps, final_number_of_episodes ] needs to be null
                    number_of_steps: null
                    number_of_episodes: null
                
            verbose: True
            number_of_processes: 10
            number_of_malicious_processes: 3
            expected_number_of_malicious_processes: 3 # set this to 0 to disable UCB
            logarithmic_scale_reference: [ 0.1, 0.08, 0.063, 0.05, 0.04, 0.032, 0.025, 0.02, 0.016, 0.012, 0.01, 0.008, 0.0063, 0.005, 0.004, 0.0032, 0.0025, 0.002, 0.0016, 0.0012, 0.001, 0.0008, 0.00063, 0.0005, 0.0004, 0.00032, 0.00025, 0.0002, 0.00016, 0.00012, 0.00001, 0.000008, 0.0000063, 0.000005, 0.000004, 0.0000032, 0.0000025, 0.000002, 0.0000016, 0.0000012, ]
            attack_method: 'sign' # 
                                  # 'sign' being the reverse of a good agent (* 2.5)
                                  # 'act' being from a random policy (very dumb agent, but gradient in correct direction)
                                  # 'noise' being simply return noise instead of a gradient
            use_frozen_random_seed: False
            random_seeds: [ 0 ]
            value_trend_lookback_size: 10
            early_stopping:
                lowerbound_for_max_recent_change: 0 # any change is acceptable
                min_number_of_episodes: 30
                thresholds: {}
            
            tuning:
                number_of_trials: 300
                phase_1:
                    categorical_options:
                        activation: [0, 1, 2]
                    sequential_options:
                        t_max: [3, 5, 10, 20, 30, 50]
                        hidden_size: [16, 32, 64, 128]
                        # learning_rate: [ 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.03, 0.04, 0.001, 0.002, 0.003, 0.004, 0.0001, 0.0002, 0.0003, 0.0004, 0.00001, 0.00002, 0.00003, 0.00004, 0.000001, 0.000002, 0.000003, 0.000004, 0.0000001, 0.0000002, 0.0000003, 0.0000004, 0.00000001, 0.00000002, 0.00000003, 0.00000004, ]
                        # beta:          [ 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.03, 0.04, 0.001, 0.002, 0.003, 0.004, 0.0001, 0.0002, 0.0003, 0.0004, 0.00001, 0.00002, 0.00003, 0.00004, 0.000001, 0.000002, 0.000003, 0.000004, 0.0000001, 0.0000002, 0.0000003, 0.0000004, 0.00000001, 0.00000002, 0.00000003, 0.00000004, ]
                    sequential_exponential_options:
                        learning_rate:
                            base: [ 1, 3, 5, 7, 9 ]
                            exponent: [ -1, -2, -3, -4, -5, -6, -7, -8, ]
                        beta:
                            base: [ 1, 3, 5, 7, 9 ]
                            exponent: [ -1, -2, -3, -4, -5, -6, -7, -8, ]
            
        CARTPOLE:
            evaluation:
                enabled: True
                number_of_epsiodes_during_eval: 10 # this value matches the prev paper
                number_of_episodes_before_eval: 130
                final_eval:
                    number_of_steps: 125_000
            
            training:
                episode_count: 21_000
            
            env_config:
                env_name: CartPole-v1
                learning_rate: 0.001
                beta: 0.00002
                t_max: 5
                activation: 1
                hidden_size: 64
                permaban_threshold: 500
                variance_scaling_factor: 1
            
            tuning:
                number_of_trials: 300
                phase_1:
                    categorical_options:
                        activation: [0, 1, 2]
                    sequential_options:
                        t_max: [3, 5, 10, 20, 30, 50]
                        hidden_size: [16, 32, 64, 128]
                        learning_rate: [ 0.1, 0.08, 0.063, 0.05, 0.04, 0.032, 0.025, 0.02, 0.016, 0.012, 0.01, 0.008, 0.0063, 0.005, 0.004, 0.0032, 0.0025, 0.002, 0.0016, 0.0012, 0.001, 0.0008, 0.00063, 0.0005, 0.0004, 0.00032, 0.00025, 0.0002, 0.00016, 0.00012, 0.00001, 0.000008, 0.0000063, 0.000005, 0.000004, 0.0000032, 0.0000025, 0.000002, 0.0000016, 0.0000012, ]
                        beta:          [ 0.1, 0.08, 0.063, 0.05, 0.04, 0.032, 0.025, 0.02, 0.016, 0.012, 0.01, 0.008, 0.0063, 0.005, 0.004, 0.0032, 0.0025, 0.002, 0.0016, 0.0012, 0.001, 0.0008, 0.00063, 0.0005, 0.0004, 0.00032, 0.00025, 0.0002, 0.00016, 0.00012, 0.00001, 0.000008, 0.0000063, 0.000005, 0.000004, 0.0000032, 0.0000025, 0.000002, 0.0000016, 0.0000012, ]
            
        CHEETAH:
            verbose: False
            evaluation:
                enabled: False
                number_of_epsiodes_during_eval: 10 # matches the prev paper
                number_of_episodes_before_eval: 550 # (if this number is 550, and there are 10 processes => 55 epsiodes per process)
                final_eval:
                    number_of_episodes: 10 # TODO maybe increase this?
            
            training:
                episode_count: 80_000 # 4,000 => ~40,000,000 timesteps (as sum of all agents)
            
            env_config:
                env_name: "HalfCheetah-v2"
                learning_rate: 0.0003
                beta: 0.0000001
                t_max: 0
                activation: 1
                hidden_size: 2
                variance_scaling_factor: 1 #FIXME: not optimized at all
                permaban_threshold: 500    #FIXME: not optimized at all
            
            tuning:
                phase_1:
                    categorical_options:
                        activation: [0, 1, 2]
                    sequential_options:
                        t_max: [3, 5, 10, 20, 30, 50]
                        hidden_size: [16, 32, 64, 128]
                        # learning_rate: [ 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.03, 0.04, 0.001, 0.002, 0.003, 0.004, 0.0001, 0.0002, 0.0003, 0.0004, 0.00001, 0.00002, 0.00003, 0.00004, 0.000001, 0.000002, 0.000003, 0.000004, 0.0000001, 0.0000002, 0.0000003, 0.0000004, 0.00000001, 0.00000002, 0.00000003, 0.00000004, ]
                        # beta:          [ 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.03, 0.04, 0.001, 0.002, 0.003, 0.004, 0.0001, 0.0002, 0.0003, 0.0004, 0.00001, 0.00002, 0.00003, 0.00004, 0.000001, 0.000002, 0.000003, 0.000004, 0.0000001, 0.0000002, 0.0000003, 0.0000004, 0.00000001, 0.00000002, 0.00000003, 0.00000004, ]
                    sequential_exponential_options:
                        learning_rate:
                            base: [ 1, 2, 3, 4, ] # this is only accidentally different from the default [ 1, 3, 5, 7, 9 ]
                            exponent: [ -1, -2, -3, -4, -5, -6, -7, -8, ]
                        beta:
                            base: [ 1, 2, 3, 4, ]
                            exponent: [ -1, -2, -3, -4, -5, -6, -7, -8, ]
                phase_2:
                    sequential_options:
                        t_max: [3, 5,]
                        hidden_size: [64, 128]
                        variance_scaling_factor: [ 0.01, 1, 100, 1000 ]
                        permaban_threshold: [ 100, 500, 1000, 2000, 10_000 ]
                    sequential_exponential_options:
                        learning_rate:
                            base: [ 2, 3, ]
                            exponent: [ -4, -5, ]
                        beta:
                            base: [ 1, 2, 3, 4, ]
                            exponent: [ -4, -5, -6, -7, ]
            
            early_stopping:
                lowerbound_for_max_recent_change: 0.1 # if changes in average episode reward are this stable, it needs to just end
                min_number_of_episodes: 10
                thresholds: 
                    # epsiode_number: min_average_episode_value
                    10:  -700
                    # 100: -900
                    # 200: -800 #  by epsiode 200, get -600 or better
                    # 1000: -500 # probably too agressive
        
        BEST_BENIGN_CHEETAH:
            number_of_malicious_processes: 0
            evaluation:
                enabled: True
            env_config:
                env_name: "HalfCheetah-v2"
                learning_rate: 0.0002
                t_max: 5
                hidden_size: 128
                variance_scaling_factor: 100
                permaban_threshold: 1000
                beta: 0.00002
                activation: 1
        
        LUNAR_LANDER:
            env_config: # best hyperparams from Lunar_sync.json #TODO: check if they are with or without malicious actors
                env_name: "LunarLander-v2"
                activation: 1
                learning_rate: 0.0009
                beta: 0.00002
                t_max: 10
                hidden_size: 128
            training:
                episode_count: 40_000
            evaluation:
                number_of_episodes_before_eval: 130
                number_of_epsiodes_during_eval: 10 # this value matches the prev paper
                final_eval:
                    number_of_steps: 125_000 # TODO: not sure if this is a good number