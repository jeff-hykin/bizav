[32m[I 2022-11-13 13:53:52,633][0m A new study created in RDB with name: halfcheetah_sync__no_malicious_2[0m
config = {
    "evaluation": {
        "enabled": False, 
        "number_of_episodes_before_eval": 550, 
        "number_of_epsiodes_during_eval": 10, 
        "final_eval": {
            "number_of_steps": None, 
            "number_of_episodes": 10, 
        }, 
    }, 
    "verbose": False, 
    "number_of_processes": 10, 
    "number_of_malicious_processes": 3, 
    "logarithmic_scale_reference": [
        0.1, 
        0.08, 
        0.063, 
        0.05, 
        0.04, 
        0.032, 
        0.025, 
        0.02, 
        0.016, 
        0.012, 
        0.01, 
        0.008, 
        0.0063, 
        0.005, 
        0.004, 
        0.0032, 
        0.0025, 
        0.002, 
        0.0016, 
        0.0012, 
        0.001, 
        0.0008, 
        0.00063, 
        0.0005, 
        0.0004, 
        0.00032, 
        0.00025, 
        0.0002, 
        0.00016, 
        0.00012, 
        1e-05, 
        8e-06, 
        6.3e-06, 
        5e-06, 
        4e-06, 
        3.2e-06, 
        2.5e-06, 
        2e-06, 
        1.6e-06, 
        1.2e-06, 
    ], 
    "attack_method": "sign", 
    "use_frozen_random_seed": False, 
    "random_seeds": [0, ], 
    "value_trend_lookback_size": 10, 
    "early_stopping": {
        "lowerbound_for_max_recent_change": 0.1, 
        "min_number_of_episodes": 10, 
        "thresholds": {10: -700, }, 
    }, 
    "tuning": {
        "number_of_trials": 300, 
        "phase_1": {
            "categorical_options": {"activation": [0, 1, 2, ], }, 
            "sequential_options": {
                "t_max": [3, 5, 10, 20, 30, 50, ], 
                "hidden_size": [16, 32, 64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
            }, 
        }, 
        "phase_2": {
            "sequential_options": {
                "t_max": [3, 5, ], 
                "hidden_size": [64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, -6, -7, ], 
                }, 
            }, 
        }, 
    }, 
    "training": {"episode_count": 80000, }, 
    "env_config": {
        "env_name": "HalfCheetah-v2", 
        "learning_rate": 0.07, 
        "beta": 7.000000000000001e-05, 
        "t_max": 10, 
        "activation": 2, 
        "hidden_size": 16, 
        "variance_scaling_factor": 0.01, 
        "permaban_threshold": 2000, 
    }, 
}
args = {
    "processes": 10, 
    "env": "HalfCheetah-v2", 
    "seed": 760144655, 
    "outdir": "results", 
    "t_max": 10, 
    "beta": 7.000000000000001e-05, 
    "profile": False, 
    "steps": 80000, 
    "max_frames": (108000, ), 
    "lr": 0.07, 
    "demo": False, 
    "load_pretrained": False, 
    "pretrained_type": "best", 
    "load": "", 
    "log_level": 20, 
    "render": False, 
    "monitor": False, 
    "permaban_threshold": 2000, 
    "malicious": 3, 
    "mal_type": "sign", 
    "rew_scale": 1.0, 
    "hidden_size": 16, 
    "activation": 2, 
}

{"total_number_of_episodes": 20, "number_of_timesteps": 20000, "per_episode_reward": -93195947143875.72, "episode_reward_trend_value": 0.0, "biggest_recent_change": NaN},
Hit early stopping because per_episode_reward: -93195947143875.72 < -700
exited at when_all_processes_are_updated(): 8
None
None
None
None
None
exited at update_barrier.wait(): 5
exited at update_barrier.wait(): 0
exited at update_barrier.wait(): 2
exited at update_barrier.wait(): 9
exited at update_barrier.wait(): 6
exited at update_barrier.wait(): 1
None
[32m[I 2022-11-13 13:53:59,553][0m Trial 0 finished with value: -128783703212528.92 and parameters: {'activation': 2, 't_max': 2, 'hidden_size': 0, 'variance_scaling_factor': 0, 'permaban_threshold': 3, 'learning_rate_base': 3, 'learning_rate_exponent': 1, 'beta_base': 3, 'beta_exponent': 4}. Best is trial 0 with value: -128783703212528.92.[0m
config = {
    "evaluation": {
        "enabled": False, 
        "number_of_episodes_before_eval": 550, 
        "number_of_epsiodes_during_eval": 10, 
        "final_eval": {
            "number_of_steps": None, 
            "number_of_episodes": 10, 
        }, 
    }, 
    "verbose": False, 
    "number_of_processes": 10, 
    "number_of_malicious_processes": 3, 
    "logarithmic_scale_reference": [
        0.1, 
        0.08, 
        0.063, 
        0.05, 
        0.04, 
        0.032, 
        0.025, 
        0.02, 
        0.016, 
        0.012, 
        0.01, 
        0.008, 
        0.0063, 
        0.005, 
        0.004, 
        0.0032, 
        0.0025, 
        0.002, 
        0.0016, 
        0.0012, 
        0.001, 
        0.0008, 
        0.00063, 
        0.0005, 
        0.0004, 
        0.00032, 
        0.00025, 
        0.0002, 
        0.00016, 
        0.00012, 
        1e-05, 
        8e-06, 
        6.3e-06, 
        5e-06, 
        4e-06, 
        3.2e-06, 
        2.5e-06, 
        2e-06, 
        1.6e-06, 
        1.2e-06, 
    ], 
    "attack_method": "sign", 
    "use_frozen_random_seed": False, 
    "random_seeds": [0, ], 
    "value_trend_lookback_size": 10, 
    "early_stopping": {
        "lowerbound_for_max_recent_change": 0.1, 
        "min_number_of_episodes": 10, 
        "thresholds": {10: -700, }, 
    }, 
    "tuning": {
        "number_of_trials": 300, 
        "phase_1": {
            "categorical_options": {"activation": [0, 1, 2, ], }, 
            "sequential_options": {
                "t_max": [3, 5, 10, 20, 30, 50, ], 
                "hidden_size": [16, 32, 64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
            }, 
        }, 
        "phase_2": {
            "sequential_options": {
                "t_max": [3, 5, ], 
                "hidden_size": [64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, -6, -7, ], 
                }, 
            }, 
        }, 
    }, 
    "training": {"episode_count": 80000, }, 
    "env_config": {
        "env_name": "HalfCheetah-v2", 
        "learning_rate": 4.9999999999999996e-06, 
        "beta": 7e-07, 
        "t_max": 30, 
        "activation": 1, 
        "hidden_size": 16, 
        "variance_scaling_factor": 0.01, 
        "permaban_threshold": 1000, 
    }, 
}
args = {
    "processes": 10, 
    "env": "HalfCheetah-v2", 
    "seed": 1718985606, 
    "outdir": "results", 
    "t_max": 30, 
    "beta": 7e-07, 
    "profile": False, 
    "steps": 80000, 
    "max_frames": (108000, ), 
    "lr": 4.9999999999999996e-06, 
    "demo": False, 
    "load_pretrained": False, 
    "pretrained_type": "best", 
    "load": "", 
    "log_level": 20, 
    "render": False, 
    "monitor": False, 
    "permaban_threshold": 1000, 
    "malicious": 3, 
    "mal_type": "sign", 
    "rew_scale": 1.0, 
    "hidden_size": 16, 
    "activation": 1, 
}
None
None
None
exited at update_barrier.wait(): 6
exited at update_barrier.wait(): 8
exited at update_barrier.wait(): 2
None
None
None
exited at update_barrier.wait(): 1
{"total_number_of_episodes": 20, "number_of_timesteps": 20000, "per_episode_reward": -690.9, "episode_reward_trend_value": 0.0, "biggest_recent_change": NaN},
{"total_number_of_episodes": 30, "number_of_timesteps": 30000, "per_episode_reward": -703.73, "episode_reward_trend_value": -1.2828067406346577, "biggest_recent_change": NaN},
Hit early stopping because per_episode_reward: -703.7301265460271 < -700
exited at when_all_processes_are_updated(): 7
exited at update_barrier.wait(): 0
exited at update_barrier.wait(): 5
[32m[I 2022-11-13 13:54:05,065][0m Trial 1 finished with value: -704.5390541816616 and parameters: {'activation': 1, 't_max': 4, 'hidden_size': 0, 'variance_scaling_factor': 0, 'permaban_threshold': 2, 'learning_rate_base': 2, 'learning_rate_exponent': 5, 'beta_base': 3, 'beta_exponent': 6}. Best is trial 1 with value: -704.5390541816616.[0m
config = {
    "evaluation": {
        "enabled": False, 
        "number_of_episodes_before_eval": 550, 
        "number_of_epsiodes_during_eval": 10, 
        "final_eval": {
            "number_of_steps": None, 
            "number_of_episodes": 10, 
        }, 
    }, 
    "verbose": False, 
    "number_of_processes": 10, 
    "number_of_malicious_processes": 3, 
    "logarithmic_scale_reference": [
        0.1, 
        0.08, 
        0.063, 
        0.05, 
        0.04, 
        0.032, 
        0.025, 
        0.02, 
        0.016, 
        0.012, 
        0.01, 
        0.008, 
        0.0063, 
        0.005, 
        0.004, 
        0.0032, 
        0.0025, 
        0.002, 
        0.0016, 
        0.0012, 
        0.001, 
        0.0008, 
        0.00063, 
        0.0005, 
        0.0004, 
        0.00032, 
        0.00025, 
        0.0002, 
        0.00016, 
        0.00012, 
        1e-05, 
        8e-06, 
        6.3e-06, 
        5e-06, 
        4e-06, 
        3.2e-06, 
        2.5e-06, 
        2e-06, 
        1.6e-06, 
        1.2e-06, 
    ], 
    "attack_method": "sign", 
    "use_frozen_random_seed": False, 
    "random_seeds": [0, ], 
    "value_trend_lookback_size": 10, 
    "early_stopping": {
        "lowerbound_for_max_recent_change": 0.1, 
        "min_number_of_episodes": 10, 
        "thresholds": {10: -700, }, 
    }, 
    "tuning": {
        "number_of_trials": 300, 
        "phase_1": {
            "categorical_options": {"activation": [0, 1, 2, ], }, 
            "sequential_options": {
                "t_max": [3, 5, 10, 20, 30, 50, ], 
                "hidden_size": [16, 32, 64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
            }, 
        }, 
        "phase_2": {
            "sequential_options": {
                "t_max": [3, 5, ], 
                "hidden_size": [64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, -6, -7, ], 
                }, 
            }, 
        }, 
    }, 
    "training": {"episode_count": 80000, }, 
    "env_config": {
        "env_name": "HalfCheetah-v2", 
        "learning_rate": 0.005, 
        "beta": 9e-07, 
        "t_max": 20, 
        "activation": 2, 
        "hidden_size": 32, 
        "variance_scaling_factor": 0.01, 
        "permaban_threshold": 500, 
    }, 
}
args = {
    "processes": 10, 
    "env": "HalfCheetah-v2", 
    "seed": 3179616192, 
    "outdir": "results", 
    "t_max": 20, 
    "beta": 9e-07, 
    "profile": False, 
    "steps": 80000, 
    "max_frames": (108000, ), 
    "lr": 0.005, 
    "demo": False, 
    "load_pretrained": False, 
    "pretrained_type": "best", 
    "load": "", 
    "log_level": 20, 
    "render": False, 
    "monitor": False, 
    "permaban_threshold": 500, 
    "malicious": 3, 
    "mal_type": "sign", 
    "rew_scale": 1.0, 
    "hidden_size": 32, 
    "activation": 2, 
}
None
None
exited at update_barrier.wait(): 5
exited at update_barrier.wait(): 3
exited at update_barrier.wait(): 0
exited at update_barrier.wait(): 9
None
None
None
None
None
None
exited at update_barrier.wait(): 2
exited at update_barrier.wait(): 6
exited at update_barrier.wait(): 4
exited at update_barrier.wait(): 1
{"total_number_of_episodes": 20, "number_of_timesteps": 20000, "per_episode_reward": -1332.6, "episode_reward_trend_value": 0.0, "biggest_recent_change": NaN},
Hit early stopping because per_episode_reward: -1332.6003897195715 < -700
exited at when_all_processes_are_updated(): 7
[32m[I 2022-11-13 13:54:10,437][0m Trial 2 finished with value: -1460.3949249745724 and parameters: {'activation': 2, 't_max': 3, 'hidden_size': 1, 'variance_scaling_factor': 0, 'permaban_threshold': 1, 'learning_rate_base': 2, 'learning_rate_exponent': 2, 'beta_base': 4, 'beta_exponent': 6}. Best is trial 1 with value: -704.5390541816616.[0m
config = {
    "evaluation": {
        "enabled": False, 
        "number_of_episodes_before_eval": 550, 
        "number_of_epsiodes_during_eval": 10, 
        "final_eval": {
            "number_of_steps": None, 
            "number_of_episodes": 10, 
        }, 
    }, 
    "verbose": False, 
    "number_of_processes": 10, 
    "number_of_malicious_processes": 3, 
    "logarithmic_scale_reference": [
        0.1, 
        0.08, 
        0.063, 
        0.05, 
        0.04, 
        0.032, 
        0.025, 
        0.02, 
        0.016, 
        0.012, 
        0.01, 
        0.008, 
        0.0063, 
        0.005, 
        0.004, 
        0.0032, 
        0.0025, 
        0.002, 
        0.0016, 
        0.0012, 
        0.001, 
        0.0008, 
        0.00063, 
        0.0005, 
        0.0004, 
        0.00032, 
        0.00025, 
        0.0002, 
        0.00016, 
        0.00012, 
        1e-05, 
        8e-06, 
        6.3e-06, 
        5e-06, 
        4e-06, 
        3.2e-06, 
        2.5e-06, 
        2e-06, 
        1.6e-06, 
        1.2e-06, 
    ], 
    "attack_method": "sign", 
    "use_frozen_random_seed": False, 
    "random_seeds": [0, ], 
    "value_trend_lookback_size": 10, 
    "early_stopping": {
        "lowerbound_for_max_recent_change": 0.1, 
        "min_number_of_episodes": 10, 
        "thresholds": {10: -700, }, 
    }, 
    "tuning": {
        "number_of_trials": 300, 
        "phase_1": {
            "categorical_options": {"activation": [0, 1, 2, ], }, 
            "sequential_options": {
                "t_max": [3, 5, 10, 20, 30, 50, ], 
                "hidden_size": [16, 32, 64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
            }, 
        }, 
        "phase_2": {
            "sequential_options": {
                "t_max": [3, 5, ], 
                "hidden_size": [64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, -6, -7, ], 
                }, 
            }, 
        }, 
    }, 
    "training": {"episode_count": 80000, }, 
    "env_config": {
        "env_name": "HalfCheetah-v2", 
        "learning_rate": 3.0000000000000004e-08, 
        "beta": 7e-08, 
        "t_max": 30, 
        "activation": 2, 
        "hidden_size": 128, 
        "variance_scaling_factor": 1, 
        "permaban_threshold": 10000, 
    }, 
}
args = {
    "processes": 10, 
    "env": "HalfCheetah-v2", 
    "seed": 614297613, 
    "outdir": "results", 
    "t_max": 30, 
    "beta": 7e-08, 
    "profile": False, 
    "steps": 80000, 
    "max_frames": (108000, ), 
    "lr": 3.0000000000000004e-08, 
    "demo": False, 
    "load_pretrained": False, 
    "pretrained_type": "best", 
    "load": "", 
    "log_level": 20, 
    "render": False, 
    "monitor": False, 
    "permaban_threshold": 10000, 
    "malicious": 3, 
    "mal_type": "sign", 
    "rew_scale": 1.0, 
    "hidden_size": 128, 
    "activation": 2, 
}
Process Process-33:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 178, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-37:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 178, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-36:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 178, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
{"total_number_of_episodes": 20, "number_of_timesteps": 20000, "per_episode_reward": -628.41, "episode_reward_trend_value": 0.0, "biggest_recent_change": NaN},
{"total_number_of_episodes": 30, "number_of_timesteps": 30000, "per_episode_reward": -637.42, "episode_reward_trend_value": -0.9012739026739496, "biggest_recent_change": NaN},
{"total_number_of_episodes": 40, "number_of_timesteps": 40000, "per_episode_reward": -637.31, "episode_reward_trend_value": -0.44517772044816867, "biggest_recent_change": NaN},
{"total_number_of_episodes": 50, "number_of_timesteps": 50000, "per_episode_reward": -652.18, "episode_reward_trend_value": -0.792334037873934, "biggest_recent_change": NaN},
{"total_number_of_episodes": 60, "number_of_timesteps": 60000, "per_episode_reward": -644.25, "episode_reward_trend_value": -0.39609826546385707, "biggest_recent_change": NaN},
{"total_number_of_episodes": 70, "number_of_timesteps": 70000, "per_episode_reward": -645.17, "episode_reward_trend_value": -0.335302095528466, "biggest_recent_change": NaN},
{"total_number_of_episodes": 80, "number_of_timesteps": 80000, "per_episode_reward": -645.62, "episode_reward_trend_value": -0.28696067667282443, "biggest_recent_change": NaN},
{"total_number_of_episodes": 90, "number_of_timesteps": 90000, "per_episode_reward": -643.43, "episode_reward_trend_value": -0.21458483739647655, "biggest_recent_change": NaN},
{"total_number_of_episodes": 100, "number_of_timesteps": 100000, "per_episode_reward": -639.62, "episode_reward_trend_value": -0.14022822918900318, "biggest_recent_change": NaN},
{"total_number_of_episodes": 110, "number_of_timesteps": 110000, "per_episode_reward": -641.9, "episode_reward_trend_value": -0.1499436362073286, "biggest_recent_change": 14.866466727254647},
{"total_number_of_episodes": 120, "number_of_timesteps": 120000, "per_episode_reward": -638.73, "episode_reward_trend_value": -0.014621855537899997, "biggest_recent_change": 14.866466727254647},
{"total_number_of_episodes": 130, "number_of_timesteps": 130000, "per_episode_reward": -640.05, "episode_reward_trend_value": -0.030480199335405207, "biggest_recent_change": 14.866466727254647},
{"total_number_of_episodes": 140, "number_of_timesteps": 140000, "per_episode_reward": -641.23, "episode_reward_trend_value": 0.1215654939262663, "biggest_recent_change": 7.926090517663738},
{"total_number_of_episodes": 150, "number_of_timesteps": 150000, "per_episode_reward": -637.66, "episode_reward_trend_value": 0.07326113733700544, "biggest_recent_change": 3.802680282633105},
{"total_number_of_episodes": 160, "number_of_timesteps": 160000, "per_episode_reward": -640.03, "episode_reward_trend_value": 0.05708971066477235, "biggest_recent_change": 3.802680282633105},
{"total_number_of_episodes": 170, "number_of_timesteps": 170000, "per_episode_reward": -637.15, "episode_reward_trend_value": 0.094149948636468, "biggest_recent_change": 3.802680282633105},
{"total_number_of_episodes": 180, "number_of_timesteps": 180000, "per_episode_reward": -637.67, "episode_reward_trend_value": 0.0639776026758606, "biggest_recent_change": 3.802680282633105},
{"total_number_of_episodes": 190, "number_of_timesteps": 190000, "per_episode_reward": -636.6, "episode_reward_trend_value": 0.033567713646117325, "biggest_recent_change": 3.578698424630261},
{"total_number_of_episodes": 200, "number_of_timesteps": 200000, "per_episode_reward": -637.35, "episode_reward_trend_value": 0.050605798037440766, "biggest_recent_change": 3.578698424630261},
{"total_number_of_episodes": 210, "number_of_timesteps": 210000, "per_episode_reward": -637.23, "episode_reward_trend_value": 0.016727467117409058, "biggest_recent_change": 3.578698424630261},
{"total_number_of_episodes": 220, "number_of_timesteps": 220000, "per_episode_reward": -638.51, "episode_reward_trend_value": 0.017174552285776676, "biggest_recent_change": 3.578698424630261},
{"total_number_of_episodes": 230, "number_of_timesteps": 230000, "per_episode_reward": -639.75, "episode_reward_trend_value": 0.01651163180425657, "biggest_recent_change": 3.578698424630261},
{"total_number_of_episodes": 240, "number_of_timesteps": 240000, "per_episode_reward": -639.5, "episode_reward_trend_value": -0.020480053571684996, "biggest_recent_change": 2.882885593506444},
{"total_number_of_episodes": 250, "number_of_timesteps": 250000, "per_episode_reward": -640.34, "episode_reward_trend_value": -0.003389884813941535, "biggest_recent_change": 2.882885593506444},
{"total_number_of_episodes": 260, "number_of_timesteps": 260000, "per_episode_reward": -642.25, "episode_reward_trend_value": -0.05665853966354208, "biggest_recent_change": 1.911293342957606},
{"total_number_of_episodes": 270, "number_of_timesteps": 270000, "per_episode_reward": -642.98, "episode_reward_trend_value": -0.05904807301343958, "biggest_recent_change": 1.911293342957606},
{"total_number_of_episodes": 280, "number_of_timesteps": 280000, "per_episode_reward": -641.27, "episode_reward_trend_value": -0.05185204903737663, "biggest_recent_change": 1.911293342957606},
{"total_number_of_episodes": 290, "number_of_timesteps": 290000, "per_episode_reward": -641.39, "episode_reward_trend_value": -0.04496416118711043, "biggest_recent_change": 1.911293342957606},
{"total_number_of_episodes": 300, "number_of_timesteps": 300000, "per_episode_reward": -640.67, "episode_reward_trend_value": -0.038274358179209385, "biggest_recent_change": 1.911293342957606},
{"total_number_of_episodes": 310, "number_of_timesteps": 310000, "per_episode_reward": -640.95, "episode_reward_trend_value": -0.027177177266623352, "biggest_recent_change": 1.911293342957606},
{"total_number_of_episodes": 320, "number_of_timesteps": 320000, "per_episode_reward": -640.89, "episode_reward_trend_value": -0.012715364895409998, "biggest_recent_change": 1.911293342957606},
{"total_number_of_episodes": 330, "number_of_timesteps": 330000, "per_episode_reward": -641.05, "episode_reward_trend_value": -0.017268551321026532, "biggest_recent_change": 1.911293342957606},
{"total_number_of_episodes": 340, "number_of_timesteps": 340000, "per_episode_reward": -641.22, "episode_reward_trend_value": -0.009822732058518744, "biggest_recent_change": 1.911293342957606},
{"total_number_of_episodes": 350, "number_of_timesteps": 350000, "per_episode_reward": -640.73, "episode_reward_trend_value": 0.016917132467708346, "biggest_recent_change": 1.7134324278018767},
{"total_number_of_episodes": 360, "number_of_timesteps": 360000, "per_episode_reward": -641.88, "episode_reward_trend_value": 0.01224390829060111, "biggest_recent_change": 1.7134324278018767},
{"total_number_of_episodes": 370, "number_of_timesteps": 370000, "per_episode_reward": -641.22, "episode_reward_trend_value": 0.0005247180424134967, "biggest_recent_change": 1.1544573312689863},
{"total_number_of_episodes": 380, "number_of_timesteps": 380000, "per_episode_reward": -641.37, "episode_reward_trend_value": 0.00025250778042062744, "biggest_recent_change": 1.1544573312689863},
{"total_number_of_episodes": 390, "number_of_timesteps": 390000, "per_episode_reward": -641.55, "episode_reward_trend_value": -0.009689208516110487, "biggest_recent_change": 1.1544573312689863},
{"total_number_of_episodes": 400, "number_of_timesteps": 400000, "per_episode_reward": -642.05, "episode_reward_trend_value": -0.012221421013578038, "biggest_recent_change": 1.1544573312689863},
{"total_number_of_episodes": 410, "number_of_timesteps": 410000, "per_episode_reward": -642.94, "episode_reward_trend_value": -0.02272601035560658, "biggest_recent_change": 1.1544573312689863},
Process Process-40:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 178, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-32:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 178, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-38:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 178, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-35:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 167, in train_loop
    when_all_processes_are_updated()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 481, in when_all_processes_are_updated
    ucb_reward = ucb.reward_func(ucb.smart_gradient_of_agents)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 413, in smart_gradient_of_agents
    all_grads.append(np.asarray(my_grad))
KeyboardInterrupt
Process Process-34:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 178, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-39:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 178, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-31:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 178, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
[33m[W 2022-11-13 13:56:13,144][0m Trial 3 failed because of the following error: KeyboardInterrupt()[0m
Traceback (most recent call last):
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
  File "./main/0_hyp_tuning.py", line 31, in stage1_tuning
    fitness_value = float(train_a3c.train_a3c(args, trial))
  File "/home/jeffhykin/repos/bizav/examples/atari/reproduction/a3c/train_a3c.py", line 268, in train_a3c
    experiments.train_agent_async(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 628, in train_agent_async
    async_.run_async(processes, run_func)
  File "/home/jeffhykin/repos/bizav/pfrl/utils/async_.py", line 28, in run_async
    each_process.join()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Traceback (most recent call last):
  File "./main/0_hyp_tuning.py", line 122, in <module>
    start_running_trials(objective=stage1_tuning, number_of_trials=config.tuning.number_of_trials)
  File "./main/0_hyp_tuning.py", line 96, in start_running_trials
    study.optimize(objective, n_trials=number_of_trials, gc_after_trial=True)
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/study.py", line 419, in optimize
    _optimize(
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 160, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 234, in _run_trial
    raise func_err
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
  File "./main/0_hyp_tuning.py", line 31, in stage1_tuning
    fitness_value = float(train_a3c.train_a3c(args, trial))
  File "/home/jeffhykin/repos/bizav/examples/atari/reproduction/a3c/train_a3c.py", line 268, in train_a3c
    experiments.train_agent_async(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 628, in train_agent_async
    async_.run_async(processes, run_func)
  File "/home/jeffhykin/repos/bizav/pfrl/utils/async_.py", line 28, in run_async
    each_process.join()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
[32m[I 2022-11-13 13:56:29,381][0m Using an existing study with name 'halfcheetah_sync__no_malicious_2' instead of creating a new one.[0m
config = {
    "evaluation": {
        "enabled": False, 
        "number_of_episodes_before_eval": 550, 
        "number_of_epsiodes_during_eval": 10, 
        "final_eval": {
            "number_of_steps": None, 
            "number_of_episodes": 10, 
        }, 
    }, 
    "verbose": False, 
    "number_of_processes": 10, 
    "number_of_malicious_processes": 3, 
    "logarithmic_scale_reference": [
        0.1, 
        0.08, 
        0.063, 
        0.05, 
        0.04, 
        0.032, 
        0.025, 
        0.02, 
        0.016, 
        0.012, 
        0.01, 
        0.008, 
        0.0063, 
        0.005, 
        0.004, 
        0.0032, 
        0.0025, 
        0.002, 
        0.0016, 
        0.0012, 
        0.001, 
        0.0008, 
        0.00063, 
        0.0005, 
        0.0004, 
        0.00032, 
        0.00025, 
        0.0002, 
        0.00016, 
        0.00012, 
        1e-05, 
        8e-06, 
        6.3e-06, 
        5e-06, 
        4e-06, 
        3.2e-06, 
        2.5e-06, 
        2e-06, 
        1.6e-06, 
        1.2e-06, 
    ], 
    "attack_method": "sign", 
    "use_frozen_random_seed": False, 
    "random_seeds": [0, ], 
    "value_trend_lookback_size": 10, 
    "early_stopping": {
        "lowerbound_for_max_recent_change": 0.1, 
        "min_number_of_episodes": 10, 
        "thresholds": {10: -700, }, 
    }, 
    "tuning": {
        "number_of_trials": 300, 
        "phase_1": {
            "categorical_options": {"activation": [0, 1, 2, ], }, 
            "sequential_options": {
                "t_max": [3, 5, 10, 20, 30, 50, ], 
                "hidden_size": [16, 32, 64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
            }, 
        }, 
        "phase_2": {
            "sequential_options": {
                "t_max": [3, 5, ], 
                "hidden_size": [64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, -6, -7, ], 
                }, 
            }, 
        }, 
    }, 
    "training": {"episode_count": 80000, }, 
    "env_config": {
        "env_name": "HalfCheetah-v2", 
        "learning_rate": 5e-07, 
        "beta": 0.009000000000000001, 
        "t_max": 30, 
        "activation": 0, 
        "hidden_size": 64, 
        "variance_scaling_factor": 0.01, 
        "permaban_threshold": 2000, 
    }, 
}
args = {
    "processes": 10, 
    "env": "HalfCheetah-v2", 
    "seed": 1211599551, 
    "outdir": "results", 
    "t_max": 30, 
    "beta": 0.009000000000000001, 
    "profile": False, 
    "steps": 80000, 
    "max_frames": (108000, ), 
    "lr": 5e-07, 
    "demo": False, 
    "load_pretrained": False, 
    "pretrained_type": "best", 
    "load": "", 
    "log_level": 20, 
    "render": False, 
    "monitor": False, 
    "permaban_threshold": 2000, 
    "malicious": 3, 
    "mal_type": "sign", 
    "rew_scale": 1.0, 
    "hidden_size": 64, 
    "activation": 0, 
}

exited at update_barrier.wait(): 2
None
exited at update_barrier.wait(): 7
None
exited at update_barrier.wait(): 5
None
exited at update_barrier.wait(): 8
None
{"total_number_of_episodes": 20, "number_of_timesteps": 20000, "per_episode_reward": -705.51, "episode_reward_trend_value": 0.0, "biggest_recent_change": NaN},
Hit early stopping because per_episode_reward: -705.5092522066735 < -700
exited at when_all_processes_are_updated(): 9
exited at update_barrier.wait(): 1
None
exited at update_barrier.wait(): 3
None
[32m[I 2022-11-13 13:56:34,926][0m Trial 4 finished with value: -697.9788391554436 and parameters: {'activation': 0, 't_max': 4, 'hidden_size': 2, 'variance_scaling_factor': 0, 'permaban_threshold': 3, 'learning_rate_base': 2, 'learning_rate_exponent': 6, 'beta_base': 4, 'beta_exponent': 2}. Best is trial 4 with value: -697.9788391554436.[0m
config = {
    "evaluation": {
        "enabled": False, 
        "number_of_episodes_before_eval": 550, 
        "number_of_epsiodes_during_eval": 10, 
        "final_eval": {
            "number_of_steps": None, 
            "number_of_episodes": 10, 
        }, 
    }, 
    "verbose": False, 
    "number_of_processes": 10, 
    "number_of_malicious_processes": 3, 
    "logarithmic_scale_reference": [
        0.1, 
        0.08, 
        0.063, 
        0.05, 
        0.04, 
        0.032, 
        0.025, 
        0.02, 
        0.016, 
        0.012, 
        0.01, 
        0.008, 
        0.0063, 
        0.005, 
        0.004, 
        0.0032, 
        0.0025, 
        0.002, 
        0.0016, 
        0.0012, 
        0.001, 
        0.0008, 
        0.00063, 
        0.0005, 
        0.0004, 
        0.00032, 
        0.00025, 
        0.0002, 
        0.00016, 
        0.00012, 
        1e-05, 
        8e-06, 
        6.3e-06, 
        5e-06, 
        4e-06, 
        3.2e-06, 
        2.5e-06, 
        2e-06, 
        1.6e-06, 
        1.2e-06, 
    ], 
    "attack_method": "sign", 
    "use_frozen_random_seed": False, 
    "random_seeds": [0, ], 
    "value_trend_lookback_size": 10, 
    "early_stopping": {
        "lowerbound_for_max_recent_change": 0.1, 
        "min_number_of_episodes": 10, 
        "thresholds": {10: -700, }, 
    }, 
    "tuning": {
        "number_of_trials": 300, 
        "phase_1": {
            "categorical_options": {"activation": [0, 1, 2, ], }, 
            "sequential_options": {
                "t_max": [3, 5, 10, 20, 30, 50, ], 
                "hidden_size": [16, 32, 64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
            }, 
        }, 
        "phase_2": {
            "sequential_options": {
                "t_max": [3, 5, ], 
                "hidden_size": [64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, -6, -7, ], 
                }, 
            }, 
        }, 
    }, 
    "training": {"episode_count": 80000, }, 
    "env_config": {
        "env_name": "HalfCheetah-v2", 
        "learning_rate": 0.007, 
        "beta": 0.9, 
        "t_max": 10, 
        "activation": 2, 
        "hidden_size": 128, 
        "variance_scaling_factor": 0.01, 
        "permaban_threshold": 500, 
    }, 
}
args = {
    "processes": 10, 
    "env": "HalfCheetah-v2", 
    "seed": 742294678, 
    "outdir": "results", 
    "t_max": 10, 
    "beta": 0.9, 
    "profile": False, 
    "steps": 80000, 
    "max_frames": (108000, ), 
    "lr": 0.007, 
    "demo": False, 
    "load_pretrained": False, 
    "pretrained_type": "best", 
    "load": "", 
    "log_level": 20, 
    "render": False, 
    "monitor": False, 
    "permaban_threshold": 500, 
    "malicious": 3, 
    "mal_type": "sign", 
    "rew_scale": 1.0, 
    "hidden_size": 128, 
    "activation": 2, 
}
exited at update_barrier.wait(): 4
None
{"total_number_of_episodes": 20, "number_of_timesteps": 20000, "per_episode_reward": -139462.19, "episode_reward_trend_value": 0.0, "biggest_recent_change": NaN},
Hit early stopping because per_episode_reward: -139462.18928133036 < -700
exited at when_all_processes_are_updated(): 0
exited at update_barrier.wait(): 8
None
exited at update_barrier.wait(): 1
None
exited at update_barrier.wait(): 9
None
exited at update_barrier.wait(): 6
None
[32m[I 2022-11-13 13:56:54,388][0m Trial 5 finished with value: -211516.44332448332 and parameters: {'activation': 2, 't_max': 2, 'hidden_size': 3, 'variance_scaling_factor': 0, 'permaban_threshold': 1, 'learning_rate_base': 3, 'learning_rate_exponent': 2, 'beta_base': 4, 'beta_exponent': 0}. Best is trial 4 with value: -697.9788391554436.[0m
config = {
    "evaluation": {
        "enabled": False, 
        "number_of_episodes_before_eval": 550, 
        "number_of_epsiodes_during_eval": 10, 
        "final_eval": {
            "number_of_steps": None, 
            "number_of_episodes": 10, 
        }, 
    }, 
    "verbose": False, 
    "number_of_processes": 10, 
    "number_of_malicious_processes": 3, 
    "logarithmic_scale_reference": [
        0.1, 
        0.08, 
        0.063, 
        0.05, 
        0.04, 
        0.032, 
        0.025, 
        0.02, 
        0.016, 
        0.012, 
        0.01, 
        0.008, 
        0.0063, 
        0.005, 
        0.004, 
        0.0032, 
        0.0025, 
        0.002, 
        0.0016, 
        0.0012, 
        0.001, 
        0.0008, 
        0.00063, 
        0.0005, 
        0.0004, 
        0.00032, 
        0.00025, 
        0.0002, 
        0.00016, 
        0.00012, 
        1e-05, 
        8e-06, 
        6.3e-06, 
        5e-06, 
        4e-06, 
        3.2e-06, 
        2.5e-06, 
        2e-06, 
        1.6e-06, 
        1.2e-06, 
    ], 
    "attack_method": "sign", 
    "use_frozen_random_seed": False, 
    "random_seeds": [0, ], 
    "value_trend_lookback_size": 10, 
    "early_stopping": {
        "lowerbound_for_max_recent_change": 0.1, 
        "min_number_of_episodes": 10, 
        "thresholds": {10: -700, }, 
    }, 
    "tuning": {
        "number_of_trials": 300, 
        "phase_1": {
            "categorical_options": {"activation": [0, 1, 2, ], }, 
            "sequential_options": {
                "t_max": [3, 5, 10, 20, 30, 50, ], 
                "hidden_size": [16, 32, 64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
            }, 
        }, 
        "phase_2": {
            "sequential_options": {
                "t_max": [3, 5, ], 
                "hidden_size": [64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, -6, -7, ], 
                }, 
            }, 
        }, 
    }, 
    "training": {"episode_count": 80000, }, 
    "env_config": {
        "env_name": "HalfCheetah-v2", 
        "learning_rate": 5e-07, 
        "beta": 4.9999999999999996e-06, 
        "t_max": 3, 
        "activation": 1, 
        "hidden_size": 16, 
        "variance_scaling_factor": 1000, 
        "permaban_threshold": 2000, 
    }, 
}
args = {
    "processes": 10, 
    "env": "HalfCheetah-v2", 
    "seed": 2050000865, 
    "outdir": "results", 
    "t_max": 3, 
    "beta": 4.9999999999999996e-06, 
    "profile": False, 
    "steps": 80000, 
    "max_frames": (108000, ), 
    "lr": 5e-07, 
    "demo": False, 
    "load_pretrained": False, 
    "pretrained_type": "best", 
    "load": "", 
    "log_level": 20, 
    "render": False, 
    "monitor": False, 
    "permaban_threshold": 2000, 
    "malicious": 3, 
    "mal_type": "sign", 
    "rew_scale": 1.0, 
    "hidden_size": 16, 
    "activation": 1, 
}
Process Process-30:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 156, in train_loop
    agent.observe(obs, r, done, reset)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 295, in observe
    self._observe_train(obs, reward, done, reset)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 342, in _observe_train
    self.past_rewards[self.t - 1] = reward
KeyboardInterrupt
Process Process-26:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 156, in train_loop
    agent.observe(obs, r, done, reset)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 295, in observe
    self._observe_train(obs, reward, done, reset)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 351, in _observe_train
    statevar = self.batch_states([obs], self.device, self.phi)
  File "/home/jeffhykin/repos/bizav/pfrl/utils/batch_states.py", line 31, in batch_states
    features = [phi(s) for s in states]
  File "/home/jeffhykin/repos/bizav/pfrl/utils/batch_states.py", line 31, in <listcomp>
    features = [phi(s) for s in states]
  File "/home/jeffhykin/repos/bizav/examples/atari/reproduction/a3c/train_a3c.py", line 207, in phi
    return np.asarray(x, dtype=np.float32)
KeyboardInterrupt
{"total_number_of_episodes": 20, "number_of_timesteps": 20000, "per_episode_reward": -676.21, "episode_reward_trend_value": 0.0, "biggest_recent_change": NaN},
{"total_number_of_episodes": 30, "number_of_timesteps": 30000, "per_episode_reward": -659.3, "episode_reward_trend_value": 1.690954670923088, "biggest_recent_change": NaN},
{"total_number_of_episodes": 40, "number_of_timesteps": 40000, "per_episode_reward": -645.22, "episode_reward_trend_value": 1.5494767440419879, "biggest_recent_change": NaN},
{"total_number_of_episodes": 50, "number_of_timesteps": 50000, "per_episode_reward": -655.83, "episode_reward_trend_value": 0.6795040468268856, "biggest_recent_change": NaN},
{"total_number_of_episodes": 60, "number_of_timesteps": 60000, "per_episode_reward": -654.42, "episode_reward_trend_value": 0.5447836632608698, "biggest_recent_change": NaN},
{"total_number_of_episodes": 70, "number_of_timesteps": 70000, "per_episode_reward": -584.48, "episode_reward_trend_value": 1.8346893867002698, "biggest_recent_change": NaN},
{"total_number_of_episodes": 80, "number_of_timesteps": 80000, "per_episode_reward": -586.08, "episode_reward_trend_value": 1.502308253419858, "biggest_recent_change": NaN},
{"total_number_of_episodes": 90, "number_of_timesteps": 90000, "per_episode_reward": -584.25, "episode_reward_trend_value": 1.313753919453997, "biggest_recent_change": NaN},
{"total_number_of_episodes": 100, "number_of_timesteps": 100000, "per_episode_reward": -585.51, "episode_reward_trend_value": 1.1338337604316364, "biggest_recent_change": NaN},
{"total_number_of_episodes": 110, "number_of_timesteps": 110000, "per_episode_reward": -584.75, "episode_reward_trend_value": 1.0162500107622854, "biggest_recent_change": 69.9431228045787},
{"total_number_of_episodes": 120, "number_of_timesteps": 120000, "per_episode_reward": -581.21, "episode_reward_trend_value": 0.8676776885914401, "biggest_recent_change": 69.9431228045787},
{"total_number_of_episodes": 130, "number_of_timesteps": 130000, "per_episode_reward": -582.18, "episode_reward_trend_value": 0.7004543062331259, "biggest_recent_change": 69.9431228045787},
{"total_number_of_episodes": 140, "number_of_timesteps": 140000, "per_episode_reward": -581.77, "episode_reward_trend_value": 0.8228403071537489, "biggest_recent_change": 69.9431228045787},
{"total_number_of_episodes": 150, "number_of_timesteps": 150000, "per_episode_reward": -582.63, "episode_reward_trend_value": 0.797718003752929, "biggest_recent_change": 69.9431228045787},
{"total_number_of_episodes": 160, "number_of_timesteps": 160000, "per_episode_reward": -581.03, "episode_reward_trend_value": 0.038315065013039704, "biggest_recent_change": 3.5380377138548056},
{"total_number_of_episodes": 170, "number_of_timesteps": 170000, "per_episode_reward": -581.52, "episode_reward_trend_value": 0.050617777491701564, "biggest_recent_change": 3.5380377138548056},
{"total_number_of_episodes": 180, "number_of_timesteps": 180000, "per_episode_reward": -581.97, "episode_reward_trend_value": 0.02532586298871517, "biggest_recent_change": 3.5380377138548056},
{"total_number_of_episodes": 190, "number_of_timesteps": 190000, "per_episode_reward": -581.88, "episode_reward_trend_value": 0.04031815968074979, "biggest_recent_change": 3.5380377138548056},
{"total_number_of_episodes": 200, "number_of_timesteps": 200000, "per_episode_reward": -583.6, "episode_reward_trend_value": 0.012794098812880999, "biggest_recent_change": 3.5380377138548056},
{"total_number_of_episodes": 210, "number_of_timesteps": 210000, "per_episode_reward": -583.93, "episode_reward_trend_value": -0.030148792864521413, "biggest_recent_change": 1.7213653440334156},
{"total_number_of_episodes": 220, "number_of_timesteps": 220000, "per_episode_reward": -583.37, "episode_reward_trend_value": -0.013211717388493424, "biggest_recent_change": 1.7213653440334156},
{"total_number_of_episodes": 230, "number_of_timesteps": 230000, "per_episode_reward": -582.88, "episode_reward_trend_value": -0.012344632172084858, "biggest_recent_change": 1.7213653440334156},
Process Process-27:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 160, in train_loop
    all_updated_barrier.wait()  # Wait for all agents to complete rollout, then run when_all_processes_are_updated()
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-28:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 160, in train_loop
    all_updated_barrier.wait()  # Wait for all agents to complete rollout, then run when_all_processes_are_updated()
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-24:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 160, in train_loop
    all_updated_barrier.wait()  # Wait for all agents to complete rollout, then run when_all_processes_are_updated()
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-25:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 156, in train_loop
    agent.observe(obs, r, done, reset)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 295, in observe
    self._observe_train(obs, reward, done, reset)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 352, in _observe_train
    self.update(statevar)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 211, in update
    batch_entropy = batch_distrib.entropy()
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/torch/distributions/independent.py", line 95, in entropy
    entropy = self.base_dist.entropy()
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/torch/distributions/normal.py", line 88, in entropy
    return 0.5 + 0.5 * math.log(2 * math.pi) + torch.log(self.scale)
KeyboardInterrupt
Process Process-21:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 156, in train_loop
    agent.observe(obs, r, done, reset)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 295, in observe
    self._observe_train(obs, reward, done, reset)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 352, in _observe_train
    self.update(statevar)
  File "/home/jeffhykin/repos/bizav/pfrl/agents/a3c.py", line 241, in update
    self.total_loss.backward()
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Process Process-22:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 160, in train_loop
    all_updated_barrier.wait()  # Wait for all agents to complete rollout, then run when_all_processes_are_updated()
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-29:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 160, in train_loop
    all_updated_barrier.wait()  # Wait for all agents to complete rollout, then run when_all_processes_are_updated()
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
Process Process-23:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 619, in run_func
    f()
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 592, in <lambda>
    f = lambda : train_loop(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 160, in train_loop
    all_updated_barrier.wait()  # Wait for all agents to complete rollout, then run when_all_processes_are_updated()
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/usr/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
[33m[W 2022-11-13 13:58:28,169][0m Trial 6 failed because of the following error: KeyboardInterrupt()[0m
Traceback (most recent call last):
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
  File "./main/0_hyp_tuning.py", line 31, in stage1_tuning
    fitness_value = float(train_a3c.train_a3c(args, trial))
  File "/home/jeffhykin/repos/bizav/examples/atari/reproduction/a3c/train_a3c.py", line 268, in train_a3c
    experiments.train_agent_async(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 628, in train_agent_async
    async_.run_async(processes, run_func)
  File "/home/jeffhykin/repos/bizav/pfrl/utils/async_.py", line 28, in run_async
    each_process.join()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Traceback (most recent call last):
  File "./main/0_hyp_tuning.py", line 122, in <module>
    start_running_trials(objective=stage1_tuning, number_of_trials=config.tuning.number_of_trials)
  File "./main/0_hyp_tuning.py", line 96, in start_running_trials
    study.optimize(objective, n_trials=number_of_trials, gc_after_trial=True)
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/study.py", line 419, in optimize
    _optimize(
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 160, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 234, in _run_trial
    raise func_err
  File "/home/jeffhykin/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
  File "./main/0_hyp_tuning.py", line 31, in stage1_tuning
    fitness_value = float(train_a3c.train_a3c(args, trial))
  File "/home/jeffhykin/repos/bizav/examples/atari/reproduction/a3c/train_a3c.py", line 268, in train_a3c
    experiments.train_agent_async(
  File "/home/jeffhykin/repos/bizav/pfrl/experiments/train_agent_async.py", line 628, in train_agent_async
    async_.run_async(processes, run_func)
  File "/home/jeffhykin/repos/bizav/pfrl/utils/async_.py", line 28, in run_async
    each_process.join()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
[32m[I 2022-11-13 13:58:33,739][0m Using an existing study with name 'halfcheetah_sync__no_malicious_2' instead of creating a new one.[0m
config = {
    "evaluation": {
        "enabled": False, 
        "number_of_episodes_before_eval": 550, 
        "number_of_epsiodes_during_eval": 10, 
        "final_eval": {
            "number_of_steps": None, 
            "number_of_episodes": 10, 
        }, 
    }, 
    "verbose": False, 
    "number_of_processes": 10, 
    "number_of_malicious_processes": 3, 
    "logarithmic_scale_reference": [
        0.1, 
        0.08, 
        0.063, 
        0.05, 
        0.04, 
        0.032, 
        0.025, 
        0.02, 
        0.016, 
        0.012, 
        0.01, 
        0.008, 
        0.0063, 
        0.005, 
        0.004, 
        0.0032, 
        0.0025, 
        0.002, 
        0.0016, 
        0.0012, 
        0.001, 
        0.0008, 
        0.00063, 
        0.0005, 
        0.0004, 
        0.00032, 
        0.00025, 
        0.0002, 
        0.00016, 
        0.00012, 
        1e-05, 
        8e-06, 
        6.3e-06, 
        5e-06, 
        4e-06, 
        3.2e-06, 
        2.5e-06, 
        2e-06, 
        1.6e-06, 
        1.2e-06, 
    ], 
    "attack_method": "sign", 
    "use_frozen_random_seed": False, 
    "random_seeds": [0, ], 
    "value_trend_lookback_size": 10, 
    "early_stopping": {
        "lowerbound_for_max_recent_change": 0.1, 
        "min_number_of_episodes": 10, 
        "thresholds": {10: -700, }, 
    }, 
    "tuning": {
        "number_of_trials": 3000, 
        "phase_1": {
            "categorical_options": {"activation": [0, 1, 2, ], }, 
            "sequential_options": {
                "t_max": [3, 5, 10, 20, 30, 50, ], 
                "hidden_size": [16, 32, 64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-1, -2, -3, -4, -5, -6, -7, -8, ], 
                }, 
            }, 
        }, 
        "phase_2": {
            "sequential_options": {
                "t_max": [3, 5, ], 
                "hidden_size": [64, 128, ], 
                "variance_scaling_factor": [0.01, 1, 100, 1000, ], 
                "permaban_threshold": [100, 500, 1000, 2000, 10000, ], 
            }, 
            "sequential_exponential_options": {
                "learning_rate": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, ], 
                }, 
                "beta": {
                    "base": [1, 3, 5, 7, 9, ], 
                    "exponent": [-4, -5, -6, -7, ], 
                }, 
            }, 
        }, 
    }, 
    "training": {"episode_count": 80000, }, 
    "env_config": {
        "env_name": "HalfCheetah-v2", 
        "learning_rate": 0.0005, 
        "beta": 9e-06, 
        "t_max": 10, 
        "activation": 2, 
        "hidden_size": 128, 
        "variance_scaling_factor": 1, 
        "permaban_threshold": 2000, 
    }, 
}
args = {
    "processes": 10, 
    "env": "HalfCheetah-v2", 
    "seed": 3957826240, 
    "outdir": "results", 
    "t_max": 10, 
    "beta": 9e-06, 
    "profile": False, 
    "steps": 80000, 
    "max_frames": (108000, ), 
    "lr": 0.0005, 
    "demo": False, 
    "load_pretrained": False, 
    "pretrained_type": "best", 
    "load": "", 
    "log_level": 20, 
    "render": False, 
    "monitor": False, 
    "permaban_threshold": 2000, 
    "malicious": 3, 
    "mal_type": "sign", 
    "rew_scale": 1.0, 
    "hidden_size": 128, 
    "activation": 2, 
}

