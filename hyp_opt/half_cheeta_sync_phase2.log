[32m[I 2022-10-23 16:44:55,944][0m A new study created in memory with name: no-name-e90ffcc7-bf65-4489-877d-a2f31d17ef3e[0m

{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-808.38, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-791.92, "episode_reward_trend_value": 1.6455221310938442, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-782.05, "episode_reward_trend_value": 1.3165418081287839, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-782.76, "episode_reward_trend_value": 0.8538535719892215, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-786.11, "episode_reward_trend_value": 0.5567758366526107, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-796.79, "episode_reward_trend_value": 0.23166441125370058, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-799.97, "episode_reward_trend_value": 0.14021194166195225, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-798.09, "episode_reward_trend_value": 0.14690523256240826, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-791.93, "episode_reward_trend_value": 0.2055777976116957, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-788.93, "episode_reward_trend_value": 0.2161300885430099, "biggest_recent_change": 16.45522131093844 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-791.46, "episode_reward_trend_value": 0.005108944284530834, "biggest_recent_change": 10.687812903419399 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-790.78, "episode_reward_trend_value": -0.09701240176119276, "biggest_recent_change": 10.687812903419399 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-788.18, "episode_reward_trend_value": -0.06024220744434766, "biggest_recent_change": 10.687812903419399 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-784.49, "episode_reward_trend_value": 0.017944842679390956, "biggest_recent_change": 10.687812903419399 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-789.32, "episode_reward_trend_value": 0.08306786213997333, "biggest_recent_change": 6.162857529567077 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-788.03, "episode_reward_trend_value": 0.13256987151201355, "biggest_recent_change": 6.162857529567077 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-790.28, "episode_reward_trend_value": 0.08686859778525913, "biggest_recent_change": 6.162857529567077 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-792.67, "episode_reward_trend_value": -0.008240771409555236, "biggest_recent_change": 4.826741151966985 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-793.94, "episode_reward_trend_value": -0.055697126732445666, "biggest_recent_change": 4.826741151966985 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-793.86, "episode_reward_trend_value": -0.026590267392048693, "biggest_recent_change": 4.826741151966985 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-792.47, "episode_reward_trend_value": -0.01885194491149428, "biggest_recent_change": 4.826741151966985 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-792.54, "episode_reward_trend_value": -0.04843364459279655, "biggest_recent_change": 4.826741151966985 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-795.62, "episode_reward_trend_value": -0.12359866238679791, "biggest_recent_change": 4.826741151966985 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-795.70, "episode_reward_trend_value": -0.07093848652599868, "biggest_recent_change": 3.072590783895862 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-798.28, "episode_reward_trend_value": -0.1138256532378111, "biggest_recent_change": 3.072590783895862 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-799.04, "episode_reward_trend_value": -0.09738142441099171, "biggest_recent_change": 3.072590783895862 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-798.07, "episode_reward_trend_value": -0.05992048376628595, "biggest_recent_change": 3.072590783895862 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-797.65, "episode_reward_trend_value": -0.04124346327982064, "biggest_recent_change": 3.072590783895862 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-796.42, "episode_reward_trend_value": -0.028482065633937105, "biggest_recent_change": 3.072590783895862 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-797.45, "episode_reward_trend_value": -0.05526639826086668, "biggest_recent_change": 3.072590783895862 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-796.42, "episode_reward_trend_value": -0.04305622911510783, "biggest_recent_change": 3.072590783895862 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-795.56, "episode_reward_trend_value": 0.0005648370784241605, "biggest_recent_change": 2.5751682235473936 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-793.32, "episode_reward_trend_value": 0.026517371020872815, "biggest_recent_change": 2.5751682235473936 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-793.13, "episode_reward_trend_value": 0.05718582703345495, "biggest_recent_change": 2.2484027303253242 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-794.21, "episode_reward_trend_value": 0.0536417619651666, "biggest_recent_change": 2.2484027303253242 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-793.05, "episode_reward_trend_value": 0.055717623765550194, "biggest_recent_change": 2.2484027303253242 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-791.79, "episode_reward_trend_value": 0.06516900466681995, "biggest_recent_change": 2.2484027303253242 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-791.88, "episode_reward_trend_value": 0.05044180508613686, "biggest_recent_change": 2.2484027303253242 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-791.50, "episode_reward_trend_value": 0.06610641552433916, "biggest_recent_change": 2.2484027303253242 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-791.89, "episode_reward_trend_value": 0.05028499914537987, "biggest_recent_change": 2.2484027303253242 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-792.46, "episode_reward_trend_value": 0.0345248800583704, "biggest_recent_change": 2.2484027303253242 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-791.67, "episode_reward_trend_value": 0.018284257579171025, "biggest_recent_change": 1.2659683057712527 },
{"total_number_of_episodes":440, "number_of_timesteps":440000, "per_episode_reward":-792.32, "episode_reward_trend_value": 0.009010211048090848, "biggest_recent_change": 1.2659683057712527 },
{"total_number_of_episodes":450, "number_of_timesteps":450000, "per_episode_reward":-791.42, "episode_reward_trend_value": 0.030995547808927566, "biggest_recent_change": 1.2659683057712527 },
{"total_number_of_episodes":460, "number_of_timesteps":460000, "per_episode_reward":-789.53, "episode_reward_trend_value": 0.03918154267134039, "biggest_recent_change": 1.898066059708981 },
{"total_number_of_episodes":470, "number_of_timesteps":470000, "per_episode_reward":-788.76, "episode_reward_trend_value": 0.033637657779955685, "biggest_recent_change": 1.898066059708981 },
{"total_number_of_episodes":480, "number_of_timesteps":480000, "per_episode_reward":-789.75, "episode_reward_trend_value": 0.023644325942459292, "biggest_recent_change": 1.898066059708981 },
{"total_number_of_episodes":490, "number_of_timesteps":490000, "per_episode_reward":-788.67, "episode_reward_trend_value": 0.03139894501599529, "biggest_recent_change": 1.898066059708981 },
{"total_number_of_episodes":500, "number_of_timesteps":500000, "per_episode_reward":-788.45, "episode_reward_trend_value": 0.038300840322210664, "biggest_recent_change": 1.898066059708981 },
{"total_number_of_episodes":510, "number_of_timesteps":510000, "per_episode_reward":-789.99, "episode_reward_trend_value": 0.027462490842352354, "biggest_recent_change": 1.898066059708981 },
{"total_number_of_episodes":520, "number_of_timesteps":520000, "per_episode_reward":-789.66, "episode_reward_trend_value": 0.02237142792876941, "biggest_recent_change": 1.898066059708981 },
{"total_number_of_episodes":530, "number_of_timesteps":530000, "per_episode_reward":-788.92, "episode_reward_trend_value": 0.03777548371873031, "biggest_recent_change": 1.898066059708981 },
{"total_number_of_episodes":540, "number_of_timesteps":540000, "per_episode_reward":-789.01, "episode_reward_trend_value": 0.026818257028993587, "biggest_recent_change": 1.898066059708981 },
{"total_number_of_episodes":550, "number_of_timesteps":550000, "per_episode_reward":-788.85, "episode_reward_trend_value": 0.007477341625081459, "biggest_recent_change": 1.5405569974960827 },
{"total_number_of_episodes":560, "number_of_timesteps":560000, "per_episode_reward":-789.01, "episode_reward_trend_value": -0.002743796785128123, "biggest_recent_change": 1.5405569974960827 },
{"total_number_of_episodes":570, "number_of_timesteps":570000, "per_episode_reward":-788.43, "episode_reward_trend_value": 0.014661788908637795, "biggest_recent_change": 1.5405569974960827 },
{"total_number_of_episodes":580, "number_of_timesteps":580000, "per_episode_reward":-787.35, "episode_reward_trend_value": 0.014699414881023484, "biggest_recent_change": 1.5405569974960827 },
{"total_number_of_episodes":590, "number_of_timesteps":590000, "per_episode_reward":-786.49, "episode_reward_trend_value": 0.021693159064341216, "biggest_recent_change": 1.5405569974960827 },
{"total_number_of_episodes":600, "number_of_timesteps":600000, "per_episode_reward":-786.13, "episode_reward_trend_value": 0.04285045914161477, "biggest_recent_change": 1.081669787919509 },
{"total_number_of_episodes":610, "number_of_timesteps":610000, "per_episode_reward":-786.49, "episode_reward_trend_value": 0.03522071852629046, "biggest_recent_change": 1.081669787919509 },
{"total_number_of_episodes":620, "number_of_timesteps":620000, "per_episode_reward":-786.27, "episode_reward_trend_value": 0.029431269217827273, "biggest_recent_change": 1.081669787919509 },
{"total_number_of_episodes":630, "number_of_timesteps":630000, "per_episode_reward":-786.34, "episode_reward_trend_value": 0.029653351673689737, "biggest_recent_change": 1.081669787919509 },
{"total_number_of_episodes":640, "number_of_timesteps":640000, "per_episode_reward":-786.45, "episode_reward_trend_value": 0.026713398434355894, "biggest_recent_change": 1.081669787919509 },
{"total_number_of_episodes":650, "number_of_timesteps":650000, "per_episode_reward":-785.60, "episode_reward_trend_value": 0.03784768656876925, "biggest_recent_change": 1.081669787919509 },
{"total_number_of_episodes":660, "number_of_timesteps":660000, "per_episode_reward":-785.31, "episode_reward_trend_value": 0.034685829299915996, "biggest_recent_change": 1.081669787919509 },
{"total_number_of_episodes":670, "number_of_timesteps":670000, "per_episode_reward":-785.82, "episode_reward_trend_value": 0.017041242135347703, "biggest_recent_change": 0.8573308173697569 },
{"total_number_of_episodes":680, "number_of_timesteps":680000, "per_episode_reward":-785.88, "episode_reward_trend_value": 0.006781326252915834, "biggest_recent_change": 0.8492021407249695 },
{"total_number_of_episodes":690, "number_of_timesteps":690000, "per_episode_reward":-786.30, "episode_reward_trend_value": -0.0019058379981983207, "biggest_recent_change": 0.8492021407249695 },
{"total_number_of_episodes":700, "number_of_timesteps":700000, "per_episode_reward":-786.00, "episode_reward_trend_value": 0.005440786925001046, "biggest_recent_change": 0.8492021407249695 },
{"total_number_of_episodes":710, "number_of_timesteps":710000, "per_episode_reward":-785.98, "episode_reward_trend_value": 0.0032695390569844876, "biggest_recent_change": 0.8492021407249695 },
{"total_number_of_episodes":720, "number_of_timesteps":720000, "per_episode_reward":-785.51, "episode_reward_trend_value": 0.009213426762242863, "biggest_recent_change": 0.8492021407249695 },
{"total_number_of_episodes":730, "number_of_timesteps":730000, "per_episode_reward":-785.12, "episode_reward_trend_value": 0.014735729812947638, "biggest_recent_change": 0.8492021407249695 },
{"total_number_of_episodes":740, "number_of_timesteps":740000, "per_episode_reward":-785.83, "episode_reward_trend_value": -0.002545563752644537, "biggest_recent_change": 0.7061142801783262 },
{"total_number_of_episodes":750, "number_of_timesteps":750000, "per_episode_reward":-785.93, "episode_reward_trend_value": -0.0068728066821727735, "biggest_recent_change": 0.7061142801783262 },
{"total_number_of_episodes":760, "number_of_timesteps":760000, "per_episode_reward":-786.05, "episode_reward_trend_value": -0.0026416381184516365, "biggest_recent_change": 0.7061142801783262 },
{"total_number_of_episodes":770, "number_of_timesteps":770000, "per_episode_reward":-786.25, "episode_reward_trend_value": -0.004101689137404366, "biggest_recent_change": 0.7061142801783262 },
{"total_number_of_episodes":780, "number_of_timesteps":780000, "per_episode_reward":-786.31, "episode_reward_trend_value": -9.682085964312945e-05, "biggest_recent_change": 0.7061142801783262 },
{"total_number_of_episodes":790, "number_of_timesteps":790000, "per_episode_reward":-785.97, "episode_reward_trend_value": 0.00029604052103498666, "biggest_recent_change": 0.7061142801783262 },
{"total_number_of_episodes":800, "number_of_timesteps":800000, "per_episode_reward":-786.23, "episode_reward_trend_value": -0.002832565255103721, "biggest_recent_change": 0.7061142801783262 },
{"total_number_of_episodes":810, "number_of_timesteps":810000, "per_episode_reward":-786.54, "episode_reward_trend_value": -0.011432620670598225, "biggest_recent_change": 0.7061142801783262 },
{"total_number_of_episodes":820, "number_of_timesteps":820000, "per_episode_reward":-786.40, "episode_reward_trend_value": -0.014248294948485332, "biggest_recent_change": 0.7061142801783262 },
{"total_number_of_episodes":830, "number_of_timesteps":830000, "per_episode_reward":-786.91, "episode_reward_trend_value": -0.012012947525331835, "biggest_recent_change": 0.5049330120945115 },
{"total_number_of_episodes":840, "number_of_timesteps":840000, "per_episode_reward":-786.85, "episode_reward_trend_value": -0.010243797180050428, "biggest_recent_change": 0.5049330120945115 },
{"total_number_of_episodes":850, "number_of_timesteps":850000, "per_episode_reward":-786.98, "episode_reward_trend_value": -0.010309046787349669, "biggest_recent_change": 0.5049330120945115 },
{"total_number_of_episodes":860, "number_of_timesteps":860000, "per_episode_reward":-786.81, "episode_reward_trend_value": -0.006206010769045002, "biggest_recent_change": 0.5049330120945115 },
{"total_number_of_episodes":870, "number_of_timesteps":870000, "per_episode_reward":-786.63, "episode_reward_trend_value": -0.0035265057561509315, "biggest_recent_change": 0.5049330120945115 },
{"total_number_of_episodes":880, "number_of_timesteps":880000, "per_episode_reward":-787.01, "episode_reward_trend_value": -0.011509371552686946, "biggest_recent_change": 0.5049330120945115 },
{"total_number_of_episodes":890, "number_of_timesteps":890000, "per_episode_reward":-786.77, "episode_reward_trend_value": -0.006004136832850943, "biggest_recent_change": 0.5049330120945115 },
{"total_number_of_episodes":900, "number_of_timesteps":900000, "per_episode_reward":-787.00, "episode_reward_trend_value": -0.0050695528361795065, "biggest_recent_change": 0.5049330120945115 },
{"total_number_of_episodes":910, "number_of_timesteps":910000, "per_episode_reward":-786.69, "episode_reward_trend_value": -0.003148920356821034, "biggest_recent_change": 0.5049330120945115 },
{"total_number_of_episodes":920, "number_of_timesteps":920000, "per_episode_reward":-786.72, "episode_reward_trend_value": 0.0021387036396731673, "biggest_recent_change": 0.38002976474354 },
{"total_number_of_episodes":930, "number_of_timesteps":930000, "per_episode_reward":-786.78, "episode_reward_trend_value": 0.0007985843839984833, "biggest_recent_change": 0.38002976474354 },
{"total_number_of_episodes":940, "number_of_timesteps":940000, "per_episode_reward":-787.19, "episode_reward_trend_value": -0.002273935003243979, "biggest_recent_change": 0.4079370956654884 },
{"total_number_of_episodes":950, "number_of_timesteps":950000, "per_episode_reward":-787.73, "episode_reward_trend_value": -0.01024040577959416, "biggest_recent_change": 0.5451753319789532 },
{"total_number_of_episodes":960, "number_of_timesteps":960000, "per_episode_reward":-787.63, "episode_reward_trend_value": -0.011103600288484155, "biggest_recent_change": 0.5451753319789532 },
{"total_number_of_episodes":970, "number_of_timesteps":970000, "per_episode_reward":-787.99, "episode_reward_trend_value": -0.010971175223366774, "biggest_recent_change": 0.5451753319789532 },
{"total_number_of_episodes":980, "number_of_timesteps":980000, "per_episode_reward":-788.29, "episode_reward_trend_value": -0.01680818866846797, "biggest_recent_change": 0.5451753319789532 },
{"total_number_of_episodes":990, "number_of_timesteps":990000, "per_episode_reward":-788.39, "episode_reward_trend_value": -0.015495783771487165, "biggest_recent_change": 0.5451753319789532 },
{"total_number_of_episodes":1000, "number_of_timesteps":1000000, "per_episode_reward":-787.91, "episode_reward_trend_value": -0.013549210183564104, "biggest_recent_change": 0.5451753319789532 },
{"total_number_of_episodes":1010, "number_of_timesteps":1010000, "per_episode_reward":-787.55, "episode_reward_trend_value": -0.009235145976486416, "biggest_recent_change": 0.5451753319789532 },
{"total_number_of_episodes":1020, "number_of_timesteps":1020000, "per_episode_reward":-787.31, "episode_reward_trend_value": -0.005954665447988342, "biggest_recent_change": 0.5451753319789532 },
{"total_number_of_episodes":1030, "number_of_timesteps":1030000, "per_episode_reward":-786.67, "episode_reward_trend_value": 0.005710993884367276, "biggest_recent_change": 0.6419722442465172 },
{"total_number_of_episodes":1040, "number_of_timesteps":1040000, "per_episode_reward":-787.16, "episode_reward_trend_value": 0.006364082512496022, "biggest_recent_change": 0.6419722442465172 },
{"total_number_of_episodes":1050, "number_of_timesteps":1050000, "per_episode_reward":-786.87, "episode_reward_trend_value": 0.008361707640451892, "biggest_recent_change": 0.6419722442465172 },
{"total_number_of_episodes":1060, "number_of_timesteps":1060000, "per_episode_reward":-786.77, "episode_reward_trend_value": 0.013575760641521762, "biggest_recent_change": 0.6419722442465172 },
{"total_number_of_episodes":1070, "number_of_timesteps":1070000, "per_episode_reward":-786.86, "episode_reward_trend_value": 0.01584680665991477, "biggest_recent_change": 0.6419722442465172 },
{"total_number_of_episodes":1080, "number_of_timesteps":1080000, "per_episode_reward":-786.59, "episode_reward_trend_value": 0.02003775774988349, "biggest_recent_change": 0.6419722442465172 },
{"total_number_of_episodes":1090, "number_of_timesteps":1090000, "per_episode_reward":-786.80, "episode_reward_trend_value": 0.012274864850705097, "biggest_recent_change": 0.6419722442465172 },
{"total_number_of_episodes":1100, "number_of_timesteps":1100000, "per_episode_reward":-786.79, "episode_reward_trend_value": 0.00844245161889603, "biggest_recent_change": 0.6419722442465172 },
{"total_number_of_episodes":1110, "number_of_timesteps":1110000, "per_episode_reward":-786.60, "episode_reward_trend_value": 0.00792216733209317, "biggest_recent_change": 0.6419722442465172 },
{"total_number_of_episodes":1120, "number_of_timesteps":1120000, "per_episode_reward":-786.62, "episode_reward_trend_value": 0.0005816576929911813, "biggest_recent_change": 0.48639735544736595 },
{"total_number_of_episodes":1130, "number_of_timesteps":1130000, "per_episode_reward":-787.22, "episode_reward_trend_value": -0.0006594285339487114, "biggest_recent_change": 0.5980951158719563 },
{"total_number_of_episodes":1140, "number_of_timesteps":1140000, "per_episode_reward":-786.66, "episode_reward_trend_value": 0.002322951978419496, "biggest_recent_change": 0.5980951158719563 },
{"total_number_of_episodes":1150, "number_of_timesteps":1150000, "per_episode_reward":-786.72, "episode_reward_trend_value": 0.0006046137129677239, "biggest_recent_change": 0.5980951158719563 },
{"total_number_of_episodes":1160, "number_of_timesteps":1160000, "per_episode_reward":-786.85, "episode_reward_trend_value": 0.00015148654506699836, "biggest_recent_change": 0.5980951158719563 },
{"total_number_of_episodes":1170, "number_of_timesteps":1170000, "per_episode_reward":-786.85, "episode_reward_trend_value": -0.0029415477414444367, "biggest_recent_change": 0.5980951158719563 },
{"total_number_of_episodes":1180, "number_of_timesteps":1180000, "per_episode_reward":-786.95, "episode_reward_trend_value": -0.0016355461665309829, "biggest_recent_change": 0.5980951158719563 },
{"total_number_of_episodes":1190, "number_of_timesteps":1190000, "per_episode_reward":-786.97, "episode_reward_trend_value": -0.002063649356908728, "biggest_recent_change": 0.5980951158719563 },
{"total_number_of_episodes":1200, "number_of_timesteps":1200000, "per_episode_reward":-787.06, "episode_reward_trend_value": -0.005082270947619033, "biggest_recent_change": 0.5980951158719563 },
{"total_number_of_episodes":1210, "number_of_timesteps":1210000, "per_episode_reward":-787.08, "episode_reward_trend_value": -0.005066855119419718, "biggest_recent_change": 0.5980951158719563 },
{"total_number_of_episodes":1220, "number_of_timesteps":1220000, "per_episode_reward":-787.26, "episode_reward_trend_value": -0.0005002732678033478, "biggest_recent_change": 0.5538618248463081 },
{"total_number_of_episodes":1230, "number_of_timesteps":1230000, "per_episode_reward":-787.21, "episode_reward_trend_value": -0.006095741188232751, "biggest_recent_change": 0.18710274922648296 },
{"total_number_of_episodes":1240, "number_of_timesteps":1240000, "per_episode_reward":-786.31, "episode_reward_trend_value": 0.004527160353179877, "biggest_recent_change": 0.9025639560497893 },
{"total_number_of_episodes":1250, "number_of_timesteps":1250000, "per_episode_reward":-786.60, "episode_reward_trend_value": 0.0027775799115766454, "biggest_recent_change": 0.9025639560497893 },
{"total_number_of_episodes":1260, "number_of_timesteps":1260000, "per_episode_reward":-786.08, "episode_reward_trend_value": 0.008566223740350171, "biggest_recent_change": 0.9025639560497893 },
{"total_number_of_episodes":1270, "number_of_timesteps":1270000, "per_episode_reward":-785.49, "episode_reward_trend_value": 0.016218377577145778, "biggest_recent_change": 0.9025639560497893 },
{"total_number_of_episodes":1280, "number_of_timesteps":1280000, "per_episode_reward":-785.70, "episode_reward_trend_value": 0.014102461566754857, "biggest_recent_change": 0.9025639560497893 },
{"total_number_of_episodes":1290, "number_of_timesteps":1290000, "per_episode_reward":-785.28, "episode_reward_trend_value": 0.01976534086143273, "biggest_recent_change": 0.9025639560497893 },
{"total_number_of_episodes":1300, "number_of_timesteps":1300000, "per_episode_reward":-785.33, "episode_reward_trend_value": 0.019370238470609515, "biggest_recent_change": 0.9025639560497893 },
{"total_number_of_episodes":1310, "number_of_timesteps":1310000, "per_episode_reward":-785.33, "episode_reward_trend_value": 0.021479085776050195, "biggest_recent_change": 0.9025639560497893 },
{"total_number_of_episodes":1320, "number_of_timesteps":1320000, "per_episode_reward":-785.34, "episode_reward_trend_value": 0.020826103957247395, "biggest_recent_change": 0.9025639560497893 },
{"total_number_of_episodes":1330, "number_of_timesteps":1330000, "per_episode_reward":-785.45, "episode_reward_trend_value": 0.00952298004478962, "biggest_recent_change": 0.5920066435535318 },
{"total_number_of_episodes":1340, "number_of_timesteps":1340000, "per_episode_reward":-785.68, "episode_reward_trend_value": 0.010225646053526008, "biggest_recent_change": 0.5920066435535318 },
{"total_number_of_episodes":1350, "number_of_timesteps":1350000, "per_episode_reward":-785.71, "episode_reward_trend_value": 0.004138928114799809, "biggest_recent_change": 0.5920066435535318 },
{"total_number_of_episodes":1360, "number_of_timesteps":1360000, "per_episode_reward":-786.11, "episode_reward_trend_value": -0.006886492649997915, "biggest_recent_change": 0.42411097656327 },
{"total_number_of_episodes":1370, "number_of_timesteps":1370000, "per_episode_reward":-786.07, "episode_reward_trend_value": -0.00401897450526576, "biggest_recent_change": 0.42411097656327 },
{"total_number_of_episodes":1380, "number_of_timesteps":1380000, "per_episode_reward":-786.28, "episode_reward_trend_value": -0.011059451286337207, "biggest_recent_change": 0.40028122527826326 },
{"total_number_of_episodes":1390, "number_of_timesteps":1390000, "per_episode_reward":-786.33, "episode_reward_trend_value": -0.011098027575537293, "biggest_recent_change": 0.40028122527826326 },
{"total_number_of_episodes":1400, "number_of_timesteps":1400000, "per_episode_reward":-786.02, "episode_reward_trend_value": -0.007692736322524575, "biggest_recent_change": 0.40028122527826326 },
{"total_number_of_episodes":1410, "number_of_timesteps":1410000, "per_episode_reward":-786.34, "episode_reward_trend_value": -0.011163621949026491, "biggest_recent_change": 0.40028122527826326 },
{"total_number_of_episodes":1420, "number_of_timesteps":1420000, "per_episode_reward":-786.40, "episode_reward_trend_value": -0.010531160046086067, "biggest_recent_change": 0.40028122527826326 },
{"total_number_of_episodes":1430, "number_of_timesteps":1430000, "per_episode_reward":-786.28, "episode_reward_trend_value": -0.0067421739277152585, "biggest_recent_change": 0.40028122527826326 },
{"total_number_of_episodes":1440, "number_of_timesteps":1440000, "per_episode_reward":-786.56, "episode_reward_trend_value": -0.009481000807174194, "biggest_recent_change": 0.40028122527826326 },
{"total_number_of_episodes":1450, "number_of_timesteps":1450000, "per_episode_reward":-786.22, "episode_reward_trend_value": -0.001276414972016937, "biggest_recent_change": 0.33813149988588975 },
{"total_number_of_episodes":1460, "number_of_timesteps":1460000, "per_episode_reward":-786.32, "episode_reward_trend_value": -0.0028604803278148615, "biggest_recent_change": 0.33813149988588975 },
{"total_number_of_episodes":1470, "number_of_timesteps":1470000, "per_episode_reward":-786.34, "episode_reward_trend_value": -0.0006760229308888988, "biggest_recent_change": 0.33813149988588975 },
{"total_number_of_episodes":1480, "number_of_timesteps":1480000, "per_episode_reward":-786.38, "episode_reward_trend_value": -0.000586138366067088, "biggest_recent_change": 0.33813149988588975 },
{"total_number_of_episodes":1490, "number_of_timesteps":1490000, "per_episode_reward":-786.24, "episode_reward_trend_value": -0.002366679798761753, "biggest_recent_change": 0.33813149988588975 },
{"total_number_of_episodes":1500, "number_of_timesteps":1500000, "per_episode_reward":-786.58, "episode_reward_trend_value": -0.0026075322272415885, "biggest_recent_change": 0.34255507663294793 },
{"total_number_of_episodes":1510, "number_of_timesteps":1510000, "per_episode_reward":-786.33, "episode_reward_trend_value": 0.0007443807797509786, "biggest_recent_change": 0.34255507663294793 },
{"total_number_of_episodes":1520, "number_of_timesteps":1520000, "per_episode_reward":-786.22, "episode_reward_trend_value": 0.0006396229561207495, "biggest_recent_change": 0.34255507663294793 },
{"total_number_of_episodes":1530, "number_of_timesteps":1530000, "per_episode_reward":-786.15, "episode_reward_trend_value": 0.004619294692503849, "biggest_recent_change": 0.34255507663294793 },
{"total_number_of_episodes":1540, "number_of_timesteps":1540000, "per_episode_reward":-785.98, "episode_reward_trend_value": 0.0027351510644052722, "biggest_recent_change": 0.34255507663294793 },
{"total_number_of_episodes":1550, "number_of_timesteps":1550000, "per_episode_reward":-785.56, "episode_reward_trend_value": 0.008485315300207377, "biggest_recent_change": 0.4183655395212327 },
{"total_number_of_episodes":1560, "number_of_timesteps":1560000, "per_episode_reward":-785.77, "episode_reward_trend_value": 0.006336237685976205, "biggest_recent_change": 0.4183655395212327 },
{"total_number_of_episodes":1570, "number_of_timesteps":1570000, "per_episode_reward":-785.95, "episode_reward_trend_value": 0.004789049532363757, "biggest_recent_change": 0.4183655395212327 },
{"total_number_of_episodes":1580, "number_of_timesteps":1580000, "per_episode_reward":-785.66, "episode_reward_trend_value": 0.006392634936831757, "biggest_recent_change": 0.4183655395212327 },
{"total_number_of_episodes":1590, "number_of_timesteps":1590000, "per_episode_reward":-785.82, "episode_reward_trend_value": 0.008457486146260686, "biggest_recent_change": 0.4183655395212327 },
{"total_number_of_episodes":1600, "number_of_timesteps":1600000, "per_episode_reward":-785.71, "episode_reward_trend_value": 0.0069045361340840826, "biggest_recent_change": 0.4183655395212327 },
{"total_number_of_episodes":1610, "number_of_timesteps":1610000, "per_episode_reward":-785.96, "episode_reward_trend_value": 0.0029572343335126385, "biggest_recent_change": 0.4183655395212327 },
{"total_number_of_episodes":1620, "number_of_timesteps":1620000, "per_episode_reward":-786.33, "episode_reward_trend_value": -0.0020513782668634423, "biggest_recent_change": 0.4183655395212327 },
{"total_number_of_episodes":1630, "number_of_timesteps":1630000, "per_episode_reward":-786.30, "episode_reward_trend_value": -0.003610482851295021, "biggest_recent_change": 0.4183655395212327 },
{"total_number_of_episodes":1640, "number_of_timesteps":1640000, "per_episode_reward":-786.21, "episode_reward_trend_value": -0.007218301495524632, "biggest_recent_change": 0.37287213804984276 },
{"total_number_of_episodes":1650, "number_of_timesteps":1650000, "per_episode_reward":-785.89, "episode_reward_trend_value": -0.0013940940875525284, "biggest_recent_change": 0.37287213804984276 },
{"total_number_of_episodes":1660, "number_of_timesteps":1660000, "per_episode_reward":-785.75, "episode_reward_trend_value": 0.0022303982188923836, "biggest_recent_change": 0.37287213804984276 },
{"total_number_of_episodes":1670, "number_of_timesteps":1670000, "per_episode_reward":-785.60, "episode_reward_trend_value": 0.0006590830210408664, "biggest_recent_change": 0.37287213804984276 },
{"total_number_of_episodes":1680, "number_of_timesteps":1680000, "per_episode_reward":-785.61, "episode_reward_trend_value": 0.0022889752927262126, "biggest_recent_change": 0.37287213804984276 },
{"total_number_of_episodes":1690, "number_of_timesteps":1690000, "per_episode_reward":-785.68, "episode_reward_trend_value": 0.00033543896066450744, "biggest_recent_change": 0.37287213804984276 },
{"total_number_of_episodes":1700, "number_of_timesteps":1700000, "per_episode_reward":-785.27, "episode_reward_trend_value": 0.007601116391505356, "biggest_recent_change": 0.40842105071192236 },
{"total_number_of_episodes":1710, "number_of_timesteps":1710000, "per_episode_reward":-785.44, "episode_reward_trend_value": 0.009881001300136126, "biggest_recent_change": 0.40842105071192236 },
{"total_number_of_episodes":1720, "number_of_timesteps":1720000, "per_episode_reward":-785.47, "episode_reward_trend_value": 0.009248674633747418, "biggest_recent_change": 0.40842105071192236 },
{"total_number_of_episodes":1730, "number_of_timesteps":1730000, "per_episode_reward":-785.65, "episode_reward_trend_value": 0.006217416610739848, "biggest_recent_change": 0.40842105071192236 },
{"total_number_of_episodes":1740, "number_of_timesteps":1740000, "per_episode_reward":-785.69, "episode_reward_trend_value": 0.0022211503867172015, "biggest_recent_change": 0.40842105071192236 },
{"total_number_of_episodes":1750, "number_of_timesteps":1750000, "per_episode_reward":-785.91, "episode_reward_trend_value": -0.0017842809968758248, "biggest_recent_change": 0.40842105071192236 },
{"total_number_of_episodes":1760, "number_of_timesteps":1760000, "per_episode_reward":-785.64, "episode_reward_trend_value": -0.00040236913253390114, "biggest_recent_change": 0.40842105071192236 },
{"total_number_of_episodes":1770, "number_of_timesteps":1770000, "per_episode_reward":-785.52, "episode_reward_trend_value": 0.0010631584316443978, "biggest_recent_change": 0.40842105071192236 },
{"total_number_of_episodes":1780, "number_of_timesteps":1780000, "per_episode_reward":-785.36, "episode_reward_trend_value": 0.0035696837269483693, "biggest_recent_change": 0.40842105071192236 },
{"total_number_of_episodes":1790, "number_of_timesteps":1790000, "per_episode_reward":-785.22, "episode_reward_trend_value": 0.000627479965988995, "biggest_recent_change": 0.27619737847805936 },
{"total_number_of_episodes":1800, "number_of_timesteps":1800000, "per_episode_reward":-785.24, "episode_reward_trend_value": 0.00225006869626567, "biggest_recent_change": 0.27619737847805936 },
{"total_number_of_episodes":1810, "number_of_timesteps":1810000, "per_episode_reward":-785.39, "episode_reward_trend_value": 0.0008645616603479943, "biggest_recent_change": 0.27619737847805936 },
{"total_number_of_episodes":1820, "number_of_timesteps":1820000, "per_episode_reward":-785.28, "episode_reward_trend_value": 0.004100509356157999, "biggest_recent_change": 0.27619737847805936 },
{"total_number_of_episodes":1830, "number_of_timesteps":1830000, "per_episode_reward":-785.47, "episode_reward_trend_value": 0.002422570156483693, "biggest_recent_change": 0.27619737847805936 },
{"total_number_of_episodes":1840, "number_of_timesteps":1840000, "per_episode_reward":-785.34, "episode_reward_trend_value": 0.00636723485336006, "biggest_recent_change": 0.27619737847805936 },
{"total_number_of_episodes":1850, "number_of_timesteps":1850000, "per_episode_reward":-785.16, "episode_reward_trend_value": 0.0053360988237563505, "biggest_recent_change": 0.19284757470586555 },
{"total_number_of_episodes":1860, "number_of_timesteps":1860000, "per_episode_reward":-785.06, "episode_reward_trend_value": 0.005022328176904819, "biggest_recent_change": 0.19284757470586555 },
{"total_number_of_episodes":1870, "number_of_timesteps":1870000, "per_episode_reward":-785.11, "episode_reward_trend_value": 0.0027996128697357864, "biggest_recent_change": 0.19284757470586555 },
{"total_number_of_episodes":1880, "number_of_timesteps":1880000, "per_episode_reward":-785.30, "episode_reward_trend_value": -0.0009048106430769722, "biggest_recent_change": 0.19284757470586555 },
{"total_number_of_episodes":1890, "number_of_timesteps":1890000, "per_episode_reward":-784.96, "episode_reward_trend_value": 0.003122141849015255, "biggest_recent_change": 0.3407762137401278 },
{"total_number_of_episodes":1900, "number_of_timesteps":1900000, "per_episode_reward":-785.17, "episode_reward_trend_value": 0.0025327457323694744, "biggest_recent_change": 0.3407762137401278 },
{"total_number_of_episodes":1910, "number_of_timesteps":1910000, "per_episode_reward":-784.92, "episode_reward_trend_value": 0.004038036145122229, "biggest_recent_change": 0.3407762137401278 },
{"total_number_of_episodes":1920, "number_of_timesteps":1920000, "per_episode_reward":-785.01, "episode_reward_trend_value": 0.005195501068336246, "biggest_recent_change": 0.3407762137401278 },
{"total_number_of_episodes":1930, "number_of_timesteps":1930000, "per_episode_reward":-784.70, "episode_reward_trend_value": 0.007101501894386628, "biggest_recent_change": 0.3407762137401278 },
{"total_number_of_episodes":1940, "number_of_timesteps":1940000, "per_episode_reward":-784.87, "episode_reward_trend_value": 0.0032298113336234257, "biggest_recent_change": 0.3407762137401278 },
{"total_number_of_episodes":1950, "number_of_timesteps":1950000, "per_episode_reward":-784.62, "episode_reward_trend_value": 0.00495925232110671, "biggest_recent_change": 0.3407762137401278 },
{"total_number_of_episodes":1960, "number_of_timesteps":1960000, "per_episode_reward":-784.59, "episode_reward_trend_value": 0.005728054331642499, "biggest_recent_change": 0.3407762137401278 },
{"total_number_of_episodes":1970, "number_of_timesteps":1970000, "per_episode_reward":-784.62, "episode_reward_trend_value": 0.007561903337365392, "biggest_recent_change": 0.3407762137401278 },
{"total_number_of_episodes":1980, "number_of_timesteps":1980000, "per_episode_reward":-784.71, "episode_reward_trend_value": 0.002780524047614108, "biggest_recent_change": 0.3048007771920993 },
{"total_number_of_episodes":1990, "number_of_timesteps":1990000, "per_episode_reward":-784.71, "episode_reward_trend_value": 0.005053320399197977, "biggest_recent_change": 0.3048007771920993 },
{"total_number_of_episodes":2000, "number_of_timesteps":2000000, "per_episode_reward":-784.39, "episode_reward_trend_value": 0.005812181839243951, "biggest_recent_change": 0.3158575988446728 },
{"total_number_of_episodes":2010, "number_of_timesteps":2010000, "per_episode_reward":-784.50, "episode_reward_trend_value": 0.0055830479584213005, "biggest_recent_change": 0.3158575988446728 },
{"total_number_of_episodes":2020, "number_of_timesteps":2020000, "per_episode_reward":-784.40, "episode_reward_trend_value": 0.0033742370729378814, "biggest_recent_change": 0.3158575988446728 },
{"total_number_of_episodes":2030, "number_of_timesteps":2030000, "per_episode_reward":-784.54, "episode_reward_trend_value": 0.0035871686088954553, "biggest_recent_change": 0.3158575988446728 },
{"total_number_of_episodes":2040, "number_of_timesteps":2040000, "per_episode_reward":-784.33, "episode_reward_trend_value": 0.003223543461687465, "biggest_recent_change": 0.3158575988446728 },
{"total_number_of_episodes":2050, "number_of_timesteps":2050000, "per_episode_reward":-784.57, "episode_reward_trend_value": 0.00024109380977835017, "biggest_recent_change": 0.3158575988446728 },
{"total_number_of_episodes":2060, "number_of_timesteps":2060000, "per_episode_reward":-784.26, "episode_reward_trend_value": 0.004024290457983979, "biggest_recent_change": 0.3158575988446728 },
{"total_number_of_episodes":2070, "number_of_timesteps":2070000, "per_episode_reward":-783.94, "episode_reward_trend_value": 0.008552022087884678, "biggest_recent_change": 0.3179479243535752 },
{"total_number_of_episodes":2080, "number_of_timesteps":2080000, "per_episode_reward":-784.02, "episode_reward_trend_value": 0.007640857096173982, "biggest_recent_change": 0.3179479243535752 },
{"total_number_of_episodes":2090, "number_of_timesteps":2090000, "per_episode_reward":-783.58, "episode_reward_trend_value": 0.009056348193833703, "biggest_recent_change": 0.44325179763404776 },
{"total_number_of_episodes":2100, "number_of_timesteps":2100000, "per_episode_reward":-783.58, "episode_reward_trend_value": 0.010245226013224359, "biggest_recent_change": 0.44325179763404776 },
{"total_number_of_episodes":2110, "number_of_timesteps":2110000, "per_episode_reward":-783.49, "episode_reward_trend_value": 0.010053256959719824, "biggest_recent_change": 0.44325179763404776 },
{"total_number_of_episodes":2120, "number_of_timesteps":2120000, "per_episode_reward":-783.46, "episode_reward_trend_value": 0.012060928226754994, "biggest_recent_change": 0.44325179763404776 },
{"total_number_of_episodes":2130, "number_of_timesteps":2130000, "per_episode_reward":-783.60, "episode_reward_trend_value": 0.008079074545298682, "biggest_recent_change": 0.44325179763404776 },
{"total_number_of_episodes":2140, "number_of_timesteps":2140000, "per_episode_reward":-783.50, "episode_reward_trend_value": 0.01192203280078527, "biggest_recent_change": 0.44325179763404776 },
{"total_number_of_episodes":2150, "number_of_timesteps":2150000, "per_episode_reward":-783.42, "episode_reward_trend_value": 0.009280873516067661, "biggest_recent_change": 0.44325179763404776 },
{"total_number_of_episodes":2160, "number_of_timesteps":2160000, "per_episode_reward":-783.54, "episode_reward_trend_value": 0.004466240187868051, "biggest_recent_change": 0.44325179763404776 },
{"total_number_of_episodes":2170, "number_of_timesteps":2170000, "per_episode_reward":-783.61, "episode_reward_trend_value": 0.004623702552762805, "biggest_recent_change": 0.44325179763404776 },
{"total_number_of_episodes":2180, "number_of_timesteps":2180000, "per_episode_reward":-783.62, "episode_reward_trend_value": -0.0004730366800521956, "biggest_recent_change": 0.14181344647954575 },
{"total_number_of_episodes":2190, "number_of_timesteps":2190000, "per_episode_reward":-783.65, "episode_reward_trend_value": -0.0007520646167853961, "biggest_recent_change": 0.14181344647954575 },
{"total_number_of_episodes":2200, "number_of_timesteps":2200000, "per_episode_reward":-783.36, "episode_reward_trend_value": 0.001430605766909644, "biggest_recent_change": 0.28517091721573706 },
{"total_number_of_episodes":2210, "number_of_timesteps":2210000, "per_episode_reward":-783.35, "episode_reward_trend_value": 0.0012274456039891549, "biggest_recent_change": 0.28517091721573706 },
{"total_number_of_episodes":2220, "number_of_timesteps":2220000, "per_episode_reward":-783.44, "episode_reward_trend_value": 0.001740269681967119, "biggest_recent_change": 0.28517091721573706 },
{"total_number_of_episodes":2230, "number_of_timesteps":2230000, "per_episode_reward":-783.21, "episode_reward_trend_value": 0.0032063192793215316, "biggest_recent_change": 0.28517091721573706 },
{"total_number_of_episodes":2240, "number_of_timesteps":2240000, "per_episode_reward":-783.15, "episode_reward_trend_value": 0.0029832161806515006, "biggest_recent_change": 0.28517091721573706 },
{"total_number_of_episodes":2250, "number_of_timesteps":2250000, "per_episode_reward":-783.20, "episode_reward_trend_value": 0.0037197753172260187, "biggest_recent_change": 0.28517091721573706 },
{"total_number_of_episodes":2260, "number_of_timesteps":2260000, "per_episode_reward":-783.24, "episode_reward_trend_value": 0.004077683224084518, "biggest_recent_change": 0.28517091721573706 },
{"total_number_of_episodes":2270, "number_of_timesteps":2270000, "per_episode_reward":-783.13, "episode_reward_trend_value": 0.005429136548606392, "biggest_recent_change": 0.28517091721573706 },
{"total_number_of_episodes":2280, "number_of_timesteps":2280000, "per_episode_reward":-783.07, "episode_reward_trend_value": 0.006387732299521708, "biggest_recent_change": 0.28517091721573706 },
{"total_number_of_episodes":2290, "number_of_timesteps":2290000, "per_episode_reward":-783.03, "episode_reward_trend_value": 0.003723339382723781, "biggest_recent_change": 0.23241809280534653 },
{"total_number_of_episodes":2300, "number_of_timesteps":2300000, "per_episode_reward":-783.12, "episode_reward_trend_value": 0.002497718671127637, "biggest_recent_change": 0.23241809280534653 },
{"total_number_of_episodes":2310, "number_of_timesteps":2310000, "per_episode_reward":-782.98, "episode_reward_trend_value": 0.005158848041655043, "biggest_recent_change": 0.23241809280534653 },
{"total_number_of_episodes":2320, "number_of_timesteps":2320000, "per_episode_reward":-783.11, "episode_reward_trend_value": 0.0011143489078032569, "biggest_recent_change": 0.14384236388593763 },
{"total_number_of_episodes":2330, "number_of_timesteps":2330000, "per_episode_reward":-783.13, "episode_reward_trend_value": 0.00022504529594647872, "biggest_recent_change": 0.14384236388593763 },
{"total_number_of_episodes":2340, "number_of_timesteps":2340000, "per_episode_reward":-783.03, "episode_reward_trend_value": 0.0019137864638006856, "biggest_recent_change": 0.14384236388593763 },
{"total_number_of_episodes":2350, "number_of_timesteps":2350000, "per_episode_reward":-782.91, "episode_reward_trend_value": 0.0036736072910634396, "biggest_recent_change": 0.14384236388593763 },
{"total_number_of_episodes":2360, "number_of_timesteps":2360000, "per_episode_reward":-783.00, "episode_reward_trend_value": 0.001526906755443886, "biggest_recent_change": 0.14384236388593763 },
{"total_number_of_episodes":2370, "number_of_timesteps":2370000, "per_episode_reward":-783.05, "episode_reward_trend_value": 0.00027593093984074177, "biggest_recent_change": 0.14384236388593763 },
{"total_number_of_episodes":2380, "number_of_timesteps":2380000, "per_episode_reward":-782.91, "episode_reward_trend_value": 0.001272662409990567, "biggest_recent_change": 0.14384236388593763 },
{"total_number_of_episodes":2390, "number_of_timesteps":2390000, "per_episode_reward":-782.90, "episode_reward_trend_value": 0.002424544807550117, "biggest_recent_change": 0.14384236388593763 },
{"total_number_of_episodes":2400, "number_of_timesteps":2400000, "per_episode_reward":-783.02, "episode_reward_trend_value": -0.0004564179574976556, "biggest_recent_change": 0.1350813870174079 },
{"total_number_of_episodes":2410, "number_of_timesteps":2410000, "per_episode_reward":-782.99, "episode_reward_trend_value": 0.0013364595793316566, "biggest_recent_change": 0.1350813870174079 },
{"total_number_of_episodes":2420, "number_of_timesteps":2420000, "per_episode_reward":-782.99, "episode_reward_trend_value": 0.0016050433763944564, "biggest_recent_change": 0.1350813870174079 },
{"total_number_of_episodes":2430, "number_of_timesteps":2430000, "per_episode_reward":-782.76, "episode_reward_trend_value": 0.003001493097759875, "biggest_recent_change": 0.22858842713708327 },
{"total_number_of_episodes":2440, "number_of_timesteps":2440000, "per_episode_reward":-782.60, "episode_reward_trend_value": 0.0034837625948107923, "biggest_recent_change": 0.22858842713708327 },
{"total_number_of_episodes":2450, "number_of_timesteps":2450000, "per_episode_reward":-782.49, "episode_reward_trend_value": 0.0055890533662477536, "biggest_recent_change": 0.22858842713708327 },
{"total_number_of_episodes":2460, "number_of_timesteps":2460000, "per_episode_reward":-782.51, "episode_reward_trend_value": 0.005944368864340454, "biggest_recent_change": 0.22858842713708327 },
{"total_number_of_episodes":2470, "number_of_timesteps":2470000, "per_episode_reward":-782.65, "episode_reward_trend_value": 0.0029591383257272555, "biggest_recent_change": 0.22858842713708327 },
{"total_number_of_episodes":2480, "number_of_timesteps":2480000, "per_episode_reward":-782.43, "episode_reward_trend_value": 0.00532495797517135, "biggest_recent_change": 0.22858842713708327 },
{"total_number_of_episodes":2490, "number_of_timesteps":2490000, "per_episode_reward":-782.31, "episode_reward_trend_value": 0.007934171178299544, "biggest_recent_change": 0.22858842713708327 },
{"total_number_of_episodes":2500, "number_of_timesteps":2500000, "per_episode_reward":-782.27, "episode_reward_trend_value": 0.00795087065872369, "biggest_recent_change": 0.22858842713708327 },
{"total_number_of_episodes":2510, "number_of_timesteps":2510000, "per_episode_reward":-782.14, "episode_reward_trend_value": 0.009402754732281613, "biggest_recent_change": 0.22858842713708327 },
{"total_number_of_episodes":2520, "number_of_timesteps":2520000, "per_episode_reward":-782.11, "episode_reward_trend_value": 0.0072700801035327665, "biggest_recent_change": 0.2228001431382154 },
{"total_number_of_episodes":2530, "number_of_timesteps":2530000, "per_episode_reward":-782.00, "episode_reward_trend_value": 0.0065718659469982624, "biggest_recent_change": 0.2228001431382154 },
{"total_number_of_episodes":2540, "number_of_timesteps":2540000, "per_episode_reward":-781.99, "episode_reward_trend_value": 0.005555755245076297, "biggest_recent_change": 0.2228001431382154 },
{"total_number_of_episodes":2550, "number_of_timesteps":2550000, "per_episode_reward":-781.89, "episode_reward_trend_value": 0.006926330420071736, "biggest_recent_change": 0.2228001431382154 },
{"total_number_of_episodes":2560, "number_of_timesteps":2560000, "per_episode_reward":-782.05, "episode_reward_trend_value": 0.006678145438060154, "biggest_recent_change": 0.2228001431382154 },
{"total_number_of_episodes":2570, "number_of_timesteps":2570000, "per_episode_reward":-782.21, "episode_reward_trend_value": 0.002436513119419538, "biggest_recent_change": 0.1589467655394401 },
{"total_number_of_episodes":2580, "number_of_timesteps":2580000, "per_episode_reward":-782.21, "episode_reward_trend_value": 0.0010630570662998353, "biggest_recent_change": 0.1589467655394401 },
{"total_number_of_episodes":2590, "number_of_timesteps":2590000, "per_episode_reward":-782.20, "episode_reward_trend_value": 0.0008002071633162208, "biggest_recent_change": 0.1589467655394401 },
{"total_number_of_episodes":2600, "number_of_timesteps":2600000, "per_episode_reward":-782.21, "episode_reward_trend_value": -0.000715365704221919, "biggest_recent_change": 0.1589467655394401 },
{"total_number_of_episodes":2610, "number_of_timesteps":2610000, "per_episode_reward":-782.19, "episode_reward_trend_value": -0.0009698527890059773, "biggest_recent_change": 0.1589467655394401 },
{"total_number_of_episodes":2620, "number_of_timesteps":2620000, "per_episode_reward":-782.13, "episode_reward_trend_value": -0.001390192132412772, "biggest_recent_change": 0.1589467655394401 },
{"total_number_of_episodes":2630, "number_of_timesteps":2630000, "per_episode_reward":-782.18, "episode_reward_trend_value": -0.002106671453885964, "biggest_recent_change": 0.1589467655394401 },
{"total_number_of_episodes":2640, "number_of_timesteps":2640000, "per_episode_reward":-781.99, "episode_reward_trend_value": -0.0011314127911797793, "biggest_recent_change": 0.18937794294811283 },
{"total_number_of_episodes":2650, "number_of_timesteps":2650000, "per_episode_reward":-782.19, "episode_reward_trend_value": -0.0015925677322166445, "biggest_recent_change": 0.19742995453214007 },
{"total_number_of_episodes":2660, "number_of_timesteps":2660000, "per_episode_reward":-782.15, "episode_reward_trend_value": 0.0006388071794693436, "biggest_recent_change": 0.19742995453214007 },
{"total_number_of_episodes":2670, "number_of_timesteps":2670000, "per_episode_reward":-782.22, "episode_reward_trend_value": -0.00010947406367197094, "biggest_recent_change": 0.19742995453214007 },
{"total_number_of_episodes":2680, "number_of_timesteps":2680000, "per_episode_reward":-782.38, "episode_reward_trend_value": -0.0019190262569281913, "biggest_recent_change": 0.19742995453214007 },
{"total_number_of_episodes":2690, "number_of_timesteps":2690000, "per_episode_reward":-782.35, "episode_reward_trend_value": -0.0015939356721155998, "biggest_recent_change": 0.19742995453214007 },
{"total_number_of_episodes":2700, "number_of_timesteps":2700000, "per_episode_reward":-782.43, "episode_reward_trend_value": -0.0026389698904179366, "biggest_recent_change": 0.19742995453214007 },
{"total_number_of_episodes":2710, "number_of_timesteps":2710000, "per_episode_reward":-782.31, "episode_reward_trend_value": -0.00196962961037141, "biggest_recent_change": 0.19742995453214007 },
{"total_number_of_episodes":2720, "number_of_timesteps":2720000, "per_episode_reward":-782.42, "episode_reward_trend_value": -0.0026240584485195642, "biggest_recent_change": 0.19742995453214007 },
{"total_number_of_episodes":2730, "number_of_timesteps":2730000, "per_episode_reward":-782.33, "episode_reward_trend_value": -0.0037083617120869244, "biggest_recent_change": 0.19742995453214007 },
{"total_number_of_episodes":2740, "number_of_timesteps":2740000, "per_episode_reward":-782.29, "episode_reward_trend_value": -0.0011552465610318702, "biggest_recent_change": 0.15524108635008815 },
{"total_number_of_episodes":2750, "number_of_timesteps":2750000, "per_episode_reward":-782.23, "episode_reward_trend_value": -0.0008630500242563559, "biggest_recent_change": 0.15524108635008815 },
{"total_number_of_episodes":2760, "number_of_timesteps":2760000, "per_episode_reward":-782.13, "episode_reward_trend_value": 0.001022034087756967, "biggest_recent_change": 0.15524108635008815 },
{"total_number_of_episodes":2770, "number_of_timesteps":2770000, "per_episode_reward":-782.11, "episode_reward_trend_value": 0.002925366281288512, "biggest_recent_change": 0.12387756329655986 },
{"total_number_of_episodes":2780, "number_of_timesteps":2780000, "per_episode_reward":-782.23, "episode_reward_trend_value": 0.0013786441772266598, "biggest_recent_change": 0.12387756329655986 },
{"total_number_of_episodes":2790, "number_of_timesteps":2790000, "per_episode_reward":-782.12, "episode_reward_trend_value": 0.0034851340423554373, "biggest_recent_change": 0.12387756329655986 },
{"total_number_of_episodes":2800, "number_of_timesteps":2800000, "per_episode_reward":-781.99, "episode_reward_trend_value": 0.003493318932761566, "biggest_recent_change": 0.12461420343311147 },
{"total_number_of_episodes":2810, "number_of_timesteps":2810000, "per_episode_reward":-781.90, "episode_reward_trend_value": 0.005800035144090998, "biggest_recent_change": 0.12461420343311147 },
{"total_number_of_episodes":2820, "number_of_timesteps":2820000, "per_episode_reward":-782.10, "episode_reward_trend_value": 0.0025504290921200363, "biggest_recent_change": 0.2006738954503362 },
{"total_number_of_episodes":2830, "number_of_timesteps":2830000, "per_episode_reward":-781.99, "episode_reward_trend_value": 0.0034239255216827282, "biggest_recent_change": 0.2006738954503362 },
{"total_number_of_episodes":2840, "number_of_timesteps":2840000, "per_episode_reward":-781.92, "episode_reward_trend_value": 0.003400846144576361, "biggest_recent_change": 0.2006738954503362 },
{"total_number_of_episodes":2850, "number_of_timesteps":2850000, "per_episode_reward":-781.99, "episode_reward_trend_value": 0.0015389496485163868, "biggest_recent_change": 0.2006738954503362 },
{"total_number_of_episodes":2860, "number_of_timesteps":2860000, "per_episode_reward":-781.96, "episode_reward_trend_value": 0.0017012522688749717, "biggest_recent_change": 0.2006738954503362 },
{"total_number_of_episodes":2870, "number_of_timesteps":2870000, "per_episode_reward":-781.91, "episode_reward_trend_value": 0.003484003355760655, "biggest_recent_change": 0.2006738954503362 },
{"total_number_of_episodes":2880, "number_of_timesteps":2880000, "per_episode_reward":-782.04, "episode_reward_trend_value": 0.0008004922910408317, "biggest_recent_change": 0.2006738954503362 },
{"total_number_of_episodes":2890, "number_of_timesteps":2890000, "per_episode_reward":-781.83, "episode_reward_trend_value": 0.0018190000348858523, "biggest_recent_change": 0.21627990037916334 },
{"total_number_of_episodes":2900, "number_of_timesteps":2900000, "per_episode_reward":-781.85, "episode_reward_trend_value": 0.00046187245024940335, "biggest_recent_change": 0.21627990037916334 },
{"total_number_of_episodes":2910, "number_of_timesteps":2910000, "per_episode_reward":-781.80, "episode_reward_trend_value": 0.0032697437838755325, "biggest_recent_change": 0.21627990037916334 },
{"total_number_of_episodes":2920, "number_of_timesteps":2920000, "per_episode_reward":-781.89, "episode_reward_trend_value": 0.0010584322083509001, "biggest_recent_change": 0.21627990037916334 },
{"total_number_of_episodes":2930, "number_of_timesteps":2930000, "per_episode_reward":-781.77, "episode_reward_trend_value": 0.0016884545042545242, "biggest_recent_change": 0.21627990037916334 },
{"total_number_of_episodes":2940, "number_of_timesteps":2940000, "per_episode_reward":-781.66, "episode_reward_trend_value": 0.0036630061334853455, "biggest_recent_change": 0.21627990037916334 },
{"total_number_of_episodes":2950, "number_of_timesteps":2950000, "per_episode_reward":-781.80, "episode_reward_trend_value": 0.0017183932275543156, "biggest_recent_change": 0.21627990037916334 },
{"total_number_of_episodes":2960, "number_of_timesteps":2960000, "per_episode_reward":-781.71, "episode_reward_trend_value": 0.0022389659496335904, "biggest_recent_change": 0.21627990037916334 },
{"total_number_of_episodes":2970, "number_of_timesteps":2970000, "per_episode_reward":-781.67, "episode_reward_trend_value": 0.00411379871656234, "biggest_recent_change": 0.21627990037916334 },
{"total_number_of_episodes":2980, "number_of_timesteps":2980000, "per_episode_reward":-781.48, "episode_reward_trend_value": 0.0038131907748190008, "biggest_recent_change": 0.18922518562226287 },
{"total_number_of_episodes":2990, "number_of_timesteps":2990000, "per_episode_reward":-781.49, "episode_reward_trend_value": 0.004036051370208801, "biggest_recent_change": 0.18922518562226287 },
{"total_number_of_episodes":3000, "number_of_timesteps":3000000, "per_episode_reward":-781.51, "episode_reward_trend_value": 0.0032202005110510377, "biggest_recent_change": 0.18922518562226287 },
{"total_number_of_episodes":3010, "number_of_timesteps":3010000, "per_episode_reward":-781.60, "episode_reward_trend_value": 0.0031804562333516313, "biggest_recent_change": 0.18922518562226287 },
{"total_number_of_episodes":3020, "number_of_timesteps":3020000, "per_episode_reward":-781.71, "episode_reward_trend_value": 0.0006955966523315106, "biggest_recent_change": 0.18922518562226287 },
{"total_number_of_episodes":3030, "number_of_timesteps":3030000, "per_episode_reward":-781.64, "episode_reward_trend_value": 0.00019457787603869293, "biggest_recent_change": 0.18922518562226287 },
{"total_number_of_episodes":3040, "number_of_timesteps":3040000, "per_episode_reward":-781.47, "episode_reward_trend_value": 0.0037230073629328014, "biggest_recent_change": 0.18922518562226287 },
{"total_number_of_episodes":3050, "number_of_timesteps":3050000, "per_episode_reward":-781.43, "episode_reward_trend_value": 0.0030732325419156343, "biggest_recent_change": 0.18922518562226287 },
{"total_number_of_episodes":3060, "number_of_timesteps":3060000, "per_episode_reward":-781.43, "episode_reward_trend_value": 0.0027116968176452852, "biggest_recent_change": 0.18922518562226287 },
{"total_number_of_episodes":3070, "number_of_timesteps":3070000, "per_episode_reward":-781.17, "episode_reward_trend_value": 0.0035336590020935746, "biggest_recent_change": 0.2632017822226089 },
{"total_number_of_episodes":3080, "number_of_timesteps":3080000, "per_episode_reward":-781.22, "episode_reward_trend_value": 0.0030174504488374825, "biggest_recent_change": 0.2632017822226089 },
{"total_number_of_episodes":3090, "number_of_timesteps":3090000, "per_episode_reward":-781.16, "episode_reward_trend_value": 0.0039257517355875075, "biggest_recent_change": 0.2632017822226089 },
{"total_number_of_episodes":3100, "number_of_timesteps":3100000, "per_episode_reward":-781.12, "episode_reward_trend_value": 0.0054172042605462895, "biggest_recent_change": 0.2632017822226089 },
{"total_number_of_episodes":3110, "number_of_timesteps":3110000, "per_episode_reward":-781.10, "episode_reward_trend_value": 0.006706745180825794, "biggest_recent_change": 0.2632017822226089 },
{"total_number_of_episodes":3120, "number_of_timesteps":3120000, "per_episode_reward":-781.08, "episode_reward_trend_value": 0.006288246426839174, "biggest_recent_change": 0.2632017822226089 },
{"total_number_of_episodes":3130, "number_of_timesteps":3130000, "per_episode_reward":-781.07, "episode_reward_trend_value": 0.0043962091471611425, "biggest_recent_change": 0.2632017822226089 },
{"total_number_of_episodes":3140, "number_of_timesteps":3140000, "per_episode_reward":-781.08, "episode_reward_trend_value": 0.003923167075034447, "biggest_recent_change": 0.2632017822226089 },
{"total_number_of_episodes":3150, "number_of_timesteps":3150000, "per_episode_reward":-781.14, "episode_reward_trend_value": 0.003226766502834582, "biggest_recent_change": 0.2632017822226089 },
{"total_number_of_episodes":3160, "number_of_timesteps":3160000, "per_episode_reward":-781.04, "episode_reward_trend_value": 0.001416999271085236, "biggest_recent_change": 0.10032273136516778 },
{"total_number_of_episodes":3170, "number_of_timesteps":3170000, "per_episode_reward":-781.17, "episode_reward_trend_value": 0.0005340171210301985, "biggest_recent_change": 0.1327892437382161 },
{"total_number_of_episodes":3180, "number_of_timesteps":3180000, "per_episode_reward":-781.07, "episode_reward_trend_value": 0.0009478732053796394, "biggest_recent_change": 0.1327892437382161 },
{"total_number_of_episodes":3190, "number_of_timesteps":3190000, "per_episode_reward":-781.06, "episode_reward_trend_value": 0.0005801924417230314, "biggest_recent_change": 0.1327892437382161 },
{"total_number_of_episodes":3200, "number_of_timesteps":3200000, "per_episode_reward":-780.92, "episode_reward_trend_value": 0.002033077774068362, "biggest_recent_change": 0.14598052795827243 },
{"total_number_of_episodes":3210, "number_of_timesteps":3210000, "per_episode_reward":-780.93, "episode_reward_trend_value": 0.001626568055319745, "biggest_recent_change": 0.14598052795827243 },
{"total_number_of_episodes":3220, "number_of_timesteps":3220000, "per_episode_reward":-780.69, "episode_reward_trend_value": 0.004248066732893423, "biggest_recent_change": 0.23886106499730886 },
{"total_number_of_episodes":3230, "number_of_timesteps":3230000, "per_episode_reward":-780.69, "episode_reward_trend_value": 0.004391579125294558, "biggest_recent_change": 0.23886106499730886 },
{"total_number_of_episodes":3240, "number_of_timesteps":3240000, "per_episode_reward":-780.75, "episode_reward_trend_value": 0.0042971252876087525, "biggest_recent_change": 0.23886106499730886 },
{"total_number_of_episodes":3250, "number_of_timesteps":3250000, "per_episode_reward":-780.84, "episode_reward_trend_value": 0.002219364632832013, "biggest_recent_change": 0.23886106499730886 },
{"total_number_of_episodes":3260, "number_of_timesteps":3260000, "per_episode_reward":-780.89, "episode_reward_trend_value": 0.0031095056922784653, "biggest_recent_change": 0.23886106499730886 },
{"total_number_of_episodes":3270, "number_of_timesteps":3270000, "per_episode_reward":-780.99, "episode_reward_trend_value": 0.0009485759799304106, "biggest_recent_change": 0.23886106499730886 },
{"total_number_of_episodes":3280, "number_of_timesteps":3280000, "per_episode_reward":-781.07, "episode_reward_trend_value": -8.278577874484148e-05, "biggest_recent_change": 0.23886106499730886 },
{"total_number_of_episodes":3290, "number_of_timesteps":3290000, "per_episode_reward":-780.94, "episode_reward_trend_value": -0.00027463620230668717, "biggest_recent_change": 0.23886106499730886 },
{"total_number_of_episodes":3300, "number_of_timesteps":3300000, "per_episode_reward":-780.98, "episode_reward_trend_value": -0.000558287395692029, "biggest_recent_change": 0.23886106499730886 },
{"total_number_of_episodes":3310, "number_of_timesteps":3310000, "per_episode_reward":-781.14, "episode_reward_trend_value": -0.004973296797171164, "biggest_recent_change": 0.15848978113581325 },
{"total_number_of_episodes":3320, "number_of_timesteps":3320000, "per_episode_reward":-781.09, "episode_reward_trend_value": -0.0044990806270258565, "biggest_recent_change": 0.15848978113581325 },
{"total_number_of_episodes":3330, "number_of_timesteps":3330000, "per_episode_reward":-780.99, "episode_reward_trend_value": -0.0026409685171819948, "biggest_recent_change": 0.15848978113581325 },
{"total_number_of_episodes":3340, "number_of_timesteps":3340000, "per_episode_reward":-781.09, "episode_reward_trend_value": -0.0027781927419128906, "biggest_recent_change": 0.15848978113581325 },
{"total_number_of_episodes":3350, "number_of_timesteps":3350000, "per_episode_reward":-780.91, "episode_reward_trend_value": -0.0002428801117402044, "biggest_recent_change": 0.1755015883275064 },
{"total_number_of_episodes":3360, "number_of_timesteps":3360000, "per_episode_reward":-780.98, "episode_reward_trend_value": 0.0001525692316982309, "biggest_recent_change": 0.1755015883275064 },
{"total_number_of_episodes":3370, "number_of_timesteps":3370000, "per_episode_reward":-781.10, "episode_reward_trend_value": -0.00032956301117160867, "biggest_recent_change": 0.1755015883275064 },
{"total_number_of_episodes":3380, "number_of_timesteps":3380000, "per_episode_reward":-781.00, "episode_reward_trend_value": -0.0006388158547173993, "biggest_recent_change": 0.1755015883275064 },
{"total_number_of_episodes":3390, "number_of_timesteps":3390000, "per_episode_reward":-780.96, "episode_reward_trend_value": 0.00018134855593719598, "biggest_recent_change": 0.1755015883275064 },
{"total_number_of_episodes":3400, "number_of_timesteps":3400000, "per_episode_reward":-780.85, "episode_reward_trend_value": 0.0032527537598146714, "biggest_recent_change": 0.1755015883275064 },
{"total_number_of_episodes":3410, "number_of_timesteps":3410000, "per_episode_reward":-780.72, "episode_reward_trend_value": 0.0040889296051078525, "biggest_recent_change": 0.1755015883275064 },
{"total_number_of_episodes":3420, "number_of_timesteps":3420000, "per_episode_reward":-780.60, "episode_reward_trend_value": 0.004381131369175111, "biggest_recent_change": 0.1755015883275064 },
{"total_number_of_episodes":3430, "number_of_timesteps":3430000, "per_episode_reward":-780.34, "episode_reward_trend_value": 0.00833851499514569, "biggest_recent_change": 0.25713861854683273 },
{"total_number_of_episodes":3440, "number_of_timesteps":3440000, "per_episode_reward":-780.31, "episode_reward_trend_value": 0.0066570395076003775, "biggest_recent_change": 0.25713861854683273 },
{"total_number_of_episodes":3450, "number_of_timesteps":3450000, "per_episode_reward":-780.29, "episode_reward_trend_value": 0.007643712675165818, "biggest_recent_change": 0.25713861854683273 },
{"total_number_of_episodes":3460, "number_of_timesteps":3460000, "per_episode_reward":-780.25, "episode_reward_trend_value": 0.009475983361891042, "biggest_recent_change": 0.25713861854683273 },
{"total_number_of_episodes":3470, "number_of_timesteps":3470000, "per_episode_reward":-780.25, "episode_reward_trend_value": 0.008376070825926894, "biggest_recent_change": 0.25713861854683273 },
{"total_number_of_episodes":3480, "number_of_timesteps":3480000, "per_episode_reward":-780.28, "episode_reward_trend_value": 0.007634911355820703, "biggest_recent_change": 0.25713861854683273 },
{"total_number_of_episodes":3490, "number_of_timesteps":3490000, "per_episode_reward":-780.38, "episode_reward_trend_value": 0.005177715074350746, "biggest_recent_change": 0.25713861854683273 },
{"total_number_of_episodes":3500, "number_of_timesteps":3500000, "per_episode_reward":-780.32, "episode_reward_trend_value": 0.004485621461141262, "biggest_recent_change": 0.25713861854683273 },
{"total_number_of_episodes":3510, "number_of_timesteps":3510000, "per_episode_reward":-780.23, "episode_reward_trend_value": 0.004072364722661506, "biggest_recent_change": 0.25713861854683273 },
{"total_number_of_episodes":3520, "number_of_timesteps":3520000, "per_episode_reward":-780.13, "episode_reward_trend_value": 0.0023639101587024118, "biggest_recent_change": 0.10337770779051425 },
{"total_number_of_episodes":3530, "number_of_timesteps":3530000, "per_episode_reward":-780.10, "episode_reward_trend_value": 0.0023785944749837046, "biggest_recent_change": 0.10337770779051425 },
{"total_number_of_episodes":3540, "number_of_timesteps":3540000, "per_episode_reward":-780.06, "episode_reward_trend_value": 0.0025245308933361027, "biggest_recent_change": 0.10337770779051425 },
{"total_number_of_episodes":3550, "number_of_timesteps":3550000, "per_episode_reward":-779.98, "episode_reward_trend_value": 0.002978302568712681, "biggest_recent_change": 0.10337770779051425 },
{"total_number_of_episodes":3560, "number_of_timesteps":3560000, "per_episode_reward":-779.84, "episode_reward_trend_value": 0.004552268298113581, "biggest_recent_change": 0.14354602132789296 },
{"total_number_of_episodes":3570, "number_of_timesteps":3570000, "per_episode_reward":-779.75, "episode_reward_trend_value": 0.005856135295236224, "biggest_recent_change": 0.14354602132789296 },
{"total_number_of_episodes":3580, "number_of_timesteps":3580000, "per_episode_reward":-779.88, "episode_reward_trend_value": 0.005516764749545979, "biggest_recent_change": 0.14354602132789296 },
{"total_number_of_episodes":3590, "number_of_timesteps":3590000, "per_episode_reward":-779.91, "episode_reward_trend_value": 0.004493260132508365, "biggest_recent_change": 0.14354602132789296 },
{"total_number_of_episodes":3600, "number_of_timesteps":3600000, "per_episode_reward":-779.87, "episode_reward_trend_value": 0.003993988961155992, "biggest_recent_change": 0.14354602132789296 },
{"total_number_of_episodes":3610, "number_of_timesteps":3610000, "per_episode_reward":-779.84, "episode_reward_trend_value": 0.003131172469661831, "biggest_recent_change": 0.14354602132789296 },
{"total_number_of_episodes":3620, "number_of_timesteps":3620000, "per_episode_reward":-779.89, "episode_reward_trend_value": 0.0023212402900197856, "biggest_recent_change": 0.14354602132789296 },
{"total_number_of_episodes":3630, "number_of_timesteps":3630000, "per_episode_reward":-779.83, "episode_reward_trend_value": 0.0026112128784739575, "biggest_recent_change": 0.14354602132789296 },
{"total_number_of_episodes":3640, "number_of_timesteps":3640000, "per_episode_reward":-779.79, "episode_reward_trend_value": 0.0021472825155759287, "biggest_recent_change": 0.14354602132789296 },
{"total_number_of_episodes":3650, "number_of_timesteps":3650000, "per_episode_reward":-779.66, "episode_reward_trend_value": 0.0019321785735782113, "biggest_recent_change": 0.13375432723125869 },
{"total_number_of_episodes":3660, "number_of_timesteps":3660000, "per_episode_reward":-779.55, "episode_reward_trend_value": 0.0021944141205873773, "biggest_recent_change": 0.13375432723125869 },
{"total_number_of_episodes":3670, "number_of_timesteps":3670000, "per_episode_reward":-779.56, "episode_reward_trend_value": 0.0035459917251185087, "biggest_recent_change": 0.1241866665480984 },
{"total_number_of_episodes":3680, "number_of_timesteps":3680000, "per_episode_reward":-779.58, "episode_reward_trend_value": 0.0037159960995558723, "biggest_recent_change": 0.1241866665480984 },
{"total_number_of_episodes":3690, "number_of_timesteps":3690000, "per_episode_reward":-779.62, "episode_reward_trend_value": 0.0027608007200860306, "biggest_recent_change": 0.1241866665480984 },
{"total_number_of_episodes":3700, "number_of_timesteps":3700000, "per_episode_reward":-779.61, "episode_reward_trend_value": 0.0025638334713789846, "biggest_recent_change": 0.1241866665480984 },
{"total_number_of_episodes":3710, "number_of_timesteps":3710000, "per_episode_reward":-779.40, "episode_reward_trend_value": 0.005500854957967425, "biggest_recent_change": 0.21692842053892036 },
{"total_number_of_episodes":3720, "number_of_timesteps":3720000, "per_episode_reward":-779.48, "episode_reward_trend_value": 0.003827395076643825, "biggest_recent_change": 0.21692842053892036 },
{"total_number_of_episodes":3730, "number_of_timesteps":3730000, "per_episode_reward":-779.49, "episode_reward_trend_value": 0.003332675758040043, "biggest_recent_change": 0.21692842053892036 },
{"total_number_of_episodes":3740, "number_of_timesteps":3740000, "per_episode_reward":-779.70, "episode_reward_trend_value": -0.00038627868689218304, "biggest_recent_change": 0.21692842053892036 },
{"total_number_of_episodes":3750, "number_of_timesteps":3750000, "per_episode_reward":-779.80, "episode_reward_trend_value": -0.002789095168743137, "biggest_recent_change": 0.21692842053892036 },
{"total_number_of_episodes":3760, "number_of_timesteps":3760000, "per_episode_reward":-779.69, "episode_reward_trend_value": -0.0013793783270153249, "biggest_recent_change": 0.21692842053892036 },
{"total_number_of_episodes":3770, "number_of_timesteps":3770000, "per_episode_reward":-779.65, "episode_reward_trend_value": -0.0007690560221337566, "biggest_recent_change": 0.21692842053892036 },
{"total_number_of_episodes":3780, "number_of_timesteps":3780000, "per_episode_reward":-779.66, "episode_reward_trend_value": -0.0004557275551999535, "biggest_recent_change": 0.21692842053892036 },
{"total_number_of_episodes":3790, "number_of_timesteps":3790000, "per_episode_reward":-779.64, "episode_reward_trend_value": -0.00024445115623545157, "biggest_recent_change": 0.21692842053892036 },
{"total_number_of_episodes":3800, "number_of_timesteps":3800000, "per_episode_reward":-779.77, "episode_reward_trend_value": -0.004128271262557822, "biggest_recent_change": 0.21051923349580193 },
{"total_number_of_episodes":3810, "number_of_timesteps":3810000, "per_episode_reward":-779.77, "episode_reward_trend_value": -0.0032624298926432656, "biggest_recent_change": 0.21051923349580193 },
{"total_number_of_episodes":3820, "number_of_timesteps":3820000, "per_episode_reward":-779.71, "episode_reward_trend_value": -0.0024606698675307195, "biggest_recent_change": 0.21051923349580193 },
{"total_number_of_episodes":3830, "number_of_timesteps":3830000, "per_episode_reward":-779.58, "episode_reward_trend_value": 0.0013250795419973353, "biggest_recent_change": 0.13261538903009296 },
{"total_number_of_episodes":3840, "number_of_timesteps":3840000, "per_episode_reward":-779.65, "episode_reward_trend_value": 0.0016982783888869965, "biggest_recent_change": 0.13261538903009296 },
{"total_number_of_episodes":3850, "number_of_timesteps":3850000, "per_episode_reward":-779.58, "episode_reward_trend_value": 0.001152909152952159, "biggest_recent_change": 0.13261538903009296 },
{"total_number_of_episodes":3860, "number_of_timesteps":3860000, "per_episode_reward":-779.72, "episode_reward_trend_value": -0.0007424469076782871, "biggest_recent_change": 0.13261538903009296 },
{"total_number_of_episodes":3870, "number_of_timesteps":3870000, "per_episode_reward":-779.59, "episode_reward_trend_value": 0.0008156873460342872, "biggest_recent_change": 0.13261538903009296 },
{"total_number_of_episodes":3880, "number_of_timesteps":3880000, "per_episode_reward":-779.65, "episode_reward_trend_value": -0.00013698266974415674, "biggest_recent_change": 0.13261538903009296 },
{"total_number_of_episodes":3890, "number_of_timesteps":3890000, "per_episode_reward":-779.66, "episode_reward_trend_value": 0.0012150762201372344, "biggest_recent_change": 0.13122798701181182 },
{"total_number_of_episodes":3900, "number_of_timesteps":3900000, "per_episode_reward":-779.62, "episode_reward_trend_value": 0.0017336719445552261, "biggest_recent_change": 0.13122798701181182 },
{"total_number_of_episodes":3910, "number_of_timesteps":3910000, "per_episode_reward":-779.56, "episode_reward_trend_value": 0.0016312062156708233, "biggest_recent_change": 0.13122798701181182 },
{"total_number_of_episodes":3920, "number_of_timesteps":3920000, "per_episode_reward":-779.57, "episode_reward_trend_value": 9.362968929862189e-05, "biggest_recent_change": 0.13122798701181182 },
{"total_number_of_episodes":3930, "number_of_timesteps":3930000, "per_episode_reward":-779.56, "episode_reward_trend_value": 0.0010296411060216491, "biggest_recent_change": 0.13122798701181182 },
{"total_number_of_episodes":3940, "number_of_timesteps":3940000, "per_episode_reward":-779.50, "episode_reward_trend_value": 0.0009788791601686676, "biggest_recent_change": 0.13122798701181182 },
{"total_number_of_episodes":3950, "number_of_timesteps":3950000, "per_episode_reward":-779.45, "episode_reward_trend_value": 0.0029181677343507947, "biggest_recent_change": 0.1266435197312603 },
Hit early stopping because biggest_recent_change: 0.0665569060222424 < 0.1
{"total_number_of_episodes":3960, "number_of_timesteps":3960000, "per_episode_reward":-779.52, "episode_reward_trend_value": 0.0007714963370896537, "biggest_recent_change": 0.0665569060222424 },




Process Process-6:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 16:51:10,850][0m Trial 0 finished with value: -779.44498588077 and parameters: {'learning_rate': 1, 'variance_scaling_factor': 0, 'permaban_threshold': 1}. Best is trial 0 with value: -779.44498588077.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-800.96, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-782.26, "episode_reward_trend_value": 1.8694338849003542, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-776.88, "episode_reward_trend_value": 1.2041082188869552, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-771.42, "episode_reward_trend_value": 0.9847182581122146, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-780.35, "episode_reward_trend_value": 0.5153044715807823, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-787.18, "episode_reward_trend_value": 0.27559872140856667, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-789.93, "episode_reward_trend_value": 0.18380134791623087, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-793.16, "episode_reward_trend_value": 0.11141675906809334, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-795.51, "episode_reward_trend_value": 0.06812555131436256, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-801.34, "episode_reward_trend_value": -0.0041796031326132126, "biggest_recent_change": 18.694338849003543 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-795.26, "episode_reward_trend_value": -0.14441936843087938, "biggest_recent_change": 8.929368880135144 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-791.32, "episode_reward_trend_value": -0.16050580673971301, "biggest_recent_change": 8.929368880135144 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-793.59, "episode_reward_trend_value": -0.24631879604119528, "biggest_recent_change": 8.929368880135144 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-796.16, "episode_reward_trend_value": -0.17573072753935373, "biggest_recent_change": 6.832242792802958 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-797.68, "episode_reward_trend_value": -0.11666918899616878, "biggest_recent_change": 6.072759972159588 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-800.92, "episode_reward_trend_value": -0.12208480220281344, "biggest_recent_change": 6.072759972159588 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-797.95, "episode_reward_trend_value": -0.05323110153811589, "biggest_recent_change": 6.072759972159588 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-794.95, "episode_reward_trend_value": 0.006209852253673236, "biggest_recent_change": 6.072759972159588 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-794.63, "episode_reward_trend_value": 0.07454990915767262, "biggest_recent_change": 6.072759972159588 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-794.22, "episode_reward_trend_value": 0.01155832165393728, "biggest_recent_change": 3.940046080940533 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-796.41, "episode_reward_trend_value": -0.05650420547512895, "biggest_recent_change": 3.2392603840525 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-793.28, "episode_reward_trend_value": 0.003402835087644639, "biggest_recent_change": 3.2392603840525 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-795.87, "episode_reward_trend_value": 0.0032103415108571827, "biggest_recent_change": 3.2392603840525 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-794.05, "episode_reward_trend_value": 0.040327506367585406, "biggest_recent_change": 3.2392603840525 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-793.62, "episode_reward_trend_value": 0.08104680508907146, "biggest_recent_change": 3.127847979143553 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-792.60, "episode_reward_trend_value": 0.05943201662810098, "biggest_recent_change": 3.127847979143553 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-795.24, "episode_reward_trend_value": -0.0032599944725158515, "biggest_recent_change": 3.127847979143553 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-793.30, "episode_reward_trend_value": 0.014758407097084123, "biggest_recent_change": 3.127847979143553 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-794.29, "episode_reward_trend_value": -0.0007754784321251564, "biggest_recent_change": 3.127847979143553 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-794.26, "episode_reward_trend_value": 0.02384980386111566, "biggest_recent_change": 3.127847979143553 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-795.32, "episode_reward_trend_value": -0.02267102772052946, "biggest_recent_change": 2.6417241874120236 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-793.89, "episode_reward_trend_value": 0.021994891503471888, "biggest_recent_change": 2.6417241874120236 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-792.87, "episode_reward_trend_value": 0.013068874130653463, "biggest_recent_change": 2.6417241874120236 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-794.44, "episode_reward_trend_value": -0.009015478186350062, "biggest_recent_change": 2.6417241874120236 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-795.05, "episode_reward_trend_value": -0.027167492900582603, "biggest_recent_change": 2.6417241874120236 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-795.98, "episode_reward_trend_value": -0.008152352600454075, "biggest_recent_change": 1.946052875539749 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-796.67, "episode_reward_trend_value": -0.037471939080959756, "biggest_recent_change": 1.5621152076490716 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-795.49, "episode_reward_trend_value": -0.013257092931032981, "biggest_recent_change": 1.5621152076490716 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-794.47, "episode_reward_trend_value": -0.00232770408334141, "biggest_recent_change": 1.5621152076490716 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-795.05, "episode_reward_trend_value": 0.003022975088300781, "biggest_recent_change": 1.5621152076490716 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-794.08, "episode_reward_trend_value": -0.0020981944727143146, "biggest_recent_change": 1.5621152076490716 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-794.57, "episode_reward_trend_value": -0.018825705385996924, "biggest_recent_change": 1.5621152076490716 },
{"total_number_of_episodes":440, "number_of_timesteps":440000, "per_episode_reward":-794.18, "episode_reward_trend_value": 0.0028766845077408715, "biggest_recent_change": 1.1848035526879812 },
{"total_number_of_episodes":450, "number_of_timesteps":450000, "per_episode_reward":-794.48, "episode_reward_trend_value": 0.006303101937829877, "biggest_recent_change": 1.1848035526879812 },
{"total_number_of_episodes":460, "number_of_timesteps":460000, "per_episode_reward":-793.50, "episode_reward_trend_value": 0.027546171154680603, "biggest_recent_change": 1.1848035526879812 },
{"total_number_of_episodes":470, "number_of_timesteps":470000, "per_episode_reward":-793.94, "episode_reward_trend_value": 0.030387230973043543, "biggest_recent_change": 1.1848035526879812 },
{"total_number_of_episodes":480, "number_of_timesteps":480000, "per_episode_reward":-792.80, "episode_reward_trend_value": 0.029819915930503767, "biggest_recent_change": 1.1337451988594012 },
{"total_number_of_episodes":490, "number_of_timesteps":490000, "per_episode_reward":-792.70, "episode_reward_trend_value": 0.01969906847197712, "biggest_recent_change": 1.1337451988594012 },
{"total_number_of_episodes":500, "number_of_timesteps":500000, "per_episode_reward":-792.45, "episode_reward_trend_value": 0.02887458861093819, "biggest_recent_change": 1.1337451988594012 },
{"total_number_of_episodes":510, "number_of_timesteps":510000, "per_episode_reward":-792.08, "episode_reward_trend_value": 0.022235170582531636, "biggest_recent_change": 1.1337451988594012 },
{"total_number_of_episodes":520, "number_of_timesteps":520000, "per_episode_reward":-792.51, "episode_reward_trend_value": 0.022883387021811863, "biggest_recent_change": 1.1337451988594012 },
{"total_number_of_episodes":530, "number_of_timesteps":530000, "per_episode_reward":-793.14, "episode_reward_trend_value": 0.01153430483382686, "biggest_recent_change": 1.1337451988594012 },
{"total_number_of_episodes":540, "number_of_timesteps":540000, "per_episode_reward":-793.26, "episode_reward_trend_value": 0.013554033361583607, "biggest_recent_change": 1.1337451988594012 },
{"total_number_of_episodes":550, "number_of_timesteps":550000, "per_episode_reward":-794.55, "episode_reward_trend_value": -0.011653996170841513, "biggest_recent_change": 1.2872079888021517 },
{"total_number_of_episodes":560, "number_of_timesteps":560000, "per_episode_reward":-793.23, "episode_reward_trend_value": 0.007836270434577524, "biggest_recent_change": 1.317109470434616 },
{"total_number_of_episodes":570, "number_of_timesteps":570000, "per_episode_reward":-793.89, "episode_reward_trend_value": -0.012059739845776424, "biggest_recent_change": 1.317109470434616 },
{"total_number_of_episodes":580, "number_of_timesteps":580000, "per_episode_reward":-794.04, "episode_reward_trend_value": -0.014870698826425471, "biggest_recent_change": 1.317109470434616 },
{"total_number_of_episodes":590, "number_of_timesteps":590000, "per_episode_reward":-793.33, "episode_reward_trend_value": -0.009758609069265529, "biggest_recent_change": 1.317109470434616 },
{"total_number_of_episodes":600, "number_of_timesteps":600000, "per_episode_reward":-792.80, "episode_reward_trend_value": -0.008001380776057785, "biggest_recent_change": 1.317109470434616 },
{"total_number_of_episodes":610, "number_of_timesteps":610000, "per_episode_reward":-792.34, "episode_reward_trend_value": 0.0018434733468476832, "biggest_recent_change": 1.317109470434616 },
{"total_number_of_episodes":620, "number_of_timesteps":620000, "per_episode_reward":-792.55, "episode_reward_trend_value": 0.006548970104993007, "biggest_recent_change": 1.317109470434616 },
{"total_number_of_episodes":630, "number_of_timesteps":630000, "per_episode_reward":-793.12, "episode_reward_trend_value": 0.0015070026663238424, "biggest_recent_change": 1.317109470434616 },
{"total_number_of_episodes":640, "number_of_timesteps":640000, "per_episode_reward":-792.75, "episode_reward_trend_value": 0.019974030527825766, "biggest_recent_change": 1.317109470434616 },
{"total_number_of_episodes":650, "number_of_timesteps":650000, "per_episode_reward":-792.74, "episode_reward_trend_value": 0.005410441843927553, "biggest_recent_change": 0.7084191528941801 },
{"total_number_of_episodes":660, "number_of_timesteps":660000, "per_episode_reward":-791.51, "episode_reward_trend_value": 0.02643001294004661, "biggest_recent_change": 1.2348656722782607 },
{"total_number_of_episodes":670, "number_of_timesteps":670000, "per_episode_reward":-790.92, "episode_reward_trend_value": 0.034644551092285134, "biggest_recent_change": 1.2348656722782607 },
{"total_number_of_episodes":680, "number_of_timesteps":680000, "per_episode_reward":-790.03, "episode_reward_trend_value": 0.03666969976537884, "biggest_recent_change": 1.2348656722782607 },
{"total_number_of_episodes":690, "number_of_timesteps":690000, "per_episode_reward":-790.02, "episode_reward_trend_value": 0.030945934634025386, "biggest_recent_change": 1.2348656722782607 },
{"total_number_of_episodes":700, "number_of_timesteps":700000, "per_episode_reward":-789.14, "episode_reward_trend_value": 0.03562511250870052, "biggest_recent_change": 1.2348656722782607 },
{"total_number_of_episodes":710, "number_of_timesteps":710000, "per_episode_reward":-789.65, "episode_reward_trend_value": 0.03225338579368958, "biggest_recent_change": 1.2348656722782607 },
{"total_number_of_episodes":720, "number_of_timesteps":720000, "per_episode_reward":-789.69, "episode_reward_trend_value": 0.038157884430892054, "biggest_recent_change": 1.2348656722782607 },
{"total_number_of_episodes":730, "number_of_timesteps":730000, "per_episode_reward":-789.71, "episode_reward_trend_value": 0.0338097400007137, "biggest_recent_change": 1.2348656722782607 },
{"total_number_of_episodes":740, "number_of_timesteps":740000, "per_episode_reward":-790.73, "episode_reward_trend_value": 0.022403043081670956, "biggest_recent_change": 1.2348656722782607 },
{"total_number_of_episodes":750, "number_of_timesteps":750000, "per_episode_reward":-791.31, "episode_reward_trend_value": 0.002212151094052691, "biggest_recent_change": 1.0202162338300695 },
{"total_number_of_episodes":760, "number_of_timesteps":760000, "per_episode_reward":-791.54, "episode_reward_trend_value": -0.006919511803036788, "biggest_recent_change": 1.0202162338300695 },
{"total_number_of_episodes":770, "number_of_timesteps":770000, "per_episode_reward":-791.57, "episode_reward_trend_value": -0.017156132550868126, "biggest_recent_change": 1.0202162338300695 },
{"total_number_of_episodes":780, "number_of_timesteps":780000, "per_episode_reward":-792.83, "episode_reward_trend_value": -0.03126498167482118, "biggest_recent_change": 1.2590720263569892 },
{"total_number_of_episodes":790, "number_of_timesteps":790000, "per_episode_reward":-794.31, "episode_reward_trend_value": -0.057441826385119535, "biggest_recent_change": 1.4753906971692459 },
{"total_number_of_episodes":800, "number_of_timesteps":800000, "per_episode_reward":-794.04, "episode_reward_trend_value": -0.048842065124685835, "biggest_recent_change": 1.4753906971692459 },
{"total_number_of_episodes":810, "number_of_timesteps":810000, "per_episode_reward":-793.65, "episode_reward_trend_value": -0.044042700337284815, "biggest_recent_change": 1.4753906971692459 },
{"total_number_of_episodes":820, "number_of_timesteps":820000, "per_episode_reward":-793.62, "episode_reward_trend_value": -0.04344009794874258, "biggest_recent_change": 1.4753906971692459 },
{"total_number_of_episodes":830, "number_of_timesteps":830000, "per_episode_reward":-794.62, "episode_reward_trend_value": -0.04324310524965792, "biggest_recent_change": 1.4753906971692459 },
{"total_number_of_episodes":840, "number_of_timesteps":840000, "per_episode_reward":-794.13, "episode_reward_trend_value": -0.0313868680569903, "biggest_recent_change": 1.4753906971692459 },
{"total_number_of_episodes":850, "number_of_timesteps":850000, "per_episode_reward":-793.63, "episode_reward_trend_value": -0.023159521955151124, "biggest_recent_change": 1.4753906971692459 },
{"total_number_of_episodes":860, "number_of_timesteps":860000, "per_episode_reward":-794.12, "episode_reward_trend_value": -0.028364226956667453, "biggest_recent_change": 1.4753906971692459 },
{"total_number_of_episodes":870, "number_of_timesteps":870000, "per_episode_reward":-794.12, "episode_reward_trend_value": -0.014290945065216772, "biggest_recent_change": 1.4753906971692459 },
{"total_number_of_episodes":880, "number_of_timesteps":880000, "per_episode_reward":-792.86, "episode_reward_trend_value": 0.016100026250391388, "biggest_recent_change": 1.2597967212354888 },
{"total_number_of_episodes":890, "number_of_timesteps":890000, "per_episode_reward":-792.33, "episode_reward_trend_value": 0.019075326723736907, "biggest_recent_change": 1.2597967212354888 },
{"total_number_of_episodes":900, "number_of_timesteps":900000, "per_episode_reward":-792.82, "episode_reward_trend_value": 0.009242962911731763, "biggest_recent_change": 1.2597967212354888 },
{"total_number_of_episodes":910, "number_of_timesteps":910000, "per_episode_reward":-793.22, "episode_reward_trend_value": 0.004434748234453107, "biggest_recent_change": 1.2597967212354888 },
{"total_number_of_episodes":920, "number_of_timesteps":920000, "per_episode_reward":-792.39, "episode_reward_trend_value": 0.02474198894317649, "biggest_recent_change": 1.2597967212354888 },
{"total_number_of_episodes":930, "number_of_timesteps":930000, "per_episode_reward":-791.59, "episode_reward_trend_value": 0.028290466883326343, "biggest_recent_change": 1.2597967212354888 },
{"total_number_of_episodes":940, "number_of_timesteps":940000, "per_episode_reward":-791.96, "episode_reward_trend_value": 0.018476470015170637, "biggest_recent_change": 1.2597967212354888 },
{"total_number_of_episodes":950, "number_of_timesteps":950000, "per_episode_reward":-791.95, "episode_reward_trend_value": 0.024160962549728993, "biggest_recent_change": 1.2597967212354888 },
{"total_number_of_episodes":960, "number_of_timesteps":960000, "per_episode_reward":-792.24, "episode_reward_trend_value": 0.02082077098056693, "biggest_recent_change": 1.2597967212354888 },
{"total_number_of_episodes":970, "number_of_timesteps":970000, "per_episode_reward":-792.39, "episode_reward_trend_value": 0.005161532041836381, "biggest_recent_change": 0.8251647728726539 },
{"total_number_of_episodes":980, "number_of_timesteps":980000, "per_episode_reward":-792.11, "episode_reward_trend_value": 0.0023551606205362277, "biggest_recent_change": 0.8251647728726539 },
{"total_number_of_episodes":990, "number_of_timesteps":990000, "per_episode_reward":-791.38, "episode_reward_trend_value": 0.016026412203451752, "biggest_recent_change": 0.8251647728726539 },
{"total_number_of_episodes":1000, "number_of_timesteps":1000000, "per_episode_reward":-790.96, "episode_reward_trend_value": 0.02511767772327984, "biggest_recent_change": 0.8251647728726539 },
{"total_number_of_episodes":1010, "number_of_timesteps":1010000, "per_episode_reward":-790.10, "episode_reward_trend_value": 0.025515210534260608, "biggest_recent_change": 0.860942725860923 },
{"total_number_of_episodes":1020, "number_of_timesteps":1020000, "per_episode_reward":-789.74, "episode_reward_trend_value": 0.020492739403955008, "biggest_recent_change": 0.860942725860923 },
{"total_number_of_episodes":1030, "number_of_timesteps":1030000, "per_episode_reward":-789.85, "episode_reward_trend_value": 0.023493001262536634, "biggest_recent_change": 0.860942725860923 },
{"total_number_of_episodes":1040, "number_of_timesteps":1040000, "per_episode_reward":-789.49, "episode_reward_trend_value": 0.027301577918520552, "biggest_recent_change": 0.860942725860923 },
{"total_number_of_episodes":1050, "number_of_timesteps":1050000, "per_episode_reward":-789.14, "episode_reward_trend_value": 0.03448581812122787, "biggest_recent_change": 0.860942725860923 },
{"total_number_of_episodes":1060, "number_of_timesteps":1060000, "per_episode_reward":-789.26, "episode_reward_trend_value": 0.034813844124767, "biggest_recent_change": 0.860942725860923 },
{"total_number_of_episodes":1070, "number_of_timesteps":1070000, "per_episode_reward":-789.53, "episode_reward_trend_value": 0.02866928436949618, "biggest_recent_change": 0.860942725860923 },
{"total_number_of_episodes":1080, "number_of_timesteps":1080000, "per_episode_reward":-789.22, "episode_reward_trend_value": 0.02404218793865943, "biggest_recent_change": 0.860942725860923 },
{"total_number_of_episodes":1090, "number_of_timesteps":1090000, "per_episode_reward":-789.54, "episode_reward_trend_value": 0.01574076770581693, "biggest_recent_change": 0.860942725860923 },
{"total_number_of_episodes":1100, "number_of_timesteps":1100000, "per_episode_reward":-710.98, "episode_reward_trend_value": 0.8790748373348126, "biggest_recent_change": 78.56100899247053 },
{"total_number_of_episodes":1110, "number_of_timesteps":1110000, "per_episode_reward":-710.68, "episode_reward_trend_value": 0.8785147821020916, "biggest_recent_change": 78.56100899247053 },
{"total_number_of_episodes":1120, "number_of_timesteps":1120000, "per_episode_reward":-710.97, "episode_reward_trend_value": 0.8764522646742332, "biggest_recent_change": 78.56100899247053 },
{"total_number_of_episodes":1130, "number_of_timesteps":1130000, "per_episode_reward":-710.66, "episode_reward_trend_value": 0.8758941321210424, "biggest_recent_change": 78.56100899247053 },
{"total_number_of_episodes":1140, "number_of_timesteps":1140000, "per_episode_reward":-710.58, "episode_reward_trend_value": 0.8728722920188753, "biggest_recent_change": 78.56100899247053 },
{"total_number_of_episodes":1150, "number_of_timesteps":1150000, "per_episode_reward":-710.96, "episode_reward_trend_value": 0.8700382187919482, "biggest_recent_change": 78.56100899247053 },
{"total_number_of_episodes":1160, "number_of_timesteps":1160000, "per_episode_reward":-711.05, "episode_reward_trend_value": 0.8720176073467719, "biggest_recent_change": 78.56100899247053 },
{"total_number_of_episodes":1170, "number_of_timesteps":1170000, "per_episode_reward":-711.28, "episode_reward_trend_value": 0.8660010692308877, "biggest_recent_change": 78.56100899247053 },
{"total_number_of_episodes":1180, "number_of_timesteps":1180000, "per_episode_reward":-710.82, "episode_reward_trend_value": 0.8746975764653371, "biggest_recent_change": 78.56100899247053 },
{"total_number_of_episodes":1190, "number_of_timesteps":1190000, "per_episode_reward":-710.67, "episode_reward_trend_value": 0.0033760348291518006, "biggest_recent_change": 0.45875814095984424 },
{"total_number_of_episodes":1200, "number_of_timesteps":1200000, "per_episode_reward":-710.55, "episode_reward_trend_value": 0.0014415025213060694, "biggest_recent_change": 0.45875814095984424 },
{"total_number_of_episodes":1210, "number_of_timesteps":1210000, "per_episode_reward":-710.74, "episode_reward_trend_value": 0.0024803968138068437, "biggest_recent_change": 0.45875814095984424 },
{"total_number_of_episodes":1220, "number_of_timesteps":1220000, "per_episode_reward":-710.10, "episode_reward_trend_value": 0.006222182324220032, "biggest_recent_change": 0.6418682093301413 },
{"total_number_of_episodes":1230, "number_of_timesteps":1230000, "per_episode_reward":-710.19, "episode_reward_trend_value": 0.0043088155382179135, "biggest_recent_change": 0.6418682093301413 },
{"total_number_of_episodes":1240, "number_of_timesteps":1240000, "per_episode_reward":-710.09, "episode_reward_trend_value": 0.009645314683402578, "biggest_recent_change": 0.6418682093301413 },
{"total_number_of_episodes":1250, "number_of_timesteps":1250000, "per_episode_reward":-710.17, "episode_reward_trend_value": 0.009757115554581712, "biggest_recent_change": 0.6418682093301413 },
{"total_number_of_episodes":1260, "number_of_timesteps":1260000, "per_episode_reward":-710.46, "episode_reward_trend_value": 0.009064620635821382, "biggest_recent_change": 0.6418682093301413 },
{"total_number_of_episodes":1270, "number_of_timesteps":1270000, "per_episode_reward":-710.52, "episode_reward_trend_value": 0.0032457568046589183, "biggest_recent_change": 0.6418682093301413 },
{"total_number_of_episodes":1280, "number_of_timesteps":1280000, "per_episode_reward":-710.10, "episode_reward_trend_value": 0.006346690415696482, "biggest_recent_change": 0.6418682093301413 },
{"total_number_of_episodes":1290, "number_of_timesteps":1290000, "per_episode_reward":-709.75, "episode_reward_trend_value": 0.008804295227999621, "biggest_recent_change": 0.6418682093301413 },
{"total_number_of_episodes":1300, "number_of_timesteps":1300000, "per_episode_reward":-709.71, "episode_reward_trend_value": 0.011455640785749842, "biggest_recent_change": 0.6418682093301413 },
{"total_number_of_episodes":1310, "number_of_timesteps":1310000, "per_episode_reward":-709.55, "episode_reward_trend_value": 0.006133266618485322, "biggest_recent_change": 0.4211542702072393 },
{"total_number_of_episodes":1320, "number_of_timesteps":1320000, "per_episode_reward":-709.36, "episode_reward_trend_value": 0.009208500387209521, "biggest_recent_change": 0.4211542702072393 },
{"total_number_of_episodes":1330, "number_of_timesteps":1330000, "per_episode_reward":-709.73, "episode_reward_trend_value": 0.003998029916102849, "biggest_recent_change": 0.4211542702072393 },
{"total_number_of_episodes":1340, "number_of_timesteps":1340000, "per_episode_reward":-709.52, "episode_reward_trend_value": 0.007281155380823951, "biggest_recent_change": 0.4211542702072393 },
{"total_number_of_episodes":1350, "number_of_timesteps":1350000, "per_episode_reward":-709.73, "episode_reward_trend_value": 0.0081024904068272, "biggest_recent_change": 0.4211542702072393 },
{"total_number_of_episodes":1360, "number_of_timesteps":1360000, "per_episode_reward":-709.60, "episode_reward_trend_value": 0.010246225256514663, "biggest_recent_change": 0.4211542702072393 },
{"total_number_of_episodes":1370, "number_of_timesteps":1370000, "per_episode_reward":-709.33, "episode_reward_trend_value": 0.008558339545488201, "biggest_recent_change": 0.36373645268815835 },
{"total_number_of_episodes":1380, "number_of_timesteps":1380000, "per_episode_reward":-709.27, "episode_reward_trend_value": 0.005382952973393963, "biggest_recent_change": 0.36373645268815835 },
{"total_number_of_episodes":1390, "number_of_timesteps":1390000, "per_episode_reward":-709.58, "episode_reward_trend_value": 0.0015245993631133894, "biggest_recent_change": 0.36373645268815835 },
{"total_number_of_episodes":1400, "number_of_timesteps":1400000, "per_episode_reward":-709.44, "episode_reward_trend_value": 0.001271566090614821, "biggest_recent_change": 0.36373645268815835 },
{"total_number_of_episodes":1410, "number_of_timesteps":1410000, "per_episode_reward":-709.55, "episode_reward_trend_value": -0.002062936774355724, "biggest_recent_change": 0.36373645268815835 },
{"total_number_of_episodes":1420, "number_of_timesteps":1420000, "per_episode_reward":-709.61, "episode_reward_trend_value": 0.001273547787540489, "biggest_recent_change": 0.3055965731599599 },
{"total_number_of_episodes":1430, "number_of_timesteps":1430000, "per_episode_reward":-709.73, "episode_reward_trend_value": -0.0023424688051780626, "biggest_recent_change": 0.3055965731599599 },
{"total_number_of_episodes":1440, "number_of_timesteps":1440000, "per_episode_reward":-709.91, "episode_reward_trend_value": -0.001963926386275085, "biggest_recent_change": 0.3055965731599599 },
{"total_number_of_episodes":1450, "number_of_timesteps":1450000, "per_episode_reward":-709.73, "episode_reward_trend_value": -0.0013841774622543805, "biggest_recent_change": 0.3055965731599599 },
{"total_number_of_episodes":1460, "number_of_timesteps":1460000, "per_episode_reward":-709.88, "episode_reward_trend_value": -0.0060364957574228755, "biggest_recent_change": 0.3055965731599599 },
{"total_number_of_episodes":1470, "number_of_timesteps":1470000, "per_episode_reward":-709.88, "episode_reward_trend_value": -0.006774679140364191, "biggest_recent_change": 0.3055965731599599 },
{"total_number_of_episodes":1480, "number_of_timesteps":1480000, "per_episode_reward":-709.97, "episode_reward_trend_value": -0.0043851011965405935, "biggest_recent_change": 0.18017393578895735 },
{"total_number_of_episodes":1490, "number_of_timesteps":1490000, "per_episode_reward":-710.48, "episode_reward_trend_value": -0.0116386085313214, "biggest_recent_change": 0.5127341203788092 },
{"total_number_of_episodes":1500, "number_of_timesteps":1500000, "per_episode_reward":-710.16, "episode_reward_trend_value": -0.006783417561675581, "biggest_recent_change": 0.5127341203788092 },
{"total_number_of_episodes":1510, "number_of_timesteps":1510000, "per_episode_reward":-710.20, "episode_reward_trend_value": -0.006539333232636738, "biggest_recent_change": 0.5127341203788092 },
{"total_number_of_episodes":1520, "number_of_timesteps":1520000, "per_episode_reward":-710.20, "episode_reward_trend_value": -0.0052269537010804215, "biggest_recent_change": 0.5127341203788092 },
{"total_number_of_episodes":1530, "number_of_timesteps":1530000, "per_episode_reward":-709.92, "episode_reward_trend_value": -0.00013916418457837507, "biggest_recent_change": 0.5127341203788092 },
{"total_number_of_episodes":1540, "number_of_timesteps":1540000, "per_episode_reward":-710.17, "episode_reward_trend_value": -0.004917813975637526, "biggest_recent_change": 0.5127341203788092 },
{"total_number_of_episodes":1550, "number_of_timesteps":1550000, "per_episode_reward":-709.95, "episode_reward_trend_value": -0.0008654896879938355, "biggest_recent_change": 0.5127341203788092 },
{"total_number_of_episodes":1560, "number_of_timesteps":1560000, "per_episode_reward":-710.18, "episode_reward_trend_value": -0.0033251405647774846, "biggest_recent_change": 0.5127341203788092 },
{"total_number_of_episodes":1570, "number_of_timesteps":1570000, "per_episode_reward":-709.77, "episode_reward_trend_value": 0.0022360047390419724, "biggest_recent_change": 0.5127341203788092 },
{"total_number_of_episodes":1580, "number_of_timesteps":1580000, "per_episode_reward":-709.93, "episode_reward_trend_value": 0.006185311967412114, "biggest_recent_change": 0.409968519127915 },
{"total_number_of_episodes":1590, "number_of_timesteps":1590000, "per_episode_reward":-709.49, "episode_reward_trend_value": 0.007497085720897051, "biggest_recent_change": 0.4410117073770152 },
{"total_number_of_episodes":1600, "number_of_timesteps":1600000, "per_episode_reward":-709.76, "episode_reward_trend_value": 0.0049528019413464285, "biggest_recent_change": 0.4410117073770152 },
{"total_number_of_episodes":1610, "number_of_timesteps":1610000, "per_episode_reward":-709.29, "episode_reward_trend_value": 0.010069528567695822, "biggest_recent_change": 0.4627599409315053 },
{"total_number_of_episodes":1620, "number_of_timesteps":1620000, "per_episode_reward":-709.73, "episode_reward_trend_value": 0.002129064881028676, "biggest_recent_change": 0.4627599409315053 },
{"total_number_of_episodes":1630, "number_of_timesteps":1630000, "per_episode_reward":-710.09, "episode_reward_trend_value": 0.000909598546921744, "biggest_recent_change": 0.4627599409315053 },
{"total_number_of_episodes":1640, "number_of_timesteps":1640000, "per_episode_reward":-709.63, "episode_reward_trend_value": 0.003626603676602574, "biggest_recent_change": 0.4627599409315053 },
{"total_number_of_episodes":1650, "number_of_timesteps":1650000, "per_episode_reward":-709.82, "episode_reward_trend_value": 0.00394583227801301, "biggest_recent_change": 0.4627599409315053 },
{"total_number_of_episodes":1660, "number_of_timesteps":1660000, "per_episode_reward":-709.57, "episode_reward_trend_value": 0.0021985882983130978, "biggest_recent_change": 0.4627599409315053 },
{"total_number_of_episodes":1670, "number_of_timesteps":1670000, "per_episode_reward":-709.30, "episode_reward_trend_value": 0.007015612876248018, "biggest_recent_change": 0.4627599409315053 },
{"total_number_of_episodes":1680, "number_of_timesteps":1680000, "per_episode_reward":-709.75, "episode_reward_trend_value": -0.002882601530587989, "biggest_recent_change": 0.4627599409315053 },
{"total_number_of_episodes":1690, "number_of_timesteps":1690000, "per_episode_reward":-709.95, "episode_reward_trend_value": -0.00211639941111975, "biggest_recent_change": 0.4627599409315053 },
{"total_number_of_episodes":1700, "number_of_timesteps":1700000, "per_episode_reward":-709.80, "episode_reward_trend_value": -0.005611708478719871, "biggest_recent_change": 0.45977555720889995 },
{"total_number_of_episodes":1710, "number_of_timesteps":1710000, "per_episode_reward":-709.46, "episode_reward_trend_value": 0.0030233761193913878, "biggest_recent_change": 0.45977555720889995 },
{"total_number_of_episodes":1720, "number_of_timesteps":1720000, "per_episode_reward":-709.60, "episode_reward_trend_value": 0.0054569784169441644, "biggest_recent_change": 0.45977555720889995 },
{"total_number_of_episodes":1730, "number_of_timesteps":1730000, "per_episode_reward":-709.42, "episode_reward_trend_value": 0.0022783665148406523, "biggest_recent_change": 0.4498275892382253 },
{"total_number_of_episodes":1740, "number_of_timesteps":1740000, "per_episode_reward":-709.61, "episode_reward_trend_value": 0.002365094598164635, "biggest_recent_change": 0.4498275892382253 },
{"total_number_of_episodes":1750, "number_of_timesteps":1750000, "per_episode_reward":-709.54, "episode_reward_trend_value": 0.0003545955993003089, "biggest_recent_change": 0.4498275892382253 },
{"total_number_of_episodes":1760, "number_of_timesteps":1760000, "per_episode_reward":-709.56, "episode_reward_trend_value": -0.002954765581992231, "biggest_recent_change": 0.4498275892382253 },
{"total_number_of_episodes":1770, "number_of_timesteps":1770000, "per_episode_reward":-710.14, "episode_reward_trend_value": -0.004427612464722744, "biggest_recent_change": 0.5823838086839714 },
{"total_number_of_episodes":1780, "number_of_timesteps":1780000, "per_episode_reward":-710.15, "episode_reward_trend_value": -0.0022574142762424444, "biggest_recent_change": 0.5823838086839714 },
{"total_number_of_episodes":1790, "number_of_timesteps":1790000, "per_episode_reward":-709.97, "episode_reward_trend_value": -0.001869115557550079, "biggest_recent_change": 0.5823838086839714 },
{"total_number_of_episodes":1800, "number_of_timesteps":1800000, "per_episode_reward":-709.85, "episode_reward_trend_value": -0.004407614829640099, "biggest_recent_change": 0.5823838086839714 },
{"total_number_of_episodes":1810, "number_of_timesteps":1810000, "per_episode_reward":-710.09, "episode_reward_trend_value": -0.005483410907111698, "biggest_recent_change": 0.5823838086839714 },
{"total_number_of_episodes":1820, "number_of_timesteps":1820000, "per_episode_reward":-710.47, "episode_reward_trend_value": -0.0116069651005988, "biggest_recent_change": 0.5823838086839714 },
{"total_number_of_episodes":1830, "number_of_timesteps":1830000, "per_episode_reward":-710.12, "episode_reward_trend_value": -0.005661426568277269, "biggest_recent_change": 0.5823838086839714 },
{"total_number_of_episodes":1840, "number_of_timesteps":1840000, "per_episode_reward":-710.42, "episode_reward_trend_value": -0.00979917433589637, "biggest_recent_change": 0.5823838086839714 },
{"total_number_of_episodes":1850, "number_of_timesteps":1850000, "per_episode_reward":-710.74, "episode_reward_trend_value": -0.01309330451781029, "biggest_recent_change": 0.5823838086839714 },
{"total_number_of_episodes":1860, "number_of_timesteps":1860000, "per_episode_reward":-710.70, "episode_reward_trend_value": -0.006194064656403953, "biggest_recent_change": 0.3774193913942554 },
{"total_number_of_episodes":1870, "number_of_timesteps":1870000, "per_episode_reward":-710.82, "episode_reward_trend_value": -0.007474301458314079, "biggest_recent_change": 0.3774193913942554 },
{"total_number_of_episodes":1880, "number_of_timesteps":1880000, "per_episode_reward":-710.60, "episode_reward_trend_value": -0.0070278113298539, "biggest_recent_change": 0.3774193913942554 },
{"total_number_of_episodes":1890, "number_of_timesteps":1890000, "per_episode_reward":-710.62, "episode_reward_trend_value": -0.0084692520466812, "biggest_recent_change": 0.3774193913942554 },
{"total_number_of_episodes":1900, "number_of_timesteps":1900000, "per_episode_reward":-710.73, "episode_reward_trend_value": -0.007141657690332674, "biggest_recent_change": 0.3774193913942554 },
{"total_number_of_episodes":1910, "number_of_timesteps":1910000, "per_episode_reward":-711.09, "episode_reward_trend_value": -0.006924652189259935, "biggest_recent_change": 0.3578888962977089 },
{"total_number_of_episodes":1920, "number_of_timesteps":1920000, "per_episode_reward":-711.17, "episode_reward_trend_value": -0.011656073544351228, "biggest_recent_change": 0.3578888962977089 },
{"total_number_of_episodes":1930, "number_of_timesteps":1930000, "per_episode_reward":-711.06, "episode_reward_trend_value": -0.0070770341341055126, "biggest_recent_change": 0.3578888962977089 },
{"total_number_of_episodes":1940, "number_of_timesteps":1940000, "per_episode_reward":-710.95, "episode_reward_trend_value": -0.0022816233277416762, "biggest_recent_change": 0.3578888962977089 },
{"total_number_of_episodes":1950, "number_of_timesteps":1950000, "per_episode_reward":-711.06, "episode_reward_trend_value": -0.00395655965481991, "biggest_recent_change": 0.3578888962977089 },
{"total_number_of_episodes":1960, "number_of_timesteps":1960000, "per_episode_reward":-710.90, "episode_reward_trend_value": -0.0008419973949255817, "biggest_recent_change": 0.3578888962977089 },
{"total_number_of_episodes":1970, "number_of_timesteps":1970000, "per_episode_reward":-710.87, "episode_reward_trend_value": -0.002975470302718956, "biggest_recent_change": 0.3578888962977089 },
{"total_number_of_episodes":1980, "number_of_timesteps":1980000, "per_episode_reward":-710.80, "episode_reward_trend_value": -0.001998666940673704, "biggest_recent_change": 0.3578888962977089 },
{"total_number_of_episodes":1990, "number_of_timesteps":1990000, "per_episode_reward":-710.67, "episode_reward_trend_value": 0.0007133454708233937, "biggest_recent_change": 0.3578888962977089 },
{"total_number_of_episodes":2000, "number_of_timesteps":2000000, "per_episode_reward":-710.54, "episode_reward_trend_value": 0.006163922529713596, "biggest_recent_change": 0.15889452627038736 },
{"total_number_of_episodes":2010, "number_of_timesteps":2010000, "per_episode_reward":-710.65, "episode_reward_trend_value": 0.005751297963492006, "biggest_recent_change": 0.15889452627038736 },
{"total_number_of_episodes":2020, "number_of_timesteps":2020000, "per_episode_reward":-710.74, "episode_reward_trend_value": 0.0034999447304117997, "biggest_recent_change": 0.15889452627038736 },
{"total_number_of_episodes":2030, "number_of_timesteps":2030000, "per_episode_reward":-710.63, "episode_reward_trend_value": 0.003486832716984938, "biggest_recent_change": 0.15889452627038736 },
{"total_number_of_episodes":2040, "number_of_timesteps":2040000, "per_episode_reward":-710.33, "episode_reward_trend_value": 0.008028804119044252, "biggest_recent_change": 0.29658093559089593 },
{"total_number_of_episodes":2050, "number_of_timesteps":2050000, "per_episode_reward":-710.09, "episode_reward_trend_value": 0.009013884547445944, "biggest_recent_change": 0.29658093559089593 },
{"total_number_of_episodes":2060, "number_of_timesteps":2060000, "per_episode_reward":-710.01, "episode_reward_trend_value": 0.009508713027370656, "biggest_recent_change": 0.29658093559089593 },
{"total_number_of_episodes":2070, "number_of_timesteps":2070000, "per_episode_reward":-709.60, "episode_reward_trend_value": 0.013268318636186096, "biggest_recent_change": 0.4103731732076312 },
{"total_number_of_episodes":2080, "number_of_timesteps":2080000, "per_episode_reward":-709.47, "episode_reward_trend_value": 0.013288575037605218, "biggest_recent_change": 0.4103731732076312 },
{"total_number_of_episodes":2090, "number_of_timesteps":2090000, "per_episode_reward":-709.32, "episode_reward_trend_value": 0.013508880578057061, "biggest_recent_change": 0.4103731732076312 },
{"total_number_of_episodes":2100, "number_of_timesteps":2100000, "per_episode_reward":-709.15, "episode_reward_trend_value": 0.01665099450553953, "biggest_recent_change": 0.4103731732076312 },
{"total_number_of_episodes":2110, "number_of_timesteps":2110000, "per_episode_reward":-709.38, "episode_reward_trend_value": 0.015109822346530816, "biggest_recent_change": 0.4103731732076312 },
{"total_number_of_episodes":2120, "number_of_timesteps":2120000, "per_episode_reward":-709.44, "episode_reward_trend_value": 0.013203541312340247, "biggest_recent_change": 0.4103731732076312 },
{"total_number_of_episodes":2130, "number_of_timesteps":2130000, "per_episode_reward":-709.22, "episode_reward_trend_value": 0.012395302009034975, "biggest_recent_change": 0.4103731732076312 },
{"total_number_of_episodes":2140, "number_of_timesteps":2140000, "per_episode_reward":-709.09, "episode_reward_trend_value": 0.01105971806875156, "biggest_recent_change": 0.4103731732076312 },
{"total_number_of_episodes":2150, "number_of_timesteps":2150000, "per_episode_reward":-709.02, "episode_reward_trend_value": 0.011036355517256248, "biggest_recent_change": 0.4103731732076312 },
{"total_number_of_episodes":2160, "number_of_timesteps":2160000, "per_episode_reward":-708.76, "episode_reward_trend_value": 0.009362138634415058, "biggest_recent_change": 0.2596936537519241 },
{"total_number_of_episodes":2170, "number_of_timesteps":2170000, "per_episode_reward":-708.89, "episode_reward_trend_value": 0.006458129437429408, "biggest_recent_change": 0.2596936537519241 },
{"total_number_of_episodes":2180, "number_of_timesteps":2180000, "per_episode_reward":-708.88, "episode_reward_trend_value": 0.004895440915278312, "biggest_recent_change": 0.2596936537519241 },
{"total_number_of_episodes":2190, "number_of_timesteps":2190000, "per_episode_reward":-708.77, "episode_reward_trend_value": 0.0043128530251275755, "biggest_recent_change": 0.2596936537519241 },
{"total_number_of_episodes":2200, "number_of_timesteps":2200000, "per_episode_reward":-708.46, "episode_reward_trend_value": 0.010309985861910162, "biggest_recent_change": 0.3099025689159589 },
{"total_number_of_episodes":2210, "number_of_timesteps":2210000, "per_episode_reward":-708.67, "episode_reward_trend_value": 0.008593193953236601, "biggest_recent_change": 0.3099025689159589 },
{"total_number_of_episodes":2220, "number_of_timesteps":2220000, "per_episode_reward":-708.59, "episode_reward_trend_value": 0.007038036809691069, "biggest_recent_change": 0.3099025689159589 },
{"total_number_of_episodes":2230, "number_of_timesteps":2230000, "per_episode_reward":-708.71, "episode_reward_trend_value": 0.004217722935152324, "biggest_recent_change": 0.3099025689159589 },
{"total_number_of_episodes":2240, "number_of_timesteps":2240000, "per_episode_reward":-708.65, "episode_reward_trend_value": 0.004131455729227835, "biggest_recent_change": 0.3099025689159589 },
{"total_number_of_episodes":2250, "number_of_timesteps":2250000, "per_episode_reward":-708.36, "episode_reward_trend_value": 0.00440977448134087, "biggest_recent_change": 0.3099025689159589 },
{"total_number_of_episodes":2260, "number_of_timesteps":2260000, "per_episode_reward":-708.42, "episode_reward_trend_value": 0.005191161132427169, "biggest_recent_change": 0.3099025689159589 },
{"total_number_of_episodes":2270, "number_of_timesteps":2270000, "per_episode_reward":-708.16, "episode_reward_trend_value": 0.008035759644082595, "biggest_recent_change": 0.3099025689159589 },
{"total_number_of_episodes":2280, "number_of_timesteps":2280000, "per_episode_reward":-708.43, "episode_reward_trend_value": 0.0037225216575532414, "biggest_recent_change": 0.3099025689159589 },
{"total_number_of_episodes":2290, "number_of_timesteps":2290000, "per_episode_reward":-708.87, "episode_reward_trend_value": -0.004585432291341679, "biggest_recent_change": 0.43781328648458384 },
{"total_number_of_episodes":2300, "number_of_timesteps":2300000, "per_episode_reward":-708.30, "episode_reward_trend_value": 0.004155739224909717, "biggest_recent_change": 0.5729572824692468 },
{"total_number_of_episodes":2310, "number_of_timesteps":2310000, "per_episode_reward":-708.19, "episode_reward_trend_value": 0.004364079241539306, "biggest_recent_change": 0.5729572824692468 },
{"total_number_of_episodes":2320, "number_of_timesteps":2320000, "per_episode_reward":-708.36, "episode_reward_trend_value": 0.003940827201318421, "biggest_recent_change": 0.5729572824692468 },
{"total_number_of_episodes":2330, "number_of_timesteps":2330000, "per_episode_reward":-708.38, "episode_reward_trend_value": 0.002945665444052439, "biggest_recent_change": 0.5729572824692468 },
{"total_number_of_episodes":2340, "number_of_timesteps":2340000, "per_episode_reward":-708.24, "episode_reward_trend_value": 0.0012948745813319974, "biggest_recent_change": 0.5729572824692468 },
{"total_number_of_episodes":2350, "number_of_timesteps":2350000, "per_episode_reward":-707.90, "episode_reward_trend_value": 0.005841602176505805, "biggest_recent_change": 0.5729572824692468 },
{"total_number_of_episodes":2360, "number_of_timesteps":2360000, "per_episode_reward":-708.15, "episode_reward_trend_value": 5.8974848027699205e-05, "biggest_recent_change": 0.5729572824692468 },
{"total_number_of_episodes":2370, "number_of_timesteps":2370000, "per_episode_reward":-708.31, "episode_reward_trend_value": 0.0013625278351254463, "biggest_recent_change": 0.5729572824692468 },
{"total_number_of_episodes":2380, "number_of_timesteps":2380000, "per_episode_reward":-707.95, "episode_reward_trend_value": 0.010193087642911299, "biggest_recent_change": 0.5729572824692468 },
{"total_number_of_episodes":2390, "number_of_timesteps":2390000, "per_episode_reward":-708.45, "episode_reward_trend_value": -0.0016985160702170613, "biggest_recent_change": 0.4972870517123056 },
{"total_number_of_episodes":2400, "number_of_timesteps":2400000, "per_episode_reward":-708.00, "episode_reward_trend_value": 0.002099526610086539, "biggest_recent_change": 0.4972870517123056 },
{"total_number_of_episodes":2410, "number_of_timesteps":2410000, "per_episode_reward":-707.58, "episode_reward_trend_value": 0.008630545656593666, "biggest_recent_change": 0.4972870517123056 },
{"total_number_of_episodes":2420, "number_of_timesteps":2420000, "per_episode_reward":-707.52, "episode_reward_trend_value": 0.009537751712604051, "biggest_recent_change": 0.4972870517123056 },
{"total_number_of_episodes":2430, "number_of_timesteps":2430000, "per_episode_reward":-707.52, "episode_reward_trend_value": 0.00807556778787178, "biggest_recent_change": 0.4972870517123056 },
{"total_number_of_episodes":2440, "number_of_timesteps":2440000, "per_episode_reward":-707.52, "episode_reward_trend_value": 0.004197601958004624, "biggest_recent_change": 0.4972870517123056 },
{"total_number_of_episodes":2450, "number_of_timesteps":2450000, "per_episode_reward":-707.58, "episode_reward_trend_value": 0.006300195847676252, "biggest_recent_change": 0.4972870517123056 },
{"total_number_of_episodes":2460, "number_of_timesteps":2460000, "per_episode_reward":-707.48, "episode_reward_trend_value": 0.009167938932862146, "biggest_recent_change": 0.4972870517123056 },
{"total_number_of_episodes":2470, "number_of_timesteps":2470000, "per_episode_reward":-707.59, "episode_reward_trend_value": 0.003980814605485092, "biggest_recent_change": 0.4972870517123056 },
{"total_number_of_episodes":2480, "number_of_timesteps":2480000, "per_episode_reward":-707.66, "episode_reward_trend_value": 0.008712732511633526, "biggest_recent_change": 0.44444969809831036 },
{"total_number_of_episodes":2490, "number_of_timesteps":2490000, "per_episode_reward":-707.48, "episode_reward_trend_value": 0.005799102875241462, "biggest_recent_change": 0.42321999205830707 },
{"total_number_of_episodes":2500, "number_of_timesteps":2500000, "per_episode_reward":-707.54, "episode_reward_trend_value": 0.0004601733710450794, "biggest_recent_change": 0.1822230308230246 },
{"total_number_of_episodes":2510, "number_of_timesteps":2510000, "per_episode_reward":-707.36, "episode_reward_trend_value": 0.0018591466753719033, "biggest_recent_change": 0.18396002869167205 },
{"total_number_of_episodes":2520, "number_of_timesteps":2520000, "per_episode_reward":-707.25, "episode_reward_trend_value": 0.002940409688067626, "biggest_recent_change": 0.18396002869167205 },
{"total_number_of_episodes":2530, "number_of_timesteps":2530000, "per_episode_reward":-707.33, "episode_reward_trend_value": 0.00213341685610481, "biggest_recent_change": 0.18396002869167205 },
{"total_number_of_episodes":2540, "number_of_timesteps":2540000, "per_episode_reward":-707.37, "episode_reward_trend_value": 0.0024031144072763607, "biggest_recent_change": 0.18396002869167205 },
{"total_number_of_episodes":2550, "number_of_timesteps":2550000, "per_episode_reward":-707.32, "episode_reward_trend_value": 0.0017608394879643028, "biggest_recent_change": 0.18396002869167205 },
{"total_number_of_episodes":2560, "number_of_timesteps":2560000, "per_episode_reward":-706.98, "episode_reward_trend_value": 0.006813531132498459, "biggest_recent_change": 0.3448381547602821 },
{"total_number_of_episodes":2570, "number_of_timesteps":2570000, "per_episode_reward":-706.73, "episode_reward_trend_value": 0.010389457033117349, "biggest_recent_change": 0.3448381547602821 },
{"total_number_of_episodes":2580, "number_of_timesteps":2580000, "per_episode_reward":-706.53, "episode_reward_trend_value": 0.01055531851291865, "biggest_recent_change": 0.3448381547602821 },
{"total_number_of_episodes":2590, "number_of_timesteps":2590000, "per_episode_reward":-706.81, "episode_reward_trend_value": 0.008059587523959837, "biggest_recent_change": 0.3448381547602821 },
{"total_number_of_episodes":2600, "number_of_timesteps":2600000, "per_episode_reward":-706.76, "episode_reward_trend_value": 0.006640116259494436, "biggest_recent_change": 0.3448381547602821 },
{"total_number_of_episodes":2610, "number_of_timesteps":2610000, "per_episode_reward":-706.74, "episode_reward_trend_value": 0.0057586132644701134, "biggest_recent_change": 0.3448381547602821 },
{"total_number_of_episodes":2620, "number_of_timesteps":2620000, "per_episode_reward":-706.88, "episode_reward_trend_value": 0.004951664928895146, "biggest_recent_change": 0.3448381547602821 },
{"total_number_of_episodes":2630, "number_of_timesteps":2630000, "per_episode_reward":-707.16, "episode_reward_trend_value": 0.002353788339619061, "biggest_recent_change": 0.3448381547602821 },
{"total_number_of_episodes":2640, "number_of_timesteps":2640000, "per_episode_reward":-706.92, "episode_reward_trend_value": 0.0044383675375065145, "biggest_recent_change": 0.3448381547602821 },
{"total_number_of_episodes":2650, "number_of_timesteps":2650000, "per_episode_reward":-707.20, "episode_reward_trend_value": -0.002406092066101135, "biggest_recent_change": 0.2818994523256606 },
{"total_number_of_episodes":2660, "number_of_timesteps":2660000, "per_episode_reward":-707.00, "episode_reward_trend_value": -0.0029692805941989617, "biggest_recent_change": 0.2818994523256606 },
{"total_number_of_episodes":2670, "number_of_timesteps":2670000, "per_episode_reward":-707.09, "episode_reward_trend_value": -0.006166134771271774, "biggest_recent_change": 0.2818994523256606 },
{"total_number_of_episodes":2680, "number_of_timesteps":2680000, "per_episode_reward":-707.24, "episode_reward_trend_value": -0.004768792249198365, "biggest_recent_change": 0.2728766862235261 },
{"total_number_of_episodes":2690, "number_of_timesteps":2690000, "per_episode_reward":-707.45, "episode_reward_trend_value": -0.007706474230910014, "biggest_recent_change": 0.2728766862235261 },
{"total_number_of_episodes":2700, "number_of_timesteps":2700000, "per_episode_reward":-707.25, "episode_reward_trend_value": -0.005757690220257751, "biggest_recent_change": 0.2728766862235261 },
{"total_number_of_episodes":2710, "number_of_timesteps":2710000, "per_episode_reward":-707.18, "episode_reward_trend_value": -0.0033191521751443054, "biggest_recent_change": 0.2728766862235261 },
{"total_number_of_episodes":2720, "number_of_timesteps":2720000, "per_episode_reward":-707.06, "episode_reward_trend_value": 0.0010836747512396668, "biggest_recent_change": 0.2711632095644063 },
{"total_number_of_episodes":2730, "number_of_timesteps":2730000, "per_episode_reward":-706.87, "episode_reward_trend_value": 0.0005977534472701034, "biggest_recent_change": 0.2711632095644063 },
{"total_number_of_episodes":2740, "number_of_timesteps":2740000, "per_episode_reward":-707.14, "episode_reward_trend_value": 0.0006793801646848705, "biggest_recent_change": 0.26381680499707727 },
{"total_number_of_episodes":2750, "number_of_timesteps":2750000, "per_episode_reward":-707.56, "episode_reward_trend_value": -0.006234798586124018, "biggest_recent_change": 0.42254416420485086 },
{"total_number_of_episodes":2760, "number_of_timesteps":2760000, "per_episode_reward":-707.68, "episode_reward_trend_value": -0.0065462624558373745, "biggest_recent_change": 0.42254416420485086 },
{"total_number_of_episodes":2770, "number_of_timesteps":2770000, "per_episode_reward":-707.48, "episode_reward_trend_value": -0.002678411051682714, "biggest_recent_change": 0.42254416420485086 },
{"total_number_of_episodes":2780, "number_of_timesteps":2780000, "per_episode_reward":-707.36, "episode_reward_trend_value": 0.0009975544054201338, "biggest_recent_change": 0.42254416420485086 },
{"total_number_of_episodes":2790, "number_of_timesteps":2790000, "per_episode_reward":-707.13, "episode_reward_trend_value": 0.0013957523535155915, "biggest_recent_change": 0.42254416420485086 },
{"total_number_of_episodes":2800, "number_of_timesteps":2800000, "per_episode_reward":-707.34, "episode_reward_trend_value": -0.0017565839924903533, "biggest_recent_change": 0.42254416420485086 },
{"total_number_of_episodes":2810, "number_of_timesteps":2810000, "per_episode_reward":-707.13, "episode_reward_trend_value": -0.0007817990011302654, "biggest_recent_change": 0.42254416420485086 },
{"total_number_of_episodes":2820, "number_of_timesteps":2820000, "per_episode_reward":-707.54, "episode_reward_trend_value": -0.007431736224795158, "biggest_recent_change": 0.42254416420485086 },
{"total_number_of_episodes":2830, "number_of_timesteps":2830000, "per_episode_reward":-707.53, "episode_reward_trend_value": -0.004334150971401919, "biggest_recent_change": 0.42254416420485086 },
{"total_number_of_episodes":2840, "number_of_timesteps":2840000, "per_episode_reward":-707.29, "episode_reward_trend_value": 0.0030212506369960263, "biggest_recent_change": 0.41099784150947016 },
{"total_number_of_episodes":2850, "number_of_timesteps":2850000, "per_episode_reward":-707.33, "episode_reward_trend_value": 0.003795329063579326, "biggest_recent_change": 0.41099784150947016 },
{"total_number_of_episodes":2860, "number_of_timesteps":2860000, "per_episode_reward":-707.16, "episode_reward_trend_value": 0.0035973678311873097, "biggest_recent_change": 0.41099784150947016 },
{"total_number_of_episodes":2870, "number_of_timesteps":2870000, "per_episode_reward":-707.07, "episode_reward_trend_value": 0.0032860465058964414, "biggest_recent_change": 0.41099784150947016 },
{"total_number_of_episodes":2880, "number_of_timesteps":2880000, "per_episode_reward":-706.99, "episode_reward_trend_value": 0.001567137335206957, "biggest_recent_change": 0.41099784150947016 },
{"total_number_of_episodes":2890, "number_of_timesteps":2890000, "per_episode_reward":-707.04, "episode_reward_trend_value": 0.0032811298880220524, "biggest_recent_change": 0.41099784150947016 },
{"total_number_of_episodes":2900, "number_of_timesteps":2900000, "per_episode_reward":-706.95, "episode_reward_trend_value": 0.0020440919140418774, "biggest_recent_change": 0.41099784150947016 },
{"total_number_of_episodes":2910, "number_of_timesteps":2910000, "per_episode_reward":-707.05, "episode_reward_trend_value": 0.005411694393632792, "biggest_recent_change": 0.23944198055096422 },
{"total_number_of_episodes":2920, "number_of_timesteps":2920000, "per_episode_reward":-706.79, "episode_reward_trend_value": 0.008142910669288187, "biggest_recent_change": 0.2607753326172997 },
{"total_number_of_episodes":2930, "number_of_timesteps":2930000, "per_episode_reward":-706.74, "episode_reward_trend_value": 0.006050136906670408, "biggest_recent_change": 0.2607753326172997 },
{"total_number_of_episodes":2940, "number_of_timesteps":2940000, "per_episode_reward":-706.89, "episode_reward_trend_value": 0.004977341783544236, "biggest_recent_change": 0.2607753326172997 },
{"total_number_of_episodes":2950, "number_of_timesteps":2950000, "per_episode_reward":-706.98, "episode_reward_trend_value": 0.0020320702838792686, "biggest_recent_change": 0.2607753326172997 },
{"total_number_of_episodes":2960, "number_of_timesteps":2960000, "per_episode_reward":-707.07, "episode_reward_trend_value": -5.2661317538928016e-05, "biggest_recent_change": 0.2607753326172997 },
{"total_number_of_episodes":2970, "number_of_timesteps":2970000, "per_episode_reward":-706.87, "episode_reward_trend_value": 0.0013213141535629801, "biggest_recent_change": 0.2607753326172997 },
{"total_number_of_episodes":2980, "number_of_timesteps":2980000, "per_episode_reward":-706.79, "episode_reward_trend_value": 0.002841165488054129, "biggest_recent_change": 0.2607753326172997 },
{"total_number_of_episodes":2990, "number_of_timesteps":2990000, "per_episode_reward":-706.58, "episode_reward_trend_value": 0.0040937447723182335, "biggest_recent_change": 0.2607753326172997 },
{"total_number_of_episodes":3000, "number_of_timesteps":3000000, "per_episode_reward":-706.83, "episode_reward_trend_value": 0.0024788169400759823, "biggest_recent_change": 0.2607753326172997 },
{"total_number_of_episodes":3010, "number_of_timesteps":3010000, "per_episode_reward":-706.86, "episode_reward_trend_value": -0.0007984237379572632, "biggest_recent_change": 0.25325712324809047 },
{"total_number_of_episodes":3020, "number_of_timesteps":3020000, "per_episode_reward":-706.99, "episode_reward_trend_value": -0.002734072764222775, "biggest_recent_change": 0.25325712324809047 },
{"total_number_of_episodes":3030, "number_of_timesteps":3030000, "per_episode_reward":-707.01, "episode_reward_trend_value": -0.0013451472166631597, "biggest_recent_change": 0.25325712324809047 },
{"total_number_of_episodes":3040, "number_of_timesteps":3040000, "per_episode_reward":-707.07, "episode_reward_trend_value": -0.0009941697952576254, "biggest_recent_change": 0.25325712324809047 },
{"total_number_of_episodes":3050, "number_of_timesteps":3050000, "per_episode_reward":-707.04, "episode_reward_trend_value": 0.0002990879998277604, "biggest_recent_change": 0.25325712324809047 },
{"total_number_of_episodes":3060, "number_of_timesteps":3060000, "per_episode_reward":-707.00, "episode_reward_trend_value": -0.0014200194847906764, "biggest_recent_change": 0.25325712324809047 },
{"total_number_of_episodes":3070, "number_of_timesteps":3070000, "per_episode_reward":-706.91, "episode_reward_trend_value": -0.0013166834688301302, "biggest_recent_change": 0.25325712324809047 },
{"total_number_of_episodes":3080, "number_of_timesteps":3080000, "per_episode_reward":-707.13, "episode_reward_trend_value": -0.006126591483400211, "biggest_recent_change": 0.25325712324809047 },
{"total_number_of_episodes":3090, "number_of_timesteps":3090000, "per_episode_reward":-707.02, "episode_reward_trend_value": -0.0020789659576318327, "biggest_recent_change": 0.22038461701231427 },
{"total_number_of_episodes":3100, "number_of_timesteps":3100000, "per_episode_reward":-707.10, "episode_reward_trend_value": -0.002668108607180228, "biggest_recent_change": 0.22038461701231427 },
{"total_number_of_episodes":3110, "number_of_timesteps":3110000, "per_episode_reward":-707.05, "episode_reward_trend_value": -0.0007149441088320903, "biggest_recent_change": 0.22038461701231427 },
{"total_number_of_episodes":3120, "number_of_timesteps":3120000, "per_episode_reward":-706.89, "episode_reward_trend_value": 0.0012813990146494487, "biggest_recent_change": 0.22038461701231427 },
{"total_number_of_episodes":3130, "number_of_timesteps":3130000, "per_episode_reward":-706.79, "episode_reward_trend_value": 0.0030791072565585233, "biggest_recent_change": 0.22038461701231427 },
{"total_number_of_episodes":3140, "number_of_timesteps":3140000, "per_episode_reward":-706.86, "episode_reward_trend_value": 0.0020589737458521994, "biggest_recent_change": 0.22038461701231427 },
{"total_number_of_episodes":3150, "number_of_timesteps":3150000, "per_episode_reward":-706.94, "episode_reward_trend_value": 0.0006598042762271285, "biggest_recent_change": 0.22038461701231427 },
{"total_number_of_episodes":3160, "number_of_timesteps":3160000, "per_episode_reward":-706.90, "episode_reward_trend_value": 0.00012087206396497801, "biggest_recent_change": 0.22038461701231427 },
{"total_number_of_episodes":3170, "number_of_timesteps":3170000, "per_episode_reward":-706.82, "episode_reward_trend_value": 0.0034176314128975113, "biggest_recent_change": 0.15919161749923205 },
{"total_number_of_episodes":3180, "number_of_timesteps":3180000, "per_episode_reward":-706.81, "episode_reward_trend_value": 0.0023461843858753025, "biggest_recent_change": 0.15919161749923205 },
{"total_number_of_episodes":3190, "number_of_timesteps":3190000, "per_episode_reward":-706.70, "episode_reward_trend_value": 0.004458789732727079, "biggest_recent_change": 0.15919161749923205 },
{"total_number_of_episodes":3200, "number_of_timesteps":3200000, "per_episode_reward":-706.64, "episode_reward_trend_value": 0.004579524122462115, "biggest_recent_change": 0.15919161749923205 },
{"total_number_of_episodes":3210, "number_of_timesteps":3210000, "per_episode_reward":-706.62, "episode_reward_trend_value": 0.003013311827765443, "biggest_recent_change": 0.10293531435161185 },
{"total_number_of_episodes":3220, "number_of_timesteps":3220000, "per_episode_reward":-706.34, "episode_reward_trend_value": 0.004950516128581562, "biggest_recent_change": 0.27680715192150274 },
{"total_number_of_episodes":3230, "number_of_timesteps":3230000, "per_episode_reward":-706.61, "episode_reward_trend_value": 0.002769710472889781, "biggest_recent_change": 0.27680715192150274 },
{"total_number_of_episodes":3240, "number_of_timesteps":3240000, "per_episode_reward":-706.52, "episode_reward_trend_value": 0.0045694588570477715, "biggest_recent_change": 0.27680715192150274 },
{"total_number_of_episodes":3250, "number_of_timesteps":3250000, "per_episode_reward":-706.56, "episode_reward_trend_value": 0.003698266857400591, "biggest_recent_change": 0.27680715192150274 },
{"total_number_of_episodes":3260, "number_of_timesteps":3260000, "per_episode_reward":-706.47, "episode_reward_trend_value": 0.003921990022601878, "biggest_recent_change": 0.27680715192150274 },
{"total_number_of_episodes":3270, "number_of_timesteps":3270000, "per_episode_reward":-706.37, "episode_reward_trend_value": 0.0048753418419399475, "biggest_recent_change": 0.27680715192150274 },
{"total_number_of_episodes":3280, "number_of_timesteps":3280000, "per_episode_reward":-706.30, "episode_reward_trend_value": 0.004488986008947096, "biggest_recent_change": 0.27680715192150274 },
{"total_number_of_episodes":3290, "number_of_timesteps":3290000, "per_episode_reward":-706.25, "episode_reward_trend_value": 0.004380356382579167, "biggest_recent_change": 0.27680715192150274 },
{"total_number_of_episodes":3300, "number_of_timesteps":3300000, "per_episode_reward":-706.56, "episode_reward_trend_value": 0.000680484282152823, "biggest_recent_change": 0.3147559780618394 },
{"total_number_of_episodes":3310, "number_of_timesteps":3310000, "per_episode_reward":-706.40, "episode_reward_trend_value": -0.0006454397355267045, "biggest_recent_change": 0.3147559780618394 },
{"total_number_of_episodes":3320, "number_of_timesteps":3320000, "per_episode_reward":-706.58, "episode_reward_trend_value": 0.00034946551294297025, "biggest_recent_change": 0.3147559780618394 },
{"total_number_of_episodes":3330, "number_of_timesteps":3330000, "per_episode_reward":-706.52, "episode_reward_trend_value": 7.934086827390274e-05, "biggest_recent_change": 0.3147559780618394 },
{"total_number_of_episodes":3340, "number_of_timesteps":3340000, "per_episode_reward":-706.43, "episode_reward_trend_value": 0.0015004653123418797, "biggest_recent_change": 0.3147559780618394 },
{"total_number_of_episodes":3350, "number_of_timesteps":3350000, "per_episode_reward":-706.52, "episode_reward_trend_value": -0.0006363674133745513, "biggest_recent_change": 0.3147559780618394 },
{"total_number_of_episodes":3360, "number_of_timesteps":3360000, "per_episode_reward":-706.29, "episode_reward_trend_value": 0.0008314821318852713, "biggest_recent_change": 0.3147559780618394 },
{"total_number_of_episodes":3370, "number_of_timesteps":3370000, "per_episode_reward":-706.47, "episode_reward_trend_value": -0.0018837475685283306, "biggest_recent_change": 0.3147559780618394 },
{"total_number_of_episodes":3380, "number_of_timesteps":3380000, "per_episode_reward":-706.69, "episode_reward_trend_value": -0.004914340119910321, "biggest_recent_change": 0.3147559780618394 },
{"total_number_of_episodes":3390, "number_of_timesteps":3390000, "per_episode_reward":-706.77, "episode_reward_trend_value": -0.00232999830110556, "biggest_recent_change": 0.23250706445287506 },
{"total_number_of_episodes":3400, "number_of_timesteps":3400000, "per_episode_reward":-706.72, "episode_reward_trend_value": -0.0034831807688381865, "biggest_recent_change": 0.23250706445287506 },
{"total_number_of_episodes":3410, "number_of_timesteps":3410000, "per_episode_reward":-706.52, "episode_reward_trend_value": 0.0006239331894625543, "biggest_recent_change": 0.23250706445287506 },
{"total_number_of_episodes":3420, "number_of_timesteps":3420000, "per_episode_reward":-706.38, "episode_reward_trend_value": 0.0015231193347075432, "biggest_recent_change": 0.23250706445287506 },
{"total_number_of_episodes":3430, "number_of_timesteps":3430000, "per_episode_reward":-706.21, "episode_reward_trend_value": 0.002430862713971591, "biggest_recent_change": 0.23250706445287506 },
{"total_number_of_episodes":3440, "number_of_timesteps":3440000, "per_episode_reward":-706.38, "episode_reward_trend_value": 0.0015566131024785236, "biggest_recent_change": 0.23250706445287506 },
{"total_number_of_episodes":3450, "number_of_timesteps":3450000, "per_episode_reward":-706.34, "episode_reward_trend_value": -0.0005841520630029764, "biggest_recent_change": 0.218995166518539 },
{"total_number_of_episodes":3460, "number_of_timesteps":3460000, "per_episode_reward":-706.33, "episode_reward_trend_value": 0.0015181353703964204, "biggest_recent_change": 0.218995166518539 },
{"total_number_of_episodes":3470, "number_of_timesteps":3470000, "per_episode_reward":-706.47, "episode_reward_trend_value": 0.0023733079724391043, "biggest_recent_change": 0.19449876946237055 },
{"total_number_of_episodes":3480, "number_of_timesteps":3480000, "per_episode_reward":-706.57, "episode_reward_trend_value": 0.00218660019949463, "biggest_recent_change": 0.19449876946237055 },
{"total_number_of_episodes":3490, "number_of_timesteps":3490000, "per_episode_reward":-706.45, "episode_reward_trend_value": 0.0029819094672790115, "biggest_recent_change": 0.19449876946237055 },
{"total_number_of_episodes":3500, "number_of_timesteps":3500000, "per_episode_reward":-706.36, "episode_reward_trend_value": 0.001804702317531337, "biggest_recent_change": 0.17453860108912522 },
{"total_number_of_episodes":3510, "number_of_timesteps":3510000, "per_episode_reward":-706.20, "episode_reward_trend_value": 0.001988405053204032, "biggest_recent_change": 0.17453860108912522 },
{"total_number_of_episodes":3520, "number_of_timesteps":3520000, "per_episode_reward":-706.46, "episode_reward_trend_value": -0.0027628499896915552, "biggest_recent_change": 0.2569901303854749 },
{"total_number_of_episodes":3530, "number_of_timesteps":3530000, "per_episode_reward":-706.40, "episode_reward_trend_value": -0.00020312869099572913, "biggest_recent_change": 0.2569901303854749 },
{"total_number_of_episodes":3540, "number_of_timesteps":3540000, "per_episode_reward":-706.43, "episode_reward_trend_value": -0.0010001494041918906, "biggest_recent_change": 0.2569901303854749 },
{"total_number_of_episodes":3550, "number_of_timesteps":3550000, "per_episode_reward":-706.54, "episode_reward_trend_value": -0.0023185768555688607, "biggest_recent_change": 0.2569901303854749 },
{"total_number_of_episodes":3560, "number_of_timesteps":3560000, "per_episode_reward":-706.52, "episode_reward_trend_value": -0.0004708828627018293, "biggest_recent_change": 0.2569901303854749 },
{"total_number_of_episodes":3570, "number_of_timesteps":3570000, "per_episode_reward":-706.50, "episode_reward_trend_value": 0.0008456725794695153, "biggest_recent_change": 0.2569901303854749 },
{"total_number_of_episodes":3580, "number_of_timesteps":3580000, "per_episode_reward":-706.45, "episode_reward_trend_value": 1.1759562326258471e-05, "biggest_recent_change": 0.2569901303854749 },
{"total_number_of_episodes":3590, "number_of_timesteps":3590000, "per_episode_reward":-706.25, "episode_reward_trend_value": 0.0012333646009867355, "biggest_recent_change": 0.2569901303854749 },
{"total_number_of_episodes":3600, "number_of_timesteps":3600000, "per_episode_reward":-706.20, "episode_reward_trend_value": 3.724563092267393e-05, "biggest_recent_change": 0.2569901303854749 },
{"total_number_of_episodes":3610, "number_of_timesteps":3610000, "per_episode_reward":-706.23, "episode_reward_trend_value": 0.0025793297385626425, "biggest_recent_change": 0.19849457946452276 },
{"total_number_of_episodes":3620, "number_of_timesteps":3620000, "per_episode_reward":-706.38, "episode_reward_trend_value": 0.0002907574126020841, "biggest_recent_change": 0.19849457946452276 },
{"total_number_of_episodes":3630, "number_of_timesteps":3630000, "per_episode_reward":-706.42, "episode_reward_trend_value": 0.00017395260828405095, "biggest_recent_change": 0.19849457946452276 },
{"total_number_of_episodes":3640, "number_of_timesteps":3640000, "per_episode_reward":-706.36, "episode_reward_trend_value": 0.0019941909429538403, "biggest_recent_change": 0.19849457946452276 },
{"total_number_of_episodes":3650, "number_of_timesteps":3650000, "per_episode_reward":-706.44, "episode_reward_trend_value": 0.0008398854167138679, "biggest_recent_change": 0.19849457946452276 },
{"total_number_of_episodes":3660, "number_of_timesteps":3660000, "per_episode_reward":-706.48, "episode_reward_trend_value": 0.0001533194938120788, "biggest_recent_change": 0.19849457946452276 },
{"total_number_of_episodes":3670, "number_of_timesteps":3670000, "per_episode_reward":-706.55, "episode_reward_trend_value": -0.0011778062793433087, "biggest_recent_change": 0.19849457946452276 },
{"total_number_of_episodes":3680, "number_of_timesteps":3680000, "per_episode_reward":-706.57, "episode_reward_trend_value": -0.003613029867244677, "biggest_recent_change": 0.15013519354295113 },
{"total_number_of_episodes":3690, "number_of_timesteps":3690000, "per_episode_reward":-706.61, "episode_reward_trend_value": -0.0046253113901545675, "biggest_recent_change": 0.15013519354295113 },
{"total_number_of_episodes":3700, "number_of_timesteps":3700000, "per_episode_reward":-706.60, "episode_reward_trend_value": -0.004135693806536741, "biggest_recent_change": 0.15013519354295113 },
Hit early stopping because biggest_recent_change: 0.07962467033826215 < 0.1
{"total_number_of_episodes":3710, "number_of_timesteps":3710000, "per_episode_reward":-706.61, "episode_reward_trend_value": -0.0026306586969478, "biggest_recent_change": 0.07962467033826215 },




Process Process-13:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-12:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-20:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 16:57:01,386][0m Trial 1 finished with value: -706.6358055663736 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 2, 'permaban_threshold': 0}. Best is trial 1 with value: -706.6358055663736.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-795.32, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-792.08, "episode_reward_trend_value": 0.3236885290913847, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-789.57, "episode_reward_trend_value": 0.2876536988428768, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-774.55, "episode_reward_trend_value": 0.6924549620816493, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-782.30, "episode_reward_trend_value": 0.3256458228039321, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-783.96, "episode_reward_trend_value": 0.22732826453114058, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-783.40, "episode_reward_trend_value": 0.1986235671175507, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-786.58, "episode_reward_trend_value": 0.12486195540632414, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-792.49, "episode_reward_trend_value": 0.035396389649629614, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-786.13, "episode_reward_trend_value": 0.10212852687925027, "biggest_recent_change": 15.020574885591941 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-785.28, "episode_reward_trend_value": 0.07563195370453286, "biggest_recent_change": 15.020574885591941 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-789.99, "episode_reward_trend_value": -0.004733550227767106, "biggest_recent_change": 15.020574885591941 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-790.33, "episode_reward_trend_value": -0.17534602880842967, "biggest_recent_change": 7.747815950292193 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-789.42, "episode_reward_trend_value": -0.07914457419977654, "biggest_recent_change": 6.359856247162156 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-790.16, "episode_reward_trend_value": -0.0689637825123782, "biggest_recent_change": 6.359856247162156 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-792.46, "episode_reward_trend_value": -0.10058507076094605, "biggest_recent_change": 6.359856247162156 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-792.73, "episode_reward_trend_value": -0.06829884931595037, "biggest_recent_change": 6.359856247162156 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-789.24, "episode_reward_trend_value": 0.036103257946297165, "biggest_recent_change": 6.359856247162156 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-793.75, "episode_reward_trend_value": -0.08469198084089966, "biggest_recent_change": 4.716706667963308 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-795.44, "episode_reward_trend_value": -0.11290175906372549, "biggest_recent_change": 4.716706667963308 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-795.47, "episode_reward_trend_value": -0.06086754316691996, "biggest_recent_change": 4.511715243685558 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-794.37, "episode_reward_trend_value": -0.04493853378106678, "biggest_recent_change": 4.511715243685558 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-798.19, "episode_reward_trend_value": -0.09744530414889772, "biggest_recent_change": 4.511715243685558 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-798.56, "episode_reward_trend_value": -0.0933627653791215, "biggest_recent_change": 4.511715243685558 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-798.49, "episode_reward_trend_value": -0.06701871305015047, "biggest_recent_change": 4.511715243685558 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-799.84, "episode_reward_trend_value": -0.07907653871612612, "biggest_recent_change": 4.511715243685558 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-798.15, "episode_reward_trend_value": -0.09902163082662381, "biggest_recent_change": 4.511715243685558 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-798.94, "episode_reward_trend_value": -0.05764550007515431, "biggest_recent_change": 3.815294368618197 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-799.61, "episode_reward_trend_value": -0.04635042432640451, "biggest_recent_change": 3.815294368618197 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-797.93, "episode_reward_trend_value": -0.027305863503096994, "biggest_recent_change": 3.815294368618197 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-799.86, "episode_reward_trend_value": -0.06090629214811164, "biggest_recent_change": 3.815294368618197 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-799.44, "episode_reward_trend_value": -0.013942318590779402, "biggest_recent_change": 1.924975919992221 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-797.46, "episode_reward_trend_value": 0.01226850812834578, "biggest_recent_change": 1.9832544602667213 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-796.00, "episode_reward_trend_value": 0.027633247034803944, "biggest_recent_change": 1.9832544602667213 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-798.10, "episode_reward_trend_value": 0.01943150810982009, "biggest_recent_change": 2.094678031747094 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-798.38, "episode_reward_trend_value": -0.0025655444736281023, "biggest_recent_change": 2.094678031747094 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-799.14, "episode_reward_trend_value": -0.002187610931879489, "biggest_recent_change": 2.094678031747094 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-799.57, "episode_reward_trend_value": 0.00042560757946653714, "biggest_recent_change": 2.094678031747094 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-798.49, "episode_reward_trend_value": -0.0061903066556068444, "biggest_recent_change": 2.094678031747094 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-799.04, "episode_reward_trend_value": 0.00908399782721795, "biggest_recent_change": 2.094678031747094 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-798.43, "episode_reward_trend_value": 0.01123826698805159, "biggest_recent_change": 2.094678031747094 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-798.40, "episode_reward_trend_value": -0.010418591488097364, "biggest_recent_change": 2.094678031747094 },
{"total_number_of_episodes":440, "number_of_timesteps":440000, "per_episode_reward":-799.63, "episode_reward_trend_value": -0.04029911999324188, "biggest_recent_change": 2.094678031747094 },
{"total_number_of_episodes":450, "number_of_timesteps":450000, "per_episode_reward":-799.30, "episode_reward_trend_value": -0.013379109885938257, "biggest_recent_change": 1.2303714961494734 },
{"total_number_of_episodes":460, "number_of_timesteps":460000, "per_episode_reward":-798.81, "episode_reward_trend_value": -0.00476829968105095, "biggest_recent_change": 1.2303714961494734 },
{"total_number_of_episodes":470, "number_of_timesteps":470000, "per_episode_reward":-798.98, "episode_reward_trend_value": 0.0017658336712757874, "biggest_recent_change": 1.2303714961494734 },
{"total_number_of_episodes":480, "number_of_timesteps":480000, "per_episode_reward":-798.03, "episode_reward_trend_value": 0.017124286646872454, "biggest_recent_change": 1.2303714961494734 },
{"total_number_of_episodes":490, "number_of_timesteps":490000, "per_episode_reward":-797.29, "episode_reward_trend_value": 0.013313309310560979, "biggest_recent_change": 1.2303714961494734 },
{"total_number_of_episodes":500, "number_of_timesteps":500000, "per_episode_reward":-797.82, "episode_reward_trend_value": 0.013581812846731132, "biggest_recent_change": 1.2303714961494734 },
{"total_number_of_episodes":510, "number_of_timesteps":510000, "per_episode_reward":-798.40, "episode_reward_trend_value": 0.00036535853462535466, "biggest_recent_change": 1.2303714961494734 },
{"total_number_of_episodes":520, "number_of_timesteps":520000, "per_episode_reward":-799.24, "episode_reward_trend_value": -0.009373901930090748, "biggest_recent_change": 1.2303714961494734 },
{"total_number_of_episodes":530, "number_of_timesteps":530000, "per_episode_reward":-798.88, "episode_reward_trend_value": 0.008331687583783302, "biggest_recent_change": 0.9473209163472802 },
{"total_number_of_episodes":540, "number_of_timesteps":540000, "per_episode_reward":-798.50, "episode_reward_trend_value": 0.008864413687096355, "biggest_recent_change": 0.9473209163472802 },
{"total_number_of_episodes":550, "number_of_timesteps":550000, "per_episode_reward":-798.01, "episode_reward_trend_value": 0.008868626360606414, "biggest_recent_change": 0.9473209163472802 },
{"total_number_of_episodes":560, "number_of_timesteps":560000, "per_episode_reward":-797.72, "episode_reward_trend_value": 0.013949869145967065, "biggest_recent_change": 0.9473209163472802 },
{"total_number_of_episodes":570, "number_of_timesteps":570000, "per_episode_reward":-797.71, "episode_reward_trend_value": 0.0035670570820482478, "biggest_recent_change": 0.8423962444111339 },
{"total_number_of_episodes":580, "number_of_timesteps":580000, "per_episode_reward":-797.55, "episode_reward_trend_value": -0.002907362389116012, "biggest_recent_change": 0.8423962444111339 },
{"total_number_of_episodes":590, "number_of_timesteps":590000, "per_episode_reward":-797.77, "episode_reward_trend_value": 0.00046856507624145, "biggest_recent_change": 0.8423962444111339 },
{"total_number_of_episodes":600, "number_of_timesteps":600000, "per_episode_reward":-797.26, "episode_reward_trend_value": 0.012654207525143749, "biggest_recent_change": 0.8423962444111339 },
{"total_number_of_episodes":610, "number_of_timesteps":610000, "per_episode_reward":-797.93, "episode_reward_trend_value": 0.014548109768547115, "biggest_recent_change": 0.6719450425048308 },
{"total_number_of_episodes":620, "number_of_timesteps":620000, "per_episode_reward":-797.33, "episode_reward_trend_value": 0.01717987941465506, "biggest_recent_change": 0.6719450425048308 },
{"total_number_of_episodes":630, "number_of_timesteps":630000, "per_episode_reward":-796.68, "episode_reward_trend_value": 0.020242603159855814, "biggest_recent_change": 0.6719450425048308 },
{"total_number_of_episodes":640, "number_of_timesteps":640000, "per_episode_reward":-796.16, "episode_reward_trend_value": 0.020593376957387794, "biggest_recent_change": 0.6719450425048308 },
{"total_number_of_episodes":650, "number_of_timesteps":650000, "per_episode_reward":-795.76, "episode_reward_trend_value": 0.021855729026583504, "biggest_recent_change": 0.6719450425048308 },
{"total_number_of_episodes":660, "number_of_timesteps":660000, "per_episode_reward":-795.77, "episode_reward_trend_value": 0.021565758487918175, "biggest_recent_change": 0.6719450425048308 },
{"total_number_of_episodes":670, "number_of_timesteps":670000, "per_episode_reward":-795.28, "episode_reward_trend_value": 0.02521169053570197, "biggest_recent_change": 0.6719450425048308 },
{"total_number_of_episodes":680, "number_of_timesteps":680000, "per_episode_reward":-795.59, "episode_reward_trend_value": 0.024276571949992054, "biggest_recent_change": 0.6719450425048308 },
{"total_number_of_episodes":690, "number_of_timesteps":690000, "per_episode_reward":-794.95, "episode_reward_trend_value": 0.02569672965679375, "biggest_recent_change": 0.6719450425048308 },
{"total_number_of_episodes":700, "number_of_timesteps":700000, "per_episode_reward":-794.00, "episode_reward_trend_value": 0.0437376264424971, "biggest_recent_change": 0.951735668208471 },
{"total_number_of_episodes":710, "number_of_timesteps":710000, "per_episode_reward":-794.79, "episode_reward_trend_value": 0.028261587374969265, "biggest_recent_change": 0.951735668208471 },
{"total_number_of_episodes":720, "number_of_timesteps":720000, "per_episode_reward":-794.85, "episode_reward_trend_value": 0.020382675660150402, "biggest_recent_change": 0.951735668208471 },
{"total_number_of_episodes":730, "number_of_timesteps":730000, "per_episode_reward":-794.53, "episode_reward_trend_value": 0.018143005750789597, "biggest_recent_change": 0.951735668208471 },
{"total_number_of_episodes":740, "number_of_timesteps":740000, "per_episode_reward":-793.87, "episode_reward_trend_value": 0.02099845922306132, "biggest_recent_change": 0.951735668208471 },
{"total_number_of_episodes":750, "number_of_timesteps":750000, "per_episode_reward":-794.07, "episode_reward_trend_value": 0.018901298809808193, "biggest_recent_change": 0.951735668208471 },
{"total_number_of_episodes":760, "number_of_timesteps":760000, "per_episode_reward":-794.78, "episode_reward_trend_value": 0.005627394238385679, "biggest_recent_change": 0.951735668208471 },
{"total_number_of_episodes":770, "number_of_timesteps":770000, "per_episode_reward":-795.59, "episode_reward_trend_value": -3.0891623127142034e-05, "biggest_recent_change": 0.951735668208471 },
{"total_number_of_episodes":780, "number_of_timesteps":780000, "per_episode_reward":-795.77, "episode_reward_trend_value": -0.00918344717250016, "biggest_recent_change": 0.951735668208471 },
{"total_number_of_episodes":790, "number_of_timesteps":790000, "per_episode_reward":-796.11, "episode_reward_trend_value": -0.02343436457009476, "biggest_recent_change": 0.8156961266505505 },
{"total_number_of_episodes":800, "number_of_timesteps":800000, "per_episode_reward":-795.49, "episode_reward_trend_value": -0.007770532000966392, "biggest_recent_change": 0.8156961266505505 },
{"total_number_of_episodes":810, "number_of_timesteps":810000, "per_episode_reward":-795.38, "episode_reward_trend_value": -0.005929549648769555, "biggest_recent_change": 0.8156961266505505 },
{"total_number_of_episodes":820, "number_of_timesteps":820000, "per_episode_reward":-795.85, "episode_reward_trend_value": -0.014707814386862436, "biggest_recent_change": 0.8156961266505505 },
{"total_number_of_episodes":830, "number_of_timesteps":830000, "per_episode_reward":-796.52, "episode_reward_trend_value": -0.02951300556464705, "biggest_recent_change": 0.8156961266505505 },
{"total_number_of_episodes":840, "number_of_timesteps":840000, "per_episode_reward":-796.22, "episode_reward_trend_value": -0.023942664435221812, "biggest_recent_change": 0.8156961266505505 },
{"total_number_of_episodes":850, "number_of_timesteps":850000, "per_episode_reward":-796.13, "episode_reward_trend_value": -0.015065867035773155, "biggest_recent_change": 0.8156961266505505 },
{"total_number_of_episodes":860, "number_of_timesteps":860000, "per_episode_reward":-795.82, "episode_reward_trend_value": -0.0024937552013120185, "biggest_recent_change": 0.6703303121726094 },
{"total_number_of_episodes":870, "number_of_timesteps":870000, "per_episode_reward":-795.65, "episode_reward_trend_value": 0.0013654256918471219, "biggest_recent_change": 0.6703303121726094 },
{"total_number_of_episodes":880, "number_of_timesteps":880000, "per_episode_reward":-795.21, "episode_reward_trend_value": 0.009971014906966502, "biggest_recent_change": 0.6703303121726094 },
{"total_number_of_episodes":890, "number_of_timesteps":890000, "per_episode_reward":-794.79, "episode_reward_trend_value": 0.0077764457549619696, "biggest_recent_change": 0.6703303121726094 },
{"total_number_of_episodes":900, "number_of_timesteps":900000, "per_episode_reward":-795.23, "episode_reward_trend_value": 0.0017039496137714297, "biggest_recent_change": 0.6703303121726094 },
{"total_number_of_episodes":910, "number_of_timesteps":910000, "per_episode_reward":-794.88, "episode_reward_trend_value": 0.010746523435619161, "biggest_recent_change": 0.6703303121726094 },
{"total_number_of_episodes":920, "number_of_timesteps":920000, "per_episode_reward":-795.48, "episode_reward_trend_value": 0.011591676525457235, "biggest_recent_change": 0.5942665340871827 },
{"total_number_of_episodes":930, "number_of_timesteps":930000, "per_episode_reward":-795.36, "episode_reward_trend_value": 0.009549365499578421, "biggest_recent_change": 0.5942665340871827 },
{"total_number_of_episodes":940, "number_of_timesteps":940000, "per_episode_reward":-795.52, "episode_reward_trend_value": 0.006835863974940064, "biggest_recent_change": 0.5942665340871827 },
{"total_number_of_episodes":950, "number_of_timesteps":950000, "per_episode_reward":-796.01, "episode_reward_trend_value": -0.002196073009378728, "biggest_recent_change": 0.5942665340871827 },
{"total_number_of_episodes":960, "number_of_timesteps":960000, "per_episode_reward":-795.95, "episode_reward_trend_value": -0.0033606378855008897, "biggest_recent_change": 0.5942665340871827 },
{"total_number_of_episodes":970, "number_of_timesteps":970000, "per_episode_reward":-796.09, "episode_reward_trend_value": -0.009812036725710033, "biggest_recent_change": 0.5942665340871827 },
{"total_number_of_episodes":980, "number_of_timesteps":980000, "per_episode_reward":-796.25, "episode_reward_trend_value": -0.016257698183023372, "biggest_recent_change": 0.5942665340871827 },
{"total_number_of_episodes":990, "number_of_timesteps":990000, "per_episode_reward":-795.90, "episode_reward_trend_value": -0.007520828170119229, "biggest_recent_change": 0.5942665340871827 },
{"total_number_of_episodes":1000, "number_of_timesteps":1000000, "per_episode_reward":-795.71, "episode_reward_trend_value": -0.009135274641798535, "biggest_recent_change": 0.5942665340871827 },
{"total_number_of_episodes":1010, "number_of_timesteps":1010000, "per_episode_reward":-795.95, "episode_reward_trend_value": -0.00524355624482319, "biggest_recent_change": 0.4970803901377394 },
{"total_number_of_episodes":1020, "number_of_timesteps":1020000, "per_episode_reward":-795.46, "episode_reward_trend_value": -0.0010634076089660184, "biggest_recent_change": 0.4970803901377394 },
{"total_number_of_episodes":1030, "number_of_timesteps":1030000, "per_episode_reward":-795.54, "episode_reward_trend_value": -0.0002987289829434303, "biggest_recent_change": 0.4970803901377394 },
{"total_number_of_episodes":1040, "number_of_timesteps":1040000, "per_episode_reward":-795.53, "episode_reward_trend_value": 0.00539903428712023, "biggest_recent_change": 0.4917621314682492 },
{"total_number_of_episodes":1050, "number_of_timesteps":1050000, "per_episode_reward":-795.04, "episode_reward_trend_value": 0.010172272590875765, "biggest_recent_change": 0.4917621314682492 },
{"total_number_of_episodes":1060, "number_of_timesteps":1060000, "per_episode_reward":-795.25, "episode_reward_trend_value": 0.009336137163613027, "biggest_recent_change": 0.4917621314682492 },
{"total_number_of_episodes":1070, "number_of_timesteps":1070000, "per_episode_reward":-795.27, "episode_reward_trend_value": 0.010887437873648449, "biggest_recent_change": 0.4917621314682492 },
{"total_number_of_episodes":1080, "number_of_timesteps":1080000, "per_episode_reward":-795.15, "episode_reward_trend_value": 0.008399012247979145, "biggest_recent_change": 0.4917621314682492 },
{"total_number_of_episodes":1090, "number_of_timesteps":1090000, "per_episode_reward":-795.15, "episode_reward_trend_value": 0.006185871438575911, "biggest_recent_change": 0.4917621314682492 },
{"total_number_of_episodes":1100, "number_of_timesteps":1100000, "per_episode_reward":-795.02, "episode_reward_trend_value": 0.010374581174710329, "biggest_recent_change": 0.4917621314682492 },
{"total_number_of_episodes":1110, "number_of_timesteps":1110000, "per_episode_reward":-795.28, "episode_reward_trend_value": 0.0019378144194787538, "biggest_recent_change": 0.48876549136832637 },
{"total_number_of_episodes":1120, "number_of_timesteps":1120000, "per_episode_reward":-795.53, "episode_reward_trend_value": 0.00014682329247206225, "biggest_recent_change": 0.48876549136832637 },
{"total_number_of_episodes":1130, "number_of_timesteps":1130000, "per_episode_reward":-795.13, "episode_reward_trend_value": 0.00442985153451395, "biggest_recent_change": 0.48876549136832637 },
{"total_number_of_episodes":1140, "number_of_timesteps":1140000, "per_episode_reward":-795.12, "episode_reward_trend_value": -0.0009497396870326864, "biggest_recent_change": 0.4011908459517599 },
{"total_number_of_episodes":1150, "number_of_timesteps":1150000, "per_episode_reward":-795.37, "episode_reward_trend_value": -0.0013702495150192185, "biggest_recent_change": 0.4011908459517599 },
{"total_number_of_episodes":1160, "number_of_timesteps":1160000, "per_episode_reward":-794.92, "episode_reward_trend_value": 0.003951699922254193, "biggest_recent_change": 0.4578640018121405 },
{"total_number_of_episodes":1170, "number_of_timesteps":1170000, "per_episode_reward":-794.58, "episode_reward_trend_value": 0.006266813947872328, "biggest_recent_change": 0.4578640018121405 },
{"total_number_of_episodes":1180, "number_of_timesteps":1180000, "per_episode_reward":-794.52, "episode_reward_trend_value": 0.006985436947594634, "biggest_recent_change": 0.4578640018121405 },
{"total_number_of_episodes":1190, "number_of_timesteps":1190000, "per_episode_reward":-794.56, "episode_reward_trend_value": 0.005114301250281035, "biggest_recent_change": 0.4578640018121405 },
{"total_number_of_episodes":1200, "number_of_timesteps":1200000, "per_episode_reward":-794.66, "episode_reward_trend_value": 0.0069562727436138835, "biggest_recent_change": 0.4578640018121405 },
{"total_number_of_episodes":1210, "number_of_timesteps":1210000, "per_episode_reward":-795.02, "episode_reward_trend_value": 0.005685406867336143, "biggest_recent_change": 0.4578640018121405 },
{"total_number_of_episodes":1220, "number_of_timesteps":1220000, "per_episode_reward":-794.95, "episode_reward_trend_value": 0.0019983133510916356, "biggest_recent_change": 0.4578640018121405 },
{"total_number_of_episodes":1230, "number_of_timesteps":1230000, "per_episode_reward":-794.99, "episode_reward_trend_value": 0.0014548539322946554, "biggest_recent_change": 0.4578640018121405 },
{"total_number_of_episodes":1240, "number_of_timesteps":1240000, "per_episode_reward":-795.18, "episode_reward_trend_value": 0.00215943816914306, "biggest_recent_change": 0.4578640018121405 },
{"total_number_of_episodes":1250, "number_of_timesteps":1250000, "per_episode_reward":-795.42, "episode_reward_trend_value": -0.005653149627616787, "biggest_recent_change": 0.35930170933067984 },
{"total_number_of_episodes":1260, "number_of_timesteps":1260000, "per_episode_reward":-795.64, "episode_reward_trend_value": -0.011763153254067522, "biggest_recent_change": 0.35930170933067984 },
{"total_number_of_episodes":1270, "number_of_timesteps":1270000, "per_episode_reward":-794.89, "episode_reward_trend_value": -0.004141457985480833, "biggest_recent_change": 0.7480559400543143 },
{"total_number_of_episodes":1280, "number_of_timesteps":1280000, "per_episode_reward":-795.47, "episode_reward_trend_value": -0.010151443458151891, "biggest_recent_change": 0.7480559400543143 },
{"total_number_of_episodes":1290, "number_of_timesteps":1290000, "per_episode_reward":-795.58, "episode_reward_trend_value": -0.010184678697496313, "biggest_recent_change": 0.7480559400543143 },
{"total_number_of_episodes":1300, "number_of_timesteps":1300000, "per_episode_reward":-795.29, "episode_reward_trend_value": -0.00306560245781687, "biggest_recent_change": 0.7480559400543143 },
{"total_number_of_episodes":1310, "number_of_timesteps":1310000, "per_episode_reward":-794.83, "episode_reward_trend_value": 0.0012860514821770468, "biggest_recent_change": 0.7480559400543143 },
{"total_number_of_episodes":1320, "number_of_timesteps":1320000, "per_episode_reward":-794.24, "episode_reward_trend_value": 0.008363057561262445, "biggest_recent_change": 0.7480559400543143 },
{"total_number_of_episodes":1330, "number_of_timesteps":1330000, "per_episode_reward":-794.23, "episode_reward_trend_value": 0.01057678878635847, "biggest_recent_change": 0.7480559400543143 },
{"total_number_of_episodes":1340, "number_of_timesteps":1340000, "per_episode_reward":-794.48, "episode_reward_trend_value": 0.010474222285536901, "biggest_recent_change": 0.7480559400543143 },
{"total_number_of_episodes":1350, "number_of_timesteps":1350000, "per_episode_reward":-794.82, "episode_reward_trend_value": 0.009163678459556953, "biggest_recent_change": 0.7480559400543143 },
{"total_number_of_episodes":1360, "number_of_timesteps":1360000, "per_episode_reward":-794.96, "episode_reward_trend_value": -0.0007830581358171508, "biggest_recent_change": 0.5926214808550867 },
{"total_number_of_episodes":1370, "number_of_timesteps":1370000, "per_episode_reward":-795.48, "episode_reward_trend_value": -0.00014284844938730303, "biggest_recent_change": 0.5926214808550867 },
{"total_number_of_episodes":1380, "number_of_timesteps":1380000, "per_episode_reward":-795.54, "episode_reward_trend_value": 0.00042809707159045097, "biggest_recent_change": 0.5926214808550867 },
{"total_number_of_episodes":1390, "number_of_timesteps":1390000, "per_episode_reward":-796.15, "episode_reward_trend_value": -0.00954530552532257, "biggest_recent_change": 0.616191081481702 },
{"total_number_of_episodes":1400, "number_of_timesteps":1400000, "per_episode_reward":-795.80, "episode_reward_trend_value": -0.010769208264972956, "biggest_recent_change": 0.616191081481702 },
{"total_number_of_episodes":1410, "number_of_timesteps":1410000, "per_episode_reward":-795.66, "episode_reward_trend_value": -0.015755251617210635, "biggest_recent_change": 0.616191081481702 },
{"total_number_of_episodes":1420, "number_of_timesteps":1420000, "per_episode_reward":-795.54, "episode_reward_trend_value": -0.01454258782044159, "biggest_recent_change": 0.616191081481702 },
{"total_number_of_episodes":1430, "number_of_timesteps":1430000, "per_episode_reward":-795.43, "episode_reward_trend_value": -0.010496866069883658, "biggest_recent_change": 0.616191081481702 },
{"total_number_of_episodes":1440, "number_of_timesteps":1440000, "per_episode_reward":-795.44, "episode_reward_trend_value": -0.006909787718169962, "biggest_recent_change": 0.616191081481702 },
{"total_number_of_episodes":1450, "number_of_timesteps":1450000, "per_episode_reward":-795.34, "episode_reward_trend_value": -0.00413827588034034, "biggest_recent_change": 0.616191081481702 },
{"total_number_of_episodes":1460, "number_of_timesteps":1460000, "per_episode_reward":-795.13, "episode_reward_trend_value": 0.003953611786358932, "biggest_recent_change": 0.616191081481702 },
{"total_number_of_episodes":1470, "number_of_timesteps":1470000, "per_episode_reward":-795.64, "episode_reward_trend_value": -0.0011575196087796435, "biggest_recent_change": 0.616191081481702 },
{"total_number_of_episodes":1480, "number_of_timesteps":1480000, "per_episode_reward":-795.18, "episode_reward_trend_value": 0.010790595828264788, "biggest_recent_change": 0.5133773423181083 },
{"total_number_of_episodes":1490, "number_of_timesteps":1490000, "per_episode_reward":-795.17, "episode_reward_trend_value": 0.007004490660597185, "biggest_recent_change": 0.5133773423181083 },
{"total_number_of_episodes":1500, "number_of_timesteps":1500000, "per_episode_reward":-795.16, "episode_reward_trend_value": 0.005513772582387193, "biggest_recent_change": 0.5133773423181083 },
{"total_number_of_episodes":1510, "number_of_timesteps":1510000, "per_episode_reward":-795.60, "episode_reward_trend_value": -0.0006991789160136048, "biggest_recent_change": 0.5133773423181083 },
{"total_number_of_episodes":1520, "number_of_timesteps":1520000, "per_episode_reward":-795.52, "episode_reward_trend_value": -0.0010866618952200548, "biggest_recent_change": 0.5133773423181083 },
{"total_number_of_episodes":1530, "number_of_timesteps":1530000, "per_episode_reward":-795.10, "episode_reward_trend_value": 0.003758805585737769, "biggest_recent_change": 0.5133773423181083 },
{"total_number_of_episodes":1540, "number_of_timesteps":1540000, "per_episode_reward":-795.11, "episode_reward_trend_value": 0.002510968737884569, "biggest_recent_change": 0.5133773423181083 },
{"total_number_of_episodes":1550, "number_of_timesteps":1550000, "per_episode_reward":-795.26, "episode_reward_trend_value": -0.001419757739913368, "biggest_recent_change": 0.5133773423181083 },
{"total_number_of_episodes":1560, "number_of_timesteps":1560000, "per_episode_reward":-795.47, "episode_reward_trend_value": 0.0018755760100917641, "biggest_recent_change": 0.45913930785229695 },
{"total_number_of_episodes":1570, "number_of_timesteps":1570000, "per_episode_reward":-795.47, "episode_reward_trend_value": -0.0032035097643630358, "biggest_recent_change": 0.4374453383774153 },
{"total_number_of_episodes":1580, "number_of_timesteps":1580000, "per_episode_reward":-795.53, "episode_reward_trend_value": -0.004029491413657777, "biggest_recent_change": 0.4374453383774153 },
{"total_number_of_episodes":1590, "number_of_timesteps":1590000, "per_episode_reward":-795.19, "episode_reward_trend_value": -0.0003053906651366055, "biggest_recent_change": 0.4374453383774153 },
{"total_number_of_episodes":1600, "number_of_timesteps":1600000, "per_episode_reward":-795.44, "episode_reward_trend_value": 0.0017497638301519046, "biggest_recent_change": 0.4235751803117864 },
{"total_number_of_episodes":1610, "number_of_timesteps":1610000, "per_episode_reward":-795.11, "episode_reward_trend_value": 0.004587930831492789, "biggest_recent_change": 0.4235751803117864 },
{"total_number_of_episodes":1620, "number_of_timesteps":1620000, "per_episode_reward":-795.50, "episode_reward_trend_value": -0.0044454208730144275, "biggest_recent_change": 0.389426473093863 },
{"total_number_of_episodes":1630, "number_of_timesteps":1630000, "per_episode_reward":-795.60, "episode_reward_trend_value": -0.0054253386810614755, "biggest_recent_change": 0.389426473093863 },
{"total_number_of_episodes":1640, "number_of_timesteps":1640000, "per_episode_reward":-795.13, "episode_reward_trend_value": 0.001403318300299361, "biggest_recent_change": 0.4703735996963587 },
{"total_number_of_episodes":1650, "number_of_timesteps":1650000, "per_episode_reward":-794.63, "episode_reward_trend_value": 0.009301516486525292, "biggest_recent_change": 0.49404053194268727 },
{"total_number_of_episodes":1660, "number_of_timesteps":1660000, "per_episode_reward":-794.74, "episode_reward_trend_value": 0.008126904149402333, "biggest_recent_change": 0.49404053194268727 },
{"total_number_of_episodes":1670, "number_of_timesteps":1670000, "per_episode_reward":-794.45, "episode_reward_trend_value": 0.012056421632376493, "biggest_recent_change": 0.49404053194268727 },
{"total_number_of_episodes":1680, "number_of_timesteps":1680000, "per_episode_reward":-794.95, "episode_reward_trend_value": 0.002626313593262037, "biggest_recent_change": 0.5038277040385992 },
{"total_number_of_episodes":1690, "number_of_timesteps":1690000, "per_episode_reward":-794.76, "episode_reward_trend_value": 0.0075963645378717744, "biggest_recent_change": 0.5038277040385992 },
{"total_number_of_episodes":1700, "number_of_timesteps":1700000, "per_episode_reward":-795.00, "episode_reward_trend_value": 0.001262687988183744, "biggest_recent_change": 0.5038277040385992 },
{"total_number_of_episodes":1710, "number_of_timesteps":1710000, "per_episode_reward":-795.21, "episode_reward_trend_value": 0.0032444327871227565, "biggest_recent_change": 0.5038277040385992 },
{"total_number_of_episodes":1720, "number_of_timesteps":1720000, "per_episode_reward":-795.09, "episode_reward_trend_value": 0.005711327715545798, "biggest_recent_change": 0.5038277040385992 },
{"total_number_of_episodes":1730, "number_of_timesteps":1730000, "per_episode_reward":-795.29, "episode_reward_trend_value": -0.0018240345437738545, "biggest_recent_change": 0.5038277040385992 },
{"total_number_of_episodes":1740, "number_of_timesteps":1740000, "per_episode_reward":-795.28, "episode_reward_trend_value": -0.007209413230489442, "biggest_recent_change": 0.5038277040385992 },
{"total_number_of_episodes":1750, "number_of_timesteps":1750000, "per_episode_reward":-795.01, "episode_reward_trend_value": -0.0030445693517486285, "biggest_recent_change": 0.5038277040385992 },
{"total_number_of_episodes":1760, "number_of_timesteps":1760000, "per_episode_reward":-794.71, "episode_reward_trend_value": -0.002879291821069627, "biggest_recent_change": 0.5038277040385992 },
{"total_number_of_episodes":1770, "number_of_timesteps":1770000, "per_episode_reward":-794.53, "episode_reward_trend_value": 0.004691662718566173, "biggest_recent_change": 0.3042937752228454 },
{"total_number_of_episodes":1780, "number_of_timesteps":1780000, "per_episode_reward":-794.53, "episode_reward_trend_value": 0.002486321798063626, "biggest_recent_change": 0.3042937752228454 },
{"total_number_of_episodes":1790, "number_of_timesteps":1790000, "per_episode_reward":-794.49, "episode_reward_trend_value": 0.005613355099586039, "biggest_recent_change": 0.3042937752228454 },
{"total_number_of_episodes":1800, "number_of_timesteps":1800000, "per_episode_reward":-794.63, "episode_reward_trend_value": 0.00641608514755237, "biggest_recent_change": 0.3042937752228454 },
{"total_number_of_episodes":1810, "number_of_timesteps":1810000, "per_episode_reward":-794.55, "episode_reward_trend_value": 0.005927119876024437, "biggest_recent_change": 0.3042937752228454 },
{"total_number_of_episodes":1820, "number_of_timesteps":1820000, "per_episode_reward":-794.53, "episode_reward_trend_value": 0.008425623505465612, "biggest_recent_change": 0.3042937752228454 },
{"total_number_of_episodes":1830, "number_of_timesteps":1830000, "per_episode_reward":-794.39, "episode_reward_trend_value": 0.009924254913216322, "biggest_recent_change": 0.3042937752228454 },
{"total_number_of_episodes":1840, "number_of_timesteps":1840000, "per_episode_reward":-794.50, "episode_reward_trend_value": 0.005684159012981934, "biggest_recent_change": 0.3042937752228454 },
{"total_number_of_episodes":1850, "number_of_timesteps":1850000, "per_episode_reward":-794.16, "episode_reward_trend_value": 0.006112313788133229, "biggest_recent_change": 0.34282770498646187 },
{"total_number_of_episodes":1860, "number_of_timesteps":1860000, "per_episode_reward":-794.52, "episode_reward_trend_value": 0.00014911019615813833, "biggest_recent_change": 0.35913011874913536 },
{"total_number_of_episodes":1870, "number_of_timesteps":1870000, "per_episode_reward":-794.56, "episode_reward_trend_value": -0.00031542666650794774, "biggest_recent_change": 0.35913011874913536 },
{"total_number_of_episodes":1880, "number_of_timesteps":1880000, "per_episode_reward":-794.45, "episode_reward_trend_value": 0.0004940532167160604, "biggest_recent_change": 0.35913011874913536 },
{"total_number_of_episodes":1890, "number_of_timesteps":1890000, "per_episode_reward":-794.41, "episode_reward_trend_value": 0.0024310371886915364, "biggest_recent_change": 0.35913011874913536 },
{"total_number_of_episodes":1900, "number_of_timesteps":1900000, "per_episode_reward":-794.27, "episode_reward_trend_value": 0.0031390494773707667, "biggest_recent_change": 0.35913011874913536 },
{"total_number_of_episodes":1910, "number_of_timesteps":1910000, "per_episode_reward":-794.27, "episode_reward_trend_value": 0.0029464933695963836, "biggest_recent_change": 0.35913011874913536 },
{"total_number_of_episodes":1920, "number_of_timesteps":1920000, "per_episode_reward":-794.14, "episode_reward_trend_value": 0.0027515040481337866, "biggest_recent_change": 0.35913011874913536 },
{"total_number_of_episodes":1930, "number_of_timesteps":1930000, "per_episode_reward":-794.02, "episode_reward_trend_value": 0.005344612890662069, "biggest_recent_change": 0.35913011874913536 },
{"total_number_of_episodes":1940, "number_of_timesteps":1940000, "per_episode_reward":-794.02, "episode_reward_trend_value": 0.0015092131570137704, "biggest_recent_change": 0.35913011874913536 },
{"total_number_of_episodes":1950, "number_of_timesteps":1950000, "per_episode_reward":-793.92, "episode_reward_trend_value": 0.0066814926524115335, "biggest_recent_change": 0.14352256794597906 },
{"total_number_of_episodes":1960, "number_of_timesteps":1960000, "per_episode_reward":-793.92, "episode_reward_trend_value": 0.007123705481640425, "biggest_recent_change": 0.14352256794597906 },
{"total_number_of_episodes":1970, "number_of_timesteps":1970000, "per_episode_reward":-793.97, "episode_reward_trend_value": 0.005358870566763269, "biggest_recent_change": 0.14352256794597906 },
{"total_number_of_episodes":1980, "number_of_timesteps":1980000, "per_episode_reward":-793.95, "episode_reward_trend_value": 0.005111742730840281, "biggest_recent_change": 0.14352256794597906 },
{"total_number_of_episodes":1990, "number_of_timesteps":1990000, "per_episode_reward":-793.73, "episode_reward_trend_value": 0.005968204250656426, "biggest_recent_change": 0.22060410472943204 },
{"total_number_of_episodes":2000, "number_of_timesteps":2000000, "per_episode_reward":-793.81, "episode_reward_trend_value": 0.005120042541509267, "biggest_recent_change": 0.22060410472943204 },
{"total_number_of_episodes":2010, "number_of_timesteps":2010000, "per_episode_reward":-793.70, "episode_reward_trend_value": 0.004962006935603666, "biggest_recent_change": 0.22060410472943204 },
{"total_number_of_episodes":2020, "number_of_timesteps":2020000, "per_episode_reward":-793.86, "episode_reward_trend_value": 0.0018225914677044684, "biggest_recent_change": 0.22060410472943204 },
{"total_number_of_episodes":2030, "number_of_timesteps":2030000, "per_episode_reward":-793.83, "episode_reward_trend_value": 0.002186960184179851, "biggest_recent_change": 0.22060410472943204 },
{"total_number_of_episodes":2040, "number_of_timesteps":2040000, "per_episode_reward":-793.88, "episode_reward_trend_value": 0.0003586660993871899, "biggest_recent_change": 0.22060410472943204 },
{"total_number_of_episodes":2050, "number_of_timesteps":2050000, "per_episode_reward":-793.86, "episode_reward_trend_value": 0.0007011145763486739, "biggest_recent_change": 0.22060410472943204 },
{"total_number_of_episodes":2060, "number_of_timesteps":2060000, "per_episode_reward":-793.90, "episode_reward_trend_value": 0.0006796747563587107, "biggest_recent_change": 0.22060410472943204 },
{"total_number_of_episodes":2070, "number_of_timesteps":2070000, "per_episode_reward":-793.73, "episode_reward_trend_value": 0.0025138096549009487, "biggest_recent_change": 0.22060410472943204 },
{"total_number_of_episodes":2080, "number_of_timesteps":2080000, "per_episode_reward":-793.79, "episode_reward_trend_value": -0.0005912628701328988, "biggest_recent_change": 0.17833545624114322 },
{"total_number_of_episodes":2090, "number_of_timesteps":2090000, "per_episode_reward":-793.95, "episode_reward_trend_value": -0.0015675049532369993, "biggest_recent_change": 0.17833545624114322 },
{"total_number_of_episodes":2100, "number_of_timesteps":2100000, "per_episode_reward":-793.73, "episode_reward_trend_value": -0.00033461270412418444, "biggest_recent_change": 0.22342133579286383 },
{"total_number_of_episodes":2110, "number_of_timesteps":2110000, "per_episode_reward":-793.83, "episode_reward_trend_value": 0.00023672640723159728, "biggest_recent_change": 0.22342133579286383 },
{"total_number_of_episodes":2120, "number_of_timesteps":2120000, "per_episode_reward":-793.82, "episode_reward_trend_value": 2.0527590941633713e-05, "biggest_recent_change": 0.22342133579286383 },
{"total_number_of_episodes":2130, "number_of_timesteps":2130000, "per_episode_reward":-793.99, "episode_reward_trend_value": -0.0011653520368642804, "biggest_recent_change": 0.22342133579286383 },
{"total_number_of_episodes":2140, "number_of_timesteps":2140000, "per_episode_reward":-793.93, "episode_reward_trend_value": -0.0007483193771842404, "biggest_recent_change": 0.22342133579286383 },
{"total_number_of_episodes":2150, "number_of_timesteps":2150000, "per_episode_reward":-794.13, "episode_reward_trend_value": -0.0025137629923405154, "biggest_recent_change": 0.22342133579286383 },
{"total_number_of_episodes":2160, "number_of_timesteps":2160000, "per_episode_reward":-793.90, "episode_reward_trend_value": -0.0019165356958511539, "biggest_recent_change": 0.23208591292518577 },
{"total_number_of_episodes":2170, "number_of_timesteps":2170000, "per_episode_reward":-793.81, "episode_reward_trend_value": -0.00030379285752057207, "biggest_recent_change": 0.23208591292518577 },
{"total_number_of_episodes":2180, "number_of_timesteps":2180000, "per_episode_reward":-793.76, "episode_reward_trend_value": 0.002100901482664439, "biggest_recent_change": 0.23208591292518577 },
{"total_number_of_episodes":2190, "number_of_timesteps":2190000, "per_episode_reward":-793.87, "episode_reward_trend_value": -0.001622622163961296, "biggest_recent_change": 0.23208591292518577 },
{"total_number_of_episodes":2200, "number_of_timesteps":2200000, "per_episode_reward":-793.61, "episode_reward_trend_value": 0.0024516408990456913, "biggest_recent_change": 0.25847039528514415 },
{"total_number_of_episodes":2210, "number_of_timesteps":2210000, "per_episode_reward":-793.60, "episode_reward_trend_value": 0.0024698770369847504, "biggest_recent_change": 0.25847039528514415 },
{"total_number_of_episodes":2220, "number_of_timesteps":2220000, "per_episode_reward":-793.51, "episode_reward_trend_value": 0.005320230768633236, "biggest_recent_change": 0.25847039528514415 },
{"total_number_of_episodes":2230, "number_of_timesteps":2230000, "per_episode_reward":-793.46, "episode_reward_trend_value": 0.005126242691538765, "biggest_recent_change": 0.25847039528514415 },
{"total_number_of_episodes":2240, "number_of_timesteps":2240000, "per_episode_reward":-793.60, "episode_reward_trend_value": 0.00595329294917772, "biggest_recent_change": 0.25847039528514415 },
{"total_number_of_episodes":2250, "number_of_timesteps":2250000, "per_episode_reward":-793.70, "episode_reward_trend_value": 0.002190468349486865, "biggest_recent_change": 0.25847039528514415 },
{"total_number_of_episodes":2260, "number_of_timesteps":2260000, "per_episode_reward":-793.40, "episode_reward_trend_value": 0.004631882556961702, "biggest_recent_change": 0.3060217115988735 },
{"total_number_of_episodes":2270, "number_of_timesteps":2270000, "per_episode_reward":-793.12, "episode_reward_trend_value": 0.007123580908912824, "biggest_recent_change": 0.3060217115988735 },
{"total_number_of_episodes":2280, "number_of_timesteps":2280000, "per_episode_reward":-793.11, "episode_reward_trend_value": 0.008445239198156035, "biggest_recent_change": 0.3060217115988735 },
{"total_number_of_episodes":2290, "number_of_timesteps":2290000, "per_episode_reward":-793.15, "episode_reward_trend_value": 0.0051928150451102635, "biggest_recent_change": 0.3060217115988735 },
{"total_number_of_episodes":2300, "number_of_timesteps":2300000, "per_episode_reward":-793.16, "episode_reward_trend_value": 0.004882957940598468, "biggest_recent_change": 0.3060217115988735 },
{"total_number_of_episodes":2310, "number_of_timesteps":2310000, "per_episode_reward":-792.82, "episode_reward_trend_value": 0.007672083428335303, "biggest_recent_change": 0.3426525314474702 },
{"total_number_of_episodes":2320, "number_of_timesteps":2320000, "per_episode_reward":-792.69, "episode_reward_trend_value": 0.008576218873025078, "biggest_recent_change": 0.3426525314474702 },
{"total_number_of_episodes":2330, "number_of_timesteps":2330000, "per_episode_reward":-792.69, "episode_reward_trend_value": 0.010059438225091021, "biggest_recent_change": 0.3426525314474702 },
{"total_number_of_episodes":2340, "number_of_timesteps":2340000, "per_episode_reward":-792.44, "episode_reward_trend_value": 0.013967194452274928, "biggest_recent_change": 0.3426525314474702 },
{"total_number_of_episodes":2350, "number_of_timesteps":2350000, "per_episode_reward":-792.29, "episode_reward_trend_value": 0.012233557960651826, "biggest_recent_change": 0.3426525314474702 },
{"total_number_of_episodes":2360, "number_of_timesteps":2360000, "per_episode_reward":-792.33, "episode_reward_trend_value": 0.00873599662547046, "biggest_recent_change": 0.3426525314474702 },
{"total_number_of_episodes":2370, "number_of_timesteps":2370000, "per_episode_reward":-792.28, "episode_reward_trend_value": 0.009235906651069097, "biggest_recent_change": 0.3426525314474702 },
{"total_number_of_episodes":2380, "number_of_timesteps":2380000, "per_episode_reward":-792.26, "episode_reward_trend_value": 0.009801085243316266, "biggest_recent_change": 0.3426525314474702 },
{"total_number_of_episodes":2390, "number_of_timesteps":2390000, "per_episode_reward":-792.34, "episode_reward_trend_value": 0.00910369649036511, "biggest_recent_change": 0.3426525314474702 },
{"total_number_of_episodes":2400, "number_of_timesteps":2400000, "per_episode_reward":-792.41, "episode_reward_trend_value": 0.004560428620038692, "biggest_recent_change": 0.2451297593995605 },
{"total_number_of_episodes":2410, "number_of_timesteps":2410000, "per_episode_reward":-792.26, "episode_reward_trend_value": 0.004856003421363968, "biggest_recent_change": 0.2451297593995605 },
{"total_number_of_episodes":2420, "number_of_timesteps":2420000, "per_episode_reward":-792.23, "episode_reward_trend_value": 0.00513610048088948, "biggest_recent_change": 0.2451297593995605 },
{"total_number_of_episodes":2430, "number_of_timesteps":2430000, "per_episode_reward":-792.29, "episode_reward_trend_value": 0.001737290902629714, "biggest_recent_change": 0.15320160285943984 },
{"total_number_of_episodes":2440, "number_of_timesteps":2440000, "per_episode_reward":-792.24, "episode_reward_trend_value": 0.0005722510538589632, "biggest_recent_change": 0.15320160285943984 },
{"total_number_of_episodes":2450, "number_of_timesteps":2450000, "per_episode_reward":-792.32, "episode_reward_trend_value": 0.00010671046288987579, "biggest_recent_change": 0.15320160285943984 },
{"total_number_of_episodes":2460, "number_of_timesteps":2460000, "per_episode_reward":-792.22, "episode_reward_trend_value": 0.0006804840982795213, "biggest_recent_change": 0.15320160285943984 },
{"total_number_of_episodes":2470, "number_of_timesteps":2470000, "per_episode_reward":-792.43, "episode_reward_trend_value": -0.0017963193495057165, "biggest_recent_change": 0.20629401548740134 },
{"total_number_of_episodes":2480, "number_of_timesteps":2480000, "per_episode_reward":-792.30, "episode_reward_trend_value": 0.00047265188554395835, "biggest_recent_change": 0.20629401548740134 },
{"total_number_of_episodes":2490, "number_of_timesteps":2490000, "per_episode_reward":-792.21, "episode_reward_trend_value": 0.002251270314620039, "biggest_recent_change": 0.20629401548740134 },
{"total_number_of_episodes":2500, "number_of_timesteps":2500000, "per_episode_reward":-792.11, "episode_reward_trend_value": 0.0015682596204732765, "biggest_recent_change": 0.20629401548740134 },
{"total_number_of_episodes":2510, "number_of_timesteps":2510000, "per_episode_reward":-792.18, "episode_reward_trend_value": 0.0005198668970011668, "biggest_recent_change": 0.20629401548740134 },
{"total_number_of_episodes":2520, "number_of_timesteps":2520000, "per_episode_reward":-792.09, "episode_reward_trend_value": 0.0022460817894473925, "biggest_recent_change": 0.20629401548740134 },
{"total_number_of_episodes":2530, "number_of_timesteps":2530000, "per_episode_reward":-792.25, "episode_reward_trend_value": -0.00010021453190221211, "biggest_recent_change": 0.20629401548740134 },
{"total_number_of_episodes":2540, "number_of_timesteps":2540000, "per_episode_reward":-792.31, "episode_reward_trend_value": 9.843569382231483e-05, "biggest_recent_change": 0.20629401548740134 },
{"total_number_of_episodes":2550, "number_of_timesteps":2550000, "per_episode_reward":-792.32, "episode_reward_trend_value": -0.0011103703052236192, "biggest_recent_change": 0.20629401548740134 },
{"total_number_of_episodes":2560, "number_of_timesteps":2560000, "per_episode_reward":-792.52, "episode_reward_trend_value": -0.0010848180702914254, "biggest_recent_change": 0.2039943143435039 },
{"total_number_of_episodes":2570, "number_of_timesteps":2570000, "per_episode_reward":-792.19, "episode_reward_trend_value": 0.0011913380545365726, "biggest_recent_change": 0.33102760760664296 },
{"total_number_of_episodes":2580, "number_of_timesteps":2580000, "per_episode_reward":-792.37, "episode_reward_trend_value": -0.0018283497384636068, "biggest_recent_change": 0.33102760760664296 },
{"total_number_of_episodes":2590, "number_of_timesteps":2590000, "per_episode_reward":-792.64, "episode_reward_trend_value": -0.005819803259710928, "biggest_recent_change": 0.33102760760664296 },
{"total_number_of_episodes":2600, "number_of_timesteps":2600000, "per_episode_reward":-792.82, "episode_reward_trend_value": -0.007051560586829611, "biggest_recent_change": 0.33102760760664296 },
{"total_number_of_episodes":2610, "number_of_timesteps":2610000, "per_episode_reward":-792.35, "episode_reward_trend_value": -0.0029458562396598607, "biggest_recent_change": 0.46410962892161933 },
{"total_number_of_episodes":2620, "number_of_timesteps":2620000, "per_episode_reward":-792.28, "episode_reward_trend_value": -0.0002865241950922205, "biggest_recent_change": 0.46410962892161933 },
{"total_number_of_episodes":2630, "number_of_timesteps":2630000, "per_episode_reward":-792.59, "episode_reward_trend_value": -0.003021629921107029, "biggest_recent_change": 0.46410962892161933 },
{"total_number_of_episodes":2640, "number_of_timesteps":2640000, "per_episode_reward":-792.30, "episode_reward_trend_value": 0.00025282786455262693, "biggest_recent_change": 0.46410962892161933 },
{"total_number_of_episodes":2650, "number_of_timesteps":2650000, "per_episode_reward":-792.24, "episode_reward_trend_value": 0.0031986960784997284, "biggest_recent_change": 0.46410962892161933 },
{"total_number_of_episodes":2660, "number_of_timesteps":2660000, "per_episode_reward":-792.56, "episode_reward_trend_value": -0.0040527117448884075, "biggest_recent_change": 0.46410962892161933 },
{"total_number_of_episodes":2670, "number_of_timesteps":2670000, "per_episode_reward":-792.30, "episode_reward_trend_value": 0.0007741409807939211, "biggest_recent_change": 0.46410962892161933 },
{"total_number_of_episodes":2680, "number_of_timesteps":2680000, "per_episode_reward":-792.36, "episode_reward_trend_value": 0.0030906199014414898, "biggest_recent_change": 0.46410962892161933 },
{"total_number_of_episodes":2690, "number_of_timesteps":2690000, "per_episode_reward":-792.49, "episode_reward_trend_value": 0.003585099263476524, "biggest_recent_change": 0.46410962892161933 },
{"total_number_of_episodes":2700, "number_of_timesteps":2700000, "per_episode_reward":-792.42, "episode_reward_trend_value": -0.0007964786262543081, "biggest_recent_change": 0.3215990964982893 },
{"total_number_of_episodes":2710, "number_of_timesteps":2710000, "per_episode_reward":-792.41, "episode_reward_trend_value": -0.001453352704486911, "biggest_recent_change": 0.3215990964982893 },
{"total_number_of_episodes":2720, "number_of_timesteps":2720000, "per_episode_reward":-792.59, "episode_reward_trend_value": -3.788797145539825e-05, "biggest_recent_change": 0.3215990964982893 },
{"total_number_of_episodes":2730, "number_of_timesteps":2730000, "per_episode_reward":-792.42, "episode_reward_trend_value": -0.0013748738344563408, "biggest_recent_change": 0.3215990964982893 },
{"total_number_of_episodes":2740, "number_of_timesteps":2740000, "per_episode_reward":-792.70, "episode_reward_trend_value": -0.005204837960852476, "biggest_recent_change": 0.3215990964982893 },
{"total_number_of_episodes":2750, "number_of_timesteps":2750000, "per_episode_reward":-792.74, "episode_reward_trend_value": -0.002072337744606203, "biggest_recent_change": 0.2835629464639169 },
{"total_number_of_episodes":2760, "number_of_timesteps":2760000, "per_episode_reward":-792.90, "episode_reward_trend_value": -0.006698670475700258, "biggest_recent_change": 0.2835629464639169 },
{"total_number_of_episodes":2770, "number_of_timesteps":2770000, "per_episode_reward":-792.90, "episode_reward_trend_value": -0.006030820030911046, "biggest_recent_change": 0.2835629464639169 },
{"total_number_of_episodes":2780, "number_of_timesteps":2780000, "per_episode_reward":-792.98, "episode_reward_trend_value": -0.0054521412223051706, "biggest_recent_change": 0.2835629464639169 },
{"total_number_of_episodes":2790, "number_of_timesteps":2790000, "per_episode_reward":-792.55, "episode_reward_trend_value": -0.0014249602116150426, "biggest_recent_change": 0.43221390980795604 },
{"total_number_of_episodes":2800, "number_of_timesteps":2800000, "per_episode_reward":-792.58, "episode_reward_trend_value": -0.0018857268168390976, "biggest_recent_change": 0.43221390980795604 },
{"total_number_of_episodes":2810, "number_of_timesteps":2810000, "per_episode_reward":-792.54, "episode_reward_trend_value": 0.0005882169188427927, "biggest_recent_change": 0.43221390980795604 },
{"total_number_of_episodes":2820, "number_of_timesteps":2820000, "per_episode_reward":-792.61, "episode_reward_trend_value": -0.0021357584112694085, "biggest_recent_change": 0.43221390980795604 },
{"total_number_of_episodes":2830, "number_of_timesteps":2830000, "per_episode_reward":-792.68, "episode_reward_trend_value": 0.0003199523458521172, "biggest_recent_change": 0.43221390980795604 },
{"total_number_of_episodes":2840, "number_of_timesteps":2840000, "per_episode_reward":-792.48, "episode_reward_trend_value": 0.002976402163451692, "biggest_recent_change": 0.43221390980795604 },
{"total_number_of_episodes":2850, "number_of_timesteps":2850000, "per_episode_reward":-792.38, "episode_reward_trend_value": 0.0058230638002922, "biggest_recent_change": 0.43221390980795604 },
{"total_number_of_episodes":2860, "number_of_timesteps":2860000, "per_episode_reward":-792.27, "episode_reward_trend_value": 0.00699154872813147, "biggest_recent_change": 0.43221390980795604 },
{"total_number_of_episodes":2870, "number_of_timesteps":2870000, "per_episode_reward":-792.48, "episode_reward_trend_value": 0.005575065141220472, "biggest_recent_change": 0.43221390980795604 },
{"total_number_of_episodes":2880, "number_of_timesteps":2880000, "per_episode_reward":-792.17, "episode_reward_trend_value": 0.004270666244608239, "biggest_recent_change": 0.3148180091128552 },
{"total_number_of_episodes":2890, "number_of_timesteps":2890000, "per_episode_reward":-792.42, "episode_reward_trend_value": 0.0017463751496153337, "biggest_recent_change": 0.3148180091128552 },
{"total_number_of_episodes":2900, "number_of_timesteps":2900000, "per_episode_reward":-792.18, "episode_reward_trend_value": 0.003994267013659838, "biggest_recent_change": 0.3148180091128552 },
{"total_number_of_episodes":2910, "number_of_timesteps":2910000, "per_episode_reward":-792.04, "episode_reward_trend_value": 0.006317169740854449, "biggest_recent_change": 0.3148180091128552 },
{"total_number_of_episodes":2920, "number_of_timesteps":2920000, "per_episode_reward":-792.03, "episode_reward_trend_value": 0.007130169961389281, "biggest_recent_change": 0.3148180091128552 },
{"total_number_of_episodes":2930, "number_of_timesteps":2930000, "per_episode_reward":-791.90, "episode_reward_trend_value": 0.006381852535087597, "biggest_recent_change": 0.3148180091128552 },
{"total_number_of_episodes":2940, "number_of_timesteps":2940000, "per_episode_reward":-791.94, "episode_reward_trend_value": 0.004879584366531415, "biggest_recent_change": 0.3148180091128552 },
{"total_number_of_episodes":2950, "number_of_timesteps":2950000, "per_episode_reward":-792.08, "episode_reward_trend_value": 0.002194846168602756, "biggest_recent_change": 0.3148180091128552 },
{"total_number_of_episodes":2960, "number_of_timesteps":2960000, "per_episode_reward":-791.97, "episode_reward_trend_value": 0.005708335854587352, "biggest_recent_change": 0.3148180091128552 },
{"total_number_of_episodes":2970, "number_of_timesteps":2970000, "per_episode_reward":-792.08, "episode_reward_trend_value": 0.0009387065184897538, "biggest_recent_change": 0.25445980400741064 },
{"total_number_of_episodes":2980, "number_of_timesteps":2980000, "per_episode_reward":-792.29, "episode_reward_trend_value": 0.0014153013786994961, "biggest_recent_change": 0.24360213586578539 },
{"total_number_of_episodes":2990, "number_of_timesteps":2990000, "per_episode_reward":-792.15, "episode_reward_trend_value": 0.0002678988287496193, "biggest_recent_change": 0.21156626658853384 },
{"total_number_of_episodes":3000, "number_of_timesteps":3000000, "per_episode_reward":-792.14, "episode_reward_trend_value": -0.0011122572676969058, "biggest_recent_change": 0.21156626658853384 },
{"total_number_of_episodes":3010, "number_of_timesteps":3010000, "per_episode_reward":-792.39, "episode_reward_trend_value": -0.003912072484296106, "biggest_recent_change": 0.24136232796877266 },
{"total_number_of_episodes":3020, "number_of_timesteps":3020000, "per_episode_reward":-792.32, "episode_reward_trend_value": -0.004608910486498038, "biggest_recent_change": 0.24136232796877266 },
{"total_number_of_episodes":3030, "number_of_timesteps":3030000, "per_episode_reward":-792.24, "episode_reward_trend_value": -0.0033566402567683125, "biggest_recent_change": 0.24136232796877266 },
{"total_number_of_episodes":3040, "number_of_timesteps":3040000, "per_episode_reward":-792.27, "episode_reward_trend_value": -0.0021359445717962443, "biggest_recent_change": 0.24136232796877266 },
{"total_number_of_episodes":3050, "number_of_timesteps":3050000, "per_episode_reward":-792.23, "episode_reward_trend_value": -0.002895058898919463, "biggest_recent_change": 0.24136232796877266 },
{"total_number_of_episodes":3060, "number_of_timesteps":3060000, "per_episode_reward":-792.63, "episode_reward_trend_value": -0.006124629512448286, "biggest_recent_change": 0.4051099863535228 },
{"total_number_of_episodes":3070, "number_of_timesteps":3070000, "per_episode_reward":-792.31, "episode_reward_trend_value": -0.00022877799479677682, "biggest_recent_change": 0.4051099863535228 },
{"total_number_of_episodes":3080, "number_of_timesteps":3080000, "per_episode_reward":-792.25, "episode_reward_trend_value": -0.00102872369699551, "biggest_recent_change": 0.4051099863535228 },
{"total_number_of_episodes":3090, "number_of_timesteps":3090000, "per_episode_reward":-792.00, "episode_reward_trend_value": 0.0015534859907006648, "biggest_recent_change": 0.4051099863535228 },
{"total_number_of_episodes":3100, "number_of_timesteps":3100000, "per_episode_reward":-792.00, "episode_reward_trend_value": 0.004242520332520093, "biggest_recent_change": 0.4051099863535228 },
{"total_number_of_episodes":3110, "number_of_timesteps":3110000, "per_episode_reward":-791.69, "episode_reward_trend_value": 0.006909971202055153, "biggest_recent_change": 0.4051099863535228 },
{"total_number_of_episodes":3120, "number_of_timesteps":3120000, "per_episode_reward":-791.63, "episode_reward_trend_value": 0.006817890038826135, "biggest_recent_change": 0.4051099863535228 },
{"total_number_of_episodes":3130, "number_of_timesteps":3130000, "per_episode_reward":-791.56, "episode_reward_trend_value": 0.007906568720467374, "biggest_recent_change": 0.4051099863535228 },
{"total_number_of_episodes":3140, "number_of_timesteps":3140000, "per_episode_reward":-791.65, "episode_reward_trend_value": 0.006477475073378021, "biggest_recent_change": 0.4051099863535228 },
{"total_number_of_episodes":3150, "number_of_timesteps":3150000, "per_episode_reward":-791.67, "episode_reward_trend_value": 0.010659275923755256, "biggest_recent_change": 0.319060370000102 },
{"total_number_of_episodes":3160, "number_of_timesteps":3160000, "per_episode_reward":-791.52, "episode_reward_trend_value": 0.008870106473729164, "biggest_recent_change": 0.30941299624066687 },
{"total_number_of_episodes":3170, "number_of_timesteps":3170000, "per_episode_reward":-791.55, "episode_reward_trend_value": 0.007689201140514494, "biggest_recent_change": 0.30941299624066687 },
{"total_number_of_episodes":3180, "number_of_timesteps":3180000, "per_episode_reward":-791.66, "episode_reward_trend_value": 0.003880004790242866, "biggest_recent_change": 0.30941299624066687 },
{"total_number_of_episodes":3190, "number_of_timesteps":3190000, "per_episode_reward":-791.63, "episode_reward_trend_value": 0.004127834449409167, "biggest_recent_change": 0.30941299624066687 },
{"total_number_of_episodes":3200, "number_of_timesteps":3200000, "per_episode_reward":-791.59, "episode_reward_trend_value": 0.0011802864313280124, "biggest_recent_change": 0.15803511949775384 },
{"total_number_of_episodes":3210, "number_of_timesteps":3210000, "per_episode_reward":-791.58, "episode_reward_trend_value": 0.0005628802185949805, "biggest_recent_change": 0.15803511949775384 },
{"total_number_of_episodes":3220, "number_of_timesteps":3220000, "per_episode_reward":-791.48, "episode_reward_trend_value": 0.0009062544076363742, "biggest_recent_change": 0.15803511949775384 },
{"total_number_of_episodes":3230, "number_of_timesteps":3230000, "per_episode_reward":-791.45, "episode_reward_trend_value": 0.002131455049029935, "biggest_recent_change": 0.15803511949775384 },
{"total_number_of_episodes":3240, "number_of_timesteps":3240000, "per_episode_reward":-791.48, "episode_reward_trend_value": 0.0021810083316217465, "biggest_recent_change": 0.15803511949775384 },
{"total_number_of_episodes":3250, "number_of_timesteps":3250000, "per_episode_reward":-791.50, "episode_reward_trend_value": 0.00021993067798323158, "biggest_recent_change": 0.10337404206416068 },
{"total_number_of_episodes":3260, "number_of_timesteps":3260000, "per_episode_reward":-791.53, "episode_reward_trend_value": 0.0002503371625759125, "biggest_recent_change": 0.10337404206416068 },
{"total_number_of_episodes":3270, "number_of_timesteps":3270000, "per_episode_reward":-791.60, "episode_reward_trend_value": 0.0006581290423039516, "biggest_recent_change": 0.10337404206416068 },
{"total_number_of_episodes":3280, "number_of_timesteps":3280000, "per_episode_reward":-791.63, "episode_reward_trend_value": 3.451483193607095e-05, "biggest_recent_change": 0.10337404206416068 },
{"total_number_of_episodes":3290, "number_of_timesteps":3290000, "per_episode_reward":-791.59, "episode_reward_trend_value": -4.7346764059360184e-05, "biggest_recent_change": 0.10337404206416068 },
{"total_number_of_episodes":3300, "number_of_timesteps":3300000, "per_episode_reward":-791.66, "episode_reward_trend_value": -0.0009122540812124194, "biggest_recent_change": 0.10337404206416068 },
Hit early stopping because biggest_recent_change: 0.06788680968122662 < 0.1
{"total_number_of_episodes":3310, "number_of_timesteps":3310000, "per_episode_reward":-791.65, "episode_reward_trend_value": -0.0019325638227415867, "biggest_recent_change": 0.06788680968122662 },




Process Process-27:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-24:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:02:15,525][0m Trial 2 finished with value: -791.672933570046 and parameters: {'learning_rate': 2, 'variance_scaling_factor': 2, 'permaban_threshold': 1}. Best is trial 1 with value: -706.6358055663736.[0m




Process Process-34:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-793.08, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-798.66, "episode_reward_trend_value": -0.5582140734513814, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-814.84, "episode_reward_trend_value": -1.087776025516837, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-810.06, "episode_reward_trend_value": -0.5660841903859667, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-806.35, "episode_reward_trend_value": -0.33163214116684403, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-805.28, "episode_reward_trend_value": -0.24387347461508851, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-803.06, "episode_reward_trend_value": -0.16637873756369573, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-805.79, "episode_reward_trend_value": -0.1814801088968287, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-805.85, "episode_reward_trend_value": -0.15960477296215458, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-809.22, "episode_reward_trend_value": -0.17930882154067704, "biggest_recent_change": 16.173379775822923 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-806.09, "episode_reward_trend_value": -0.08255528963093942, "biggest_recent_change": 16.173379775822923 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-804.26, "episode_reward_trend_value": 0.11755482982325197, "biggest_recent_change": 4.772994798757736 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-803.19, "episode_reward_trend_value": 0.07635740721563403, "biggest_recent_change": 3.7172400649052406 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-803.92, "episode_reward_trend_value": 0.027005036983909277, "biggest_recent_change": 3.3694121016885674 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-805.92, "episode_reward_trend_value": -0.0071343161831792816, "biggest_recent_change": 3.3694121016885674 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-809.31, "episode_reward_trend_value": -0.06933962135569546, "biggest_recent_change": 3.3875279885937744 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-808.56, "episode_reward_trend_value": -0.0308766055155666, "biggest_recent_change": 3.3875279885937744 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-805.91, "episode_reward_trend_value": -0.0006992703381961797, "biggest_recent_change": 3.3875279885937744 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-804.70, "episode_reward_trend_value": 0.05024012057684407, "biggest_recent_change": 3.3875279885937744 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-800.55, "episode_reward_trend_value": 0.06164527583582766, "biggest_recent_change": 4.152141110671096 },
Hit early stopping because per_episode_reward: -800.5461010556875 < -800




Process Process-40:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-39:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-31:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-33:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:02:36,627][0m Trial 3 finished with value: -801.1074835906305 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 2, 'permaban_threshold': 1}. Best is trial 1 with value: -706.6358055663736.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-773.28, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-768.12, "episode_reward_trend_value": 0.5159878514496427, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-788.32, "episode_reward_trend_value": -0.7518583714280112, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-782.94, "episode_reward_trend_value": -0.32195119478804296, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-793.29, "episode_reward_trend_value": -0.5001170472325669, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-789.33, "episode_reward_trend_value": -0.321029039658431, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-790.73, "episode_reward_trend_value": -0.29078906505266106, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-791.16, "episode_reward_trend_value": -0.2553138850112159, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-799.51, "episode_reward_trend_value": -0.3278236696006104, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-797.80, "episode_reward_trend_value": -0.2724129864284325, "biggest_recent_change": 20.19704594305665 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-794.35, "episode_reward_trend_value": -0.2914364551186016, "biggest_recent_change": 20.19704594305665 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-795.29, "episode_reward_trend_value": -0.0774531450880027, "biggest_recent_change": 10.346146045661385 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-796.25, "episode_reward_trend_value": -0.1478917279815164, "biggest_recent_change": 10.346146045661385 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-797.95, "episode_reward_trend_value": -0.051767997917348416, "biggest_recent_change": 8.353921617263723 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-798.76, "episode_reward_trend_value": -0.10475944667804749, "biggest_recent_change": 8.353921617263723 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-800.13, "episode_reward_trend_value": -0.10444171922269005, "biggest_recent_change": 8.353921617263723 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-798.20, "episode_reward_trend_value": -0.07825203826739148, "biggest_recent_change": 8.353921617263723 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-796.68, "episode_reward_trend_value": 0.03141892978297594, "biggest_recent_change": 3.4477663323812067 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-797.83, "episode_reward_trend_value": -0.0002930238829599653, "biggest_recent_change": 3.4477663323812067 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-800.75, "episode_reward_trend_value": -0.07112738749838551, "biggest_recent_change": 2.9273263930070925 },
Hit early stopping because per_episode_reward: -800.7540905448487 < -800




Process Process-46:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:02:57,628][0m Trial 4 finished with value: -800.9003761264272 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 2, 'permaban_threshold': 1}. Best is trial 1 with value: -706.6358055663736.[0m




Process Process-58:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-53:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-813.64, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-814.18, "episode_reward_trend_value": -0.05419010739190071, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-816.67, "episode_reward_trend_value": -0.1516814310903271, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-825.32, "episode_reward_trend_value": -0.38950978558734733, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-815.86, "episode_reward_trend_value": -0.05567683887734347, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-809.98, "episode_reward_trend_value": 0.07312192730545349, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-813.12, "episode_reward_trend_value": 0.008578713923026272, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-807.51, "episode_reward_trend_value": 0.0875188136038251, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-813.05, "episode_reward_trend_value": 0.007302495360136163, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-805.43, "episode_reward_trend_value": 0.09116318701572809, "biggest_recent_change": 9.45822001252668 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-806.64, "episode_reward_trend_value": 0.08372390617318438, "biggest_recent_change": 9.45822001252668 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-804.88, "episode_reward_trend_value": 0.1310306894977114, "biggest_recent_change": 9.45822001252668 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-806.42, "episode_reward_trend_value": 0.21006930065468116, "biggest_recent_change": 9.45822001252668 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-804.99, "episode_reward_trend_value": 0.12082183702798628, "biggest_recent_change": 7.620487202604636 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-806.45, "episode_reward_trend_value": 0.039209657148411584, "biggest_recent_change": 7.620487202604636 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-804.34, "episode_reward_trend_value": 0.09761288892924413, "biggest_recent_change": 7.620487202604636 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-805.15, "episode_reward_trend_value": 0.02620767709461107, "biggest_recent_change": 7.620487202604636 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-805.63, "episode_reward_trend_value": 0.08244450209930518, "biggest_recent_change": 7.620487202604636 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-804.98, "episode_reward_trend_value": 0.005013499085101911, "biggest_recent_change": 2.1149173303838325 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-802.55, "episode_reward_trend_value": 0.045477879541929014, "biggest_recent_change": 2.430357891366498 },
Hit early stopping because per_episode_reward: -802.5511084851927 < -800




Process Process-57:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:03:18,857][0m Trial 5 finished with value: -806.8519014192768 and parameters: {'learning_rate': 1, 'variance_scaling_factor': 2, 'permaban_threshold': 2}. Best is trial 1 with value: -706.6358055663736.[0m




Process Process-68:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-70:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-65:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-61:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-807.68, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-811.49, "episode_reward_trend_value": -0.38027188655433974, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-812.57, "episode_reward_trend_value": -0.244541562889475, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-802.65, "episode_reward_trend_value": 0.1678003572726766, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-810.95, "episode_reward_trend_value": -0.08158429173707873, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-807.91, "episode_reward_trend_value": -0.004477148403941556, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-808.64, "episode_reward_trend_value": -0.01587712608128792, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-808.08, "episode_reward_trend_value": -0.005717895505308336, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-807.76, "episode_reward_trend_value": -0.001022431484183528, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-806.50, "episode_reward_trend_value": 0.01316838853343951, "biggest_recent_change": 9.924841975969798 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-804.33, "episode_reward_trend_value": 0.0794854074462377, "biggest_recent_change": 9.924841975969798 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-804.82, "episode_reward_trend_value": 0.08617237896568718, "biggest_recent_change": 9.924841975969798 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-800.79, "episode_reward_trend_value": 0.02070453056220079, "biggest_recent_change": 8.297382387663447 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-798.88, "episode_reward_trend_value": 0.13403326995475656, "biggest_recent_change": 4.032735619656023 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-803.52, "episode_reward_trend_value": 0.048729590989957064, "biggest_recent_change": 4.637816857545886 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-804.80, "episode_reward_trend_value": 0.042571644430204794, "biggest_recent_change": 4.637816857545886 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-805.26, "episode_reward_trend_value": 0.031356960934778674, "biggest_recent_change": 4.637816857545886 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-803.15, "episode_reward_trend_value": 0.05132597090229688, "biggest_recent_change": 4.637816857545886 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-803.11, "episode_reward_trend_value": 0.03764139132455916, "biggest_recent_change": 4.637816857545886 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-800.69, "episode_reward_trend_value": 0.040475390347905385, "biggest_recent_change": 4.637816857545886 },
Hit early stopping because per_episode_reward: -800.6894486133849 < -800




Process Process-66:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:03:39,849][0m Trial 6 finished with value: -801.065011167317 and parameters: {'learning_rate': 2, 'variance_scaling_factor': 3, 'permaban_threshold': 2}. Best is trial 1 with value: -706.6358055663736.[0m








Process Process-78:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-72:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-79:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-75:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-768.44, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-778.65, "episode_reward_trend_value": -1.0213342878144318, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-794.62, "episode_reward_trend_value": -1.3094206579841683, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-806.01, "episode_reward_trend_value": -1.2525554027858978, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-797.90, "episode_reward_trend_value": -0.736687128363468, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-798.65, "episode_reward_trend_value": -0.6042637741322483, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-793.29, "episode_reward_trend_value": -0.41423846657476515, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-797.47, "episode_reward_trend_value": -0.41480280524900603, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-800.33, "episode_reward_trend_value": -0.39861820460460534, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-803.45, "episode_reward_trend_value": -0.3890757043280132, "biggest_recent_change": 15.975070281539047 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-795.95, "episode_reward_trend_value": -0.19225285891785562, "biggest_recent_change": 15.975070281539047 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-794.15, "episode_reward_trend_value": 0.0052939890045409915, "biggest_recent_change": 11.38824892389357 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-794.18, "episode_reward_trend_value": 0.13149537428419963, "biggest_recent_change": 8.109176949038215 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-797.85, "episode_reward_trend_value": 0.0005409618586453589, "biggest_recent_change": 7.5007132087698665 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-795.18, "episode_reward_trend_value": 0.03858753954832157, "biggest_recent_change": 7.5007132087698665 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-796.10, "episode_reward_trend_value": -0.031195692017089032, "biggest_recent_change": 7.5007132087698665 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-795.13, "episode_reward_trend_value": 0.02597407479220869, "biggest_recent_change": 7.5007132087698665 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-796.45, "episode_reward_trend_value": 0.04300444949851554, "biggest_recent_change": 7.5007132087698665 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-797.91, "episode_reward_trend_value": 0.06154871885854997, "biggest_recent_change": 7.5007132087698665 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-798.59, "episode_reward_trend_value": -0.029338552382499626, "biggest_recent_change": 3.676720169261671 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-796.50, "episode_reward_trend_value": -0.026158233615811037, "biggest_recent_change": 3.676720169261671 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-797.22, "episode_reward_trend_value": -0.03376021080894386, "biggest_recent_change": 3.676720169261671 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-798.47, "episode_reward_trend_value": -0.006871174059085661, "biggest_recent_change": 2.6784884199971657 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-797.44, "episode_reward_trend_value": -0.025099606265452067, "biggest_recent_change": 2.0903747204786214 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-799.25, "episode_reward_trend_value": -0.03503381631159704, "biggest_recent_change": 2.0903747204786214 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-797.71, "episode_reward_trend_value": -0.028613240936498287, "biggest_recent_change": 2.0903747204786214 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-799.61, "episode_reward_trend_value": -0.03508840606466745, "biggest_recent_change": 2.0903747204786214 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-799.74, "episode_reward_trend_value": -0.02033791074812825, "biggest_recent_change": 2.0903747204786214 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-800.53, "episode_reward_trend_value": -0.02153562377436755, "biggest_recent_change": 2.0903747204786214 },
Hit early stopping because per_episode_reward: -800.5306137952668 < -800
[32m[I 2022-10-23 17:04:09,351][0m Trial 7 finished with value: -800.6319009859623 and parameters: {'learning_rate': 2, 'variance_scaling_factor': 0, 'permaban_threshold': 3}. Best is trial 1 with value: -706.6358055663736.[0m








Process Process-83:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-88:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-84:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-89:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-82:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-793.69, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-791.71, "episode_reward_trend_value": 0.19853563701055918, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-828.35, "episode_reward_trend_value": -1.732759895969582, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-811.84, "episode_reward_trend_value": -0.6047828656743264, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-811.53, "episode_reward_trend_value": -0.4459903504931617, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-797.65, "episode_reward_trend_value": -0.0791473166682431, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-801.99, "episode_reward_trend_value": -0.13827684091099096, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-805.92, "episode_reward_trend_value": -0.1746942869727864, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-801.80, "episode_reward_trend_value": -0.10137924178299898, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-801.46, "episode_reward_trend_value": -0.08630135674964473, "biggest_recent_change": 36.64055428949723 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-801.06, "episode_reward_trend_value": -0.10389416510088648, "biggest_recent_change": 36.64055428949723 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-801.31, "episode_reward_trend_value": 0.3004553105392069, "biggest_recent_change": 16.51171194916185 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-801.99, "episode_reward_trend_value": 0.10946358848215268, "biggest_recent_change": 13.882248186314314 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-805.67, "episode_reward_trend_value": 0.06509907595264572, "biggest_recent_change": 13.882248186314314 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-804.96, "episode_reward_trend_value": -0.0811700667664733, "biggest_recent_change": 4.339244621247303 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-802.38, "episode_reward_trend_value": -0.00434877789399353, "biggest_recent_change": 4.118260745455132 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-799.88, "episode_reward_trend_value": 0.06715723827114112, "biggest_recent_change": 4.118260745455132 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-798.01, "episode_reward_trend_value": 0.042101801815337896, "biggest_recent_change": 3.688934177152305 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-800.70, "episode_reward_trend_value": 0.00846223311865515, "biggest_recent_change": 3.688934177152305 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-803.09, "episode_reward_trend_value": -0.022561183275414957, "biggest_recent_change": 3.688934177152305 },
Hit early stopping because per_episode_reward: -803.0890150389394 < -800




Process Process-86:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:04:30,491][0m Trial 8 finished with value: -804.6406435104941 and parameters: {'learning_rate': 1, 'variance_scaling_factor': 0, 'permaban_threshold': 2}. Best is trial 1 with value: -706.6358055663736.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-794.36, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-798.79, "episode_reward_trend_value": -0.4423530955166939, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-817.39, "episode_reward_trend_value": -1.1515271188721556, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-818.80, "episode_reward_trend_value": -0.8143775764638272, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-809.54, "episode_reward_trend_value": -0.3792731168293358, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-808.62, "episode_reward_trend_value": -0.285041519316801, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-805.16, "episode_reward_trend_value": -0.17991653787638978, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-805.11, "episode_reward_trend_value": -0.15349536480928236, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-806.26, "episode_reward_trend_value": -0.14867388985114474, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-806.41, "episode_reward_trend_value": -0.13384517425347314, "biggest_recent_change": 18.607011422276173 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-801.39, "episode_reward_trend_value": -0.02892215845580747, "biggest_recent_change": 18.607011422276173 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-800.39, "episode_reward_trend_value": 0.18891998448331151, "biggest_recent_change": 9.260402620741388 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-796.74, "episode_reward_trend_value": 0.24508490846109857, "biggest_recent_change": 9.260402620741388 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-795.84, "episode_reward_trend_value": 0.15215107668845146, "biggest_recent_change": 5.019540466622971 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-798.18, "episode_reward_trend_value": 0.11592688605034658, "biggest_recent_change": 5.019540466622971 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-797.73, "episode_reward_trend_value": 0.08253909772959105, "biggest_recent_change": 5.019540466622971 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-798.65, "episode_reward_trend_value": 0.07178984766533682, "biggest_recent_change": 5.019540466622971 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-799.77, "episode_reward_trend_value": 0.07212966494497297, "biggest_recent_change": 5.019540466622971 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-797.84, "episode_reward_trend_value": 0.09526915396819757, "biggest_recent_change": 5.019540466622971 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-798.04, "episode_reward_trend_value": 0.03720758032497997, "biggest_recent_change": 3.6540582415291283 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-797.71, "episode_reward_trend_value": 0.029839922974971005, "biggest_recent_change": 3.6540582415291283 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-798.80, "episode_reward_trend_value": -0.022895904309970978, "biggest_recent_change": 2.341328450096057 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-798.58, "episode_reward_trend_value": -0.030467874643253862, "biggest_recent_change": 2.341328450096057 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-799.32, "episode_reward_trend_value": -0.01268385075620573, "biggest_recent_change": 1.9303995173692101 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-798.69, "episode_reward_trend_value": -0.01066269426515368, "biggest_recent_change": 1.9303995173692101 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-798.82, "episode_reward_trend_value": -0.0019052230168919476, "biggest_recent_change": 1.9303995173692101 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-797.90, "episode_reward_trend_value": 0.020708716754446264, "biggest_recent_change": 1.9303995173692101 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-798.29, "episode_reward_trend_value": -0.005086988835492119, "biggest_recent_change": 1.0921662141156503 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-797.66, "episode_reward_trend_value": 0.004239073763478195, "biggest_recent_change": 1.0921662141156503 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-798.83, "episode_reward_trend_value": -0.012540565936044207, "biggest_recent_change": 1.1744752922132875 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-798.61, "episode_reward_trend_value": 0.0021458246895475794, "biggest_recent_change": 1.1744752922132875 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-798.48, "episode_reward_trend_value": 0.0011784411288443962, "biggest_recent_change": 1.1744752922132875 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-799.70, "episode_reward_trend_value": -0.004134773568288589, "biggest_recent_change": 1.2189556230036942 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-798.04, "episode_reward_trend_value": 0.007220733965262601, "biggest_recent_change": 1.6560825066029565 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-799.10, "episode_reward_trend_value": -0.003169994763115685, "biggest_recent_change": 1.6560825066029565 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-799.86, "episode_reward_trend_value": -0.0217425067057977, "biggest_recent_change": 1.6560825066029565 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-799.89, "episode_reward_trend_value": -0.017734888217061477, "biggest_recent_change": 1.6560825066029565 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-798.08, "episode_reward_trend_value": -0.004667645345768455, "biggest_recent_change": 1.809396331057087 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-797.76, "episode_reward_trend_value": 0.01193799050320447, "biggest_recent_change": 1.809396331057087 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-799.43, "episode_reward_trend_value": -0.009198444076999598, "biggest_recent_change": 1.809396331057087 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-799.76, "episode_reward_trend_value": -0.014244986919077796, "biggest_recent_change": 1.809396331057087 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-801.22, "episode_reward_trend_value": -0.01695135164107013, "biggest_recent_change": 1.809396331057087 },
Hit early stopping because per_episode_reward: -801.2221301500603 < -800








Process Process-99:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-94:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-96:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-95:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-93:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-91:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:05:12,733][0m Trial 9 finished with value: -800.9320534887083 and parameters: {'learning_rate': 1, 'variance_scaling_factor': 0, 'permaban_threshold': 4}. Best is trial 1 with value: -706.6358055663736.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-775.53, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-797.26, "episode_reward_trend_value": -2.1726336767910768, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-810.46, "episode_reward_trend_value": -1.7463857577481008, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-807.64, "episode_reward_trend_value": -1.0704153401380343, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-798.04, "episode_reward_trend_value": -0.562709382700055, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-804.18, "episode_reward_trend_value": -0.5729466506123526, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-806.86, "episode_reward_trend_value": -0.5222320220416122, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-801.84, "episode_reward_trend_value": -0.3759076294668554, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-806.74, "episode_reward_trend_value": -0.3901626054659147, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-803.41, "episode_reward_trend_value": -0.30977314386427424, "biggest_recent_change": 21.726336767910766 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-803.17, "episode_reward_trend_value": -0.06574557332098342, "biggest_recent_change": 13.20137838705125 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-805.37, "episode_reward_trend_value": 0.056543041993192364, "biggest_recent_change": 9.60408489613883 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-801.07, "episode_reward_trend_value": 0.07300096660725457, "biggest_recent_change": 9.60408489613883 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-798.65, "episode_reward_trend_value": -0.006750280788903889, "biggest_recent_change": 6.138957222615431 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-800.77, "episode_reward_trend_value": 0.03787114920120555, "biggest_recent_change": 5.020387259816857 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-801.65, "episode_reward_trend_value": 0.05788703664984749, "biggest_recent_change": 5.020387259816857 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-798.12, "episode_reward_trend_value": 0.04132070544191417, "biggest_recent_change": 4.8994743745932965 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-794.13, "episode_reward_trend_value": 0.140158339150499, "biggest_recent_change": 4.2964681660865836 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-794.30, "episode_reward_trend_value": 0.10119781071248957, "biggest_recent_change": 4.2964681660865836 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-794.90, "episode_reward_trend_value": 0.09186701406337205, "biggest_recent_change": 4.2964681660865836 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-794.77, "episode_reward_trend_value": 0.11771179723630591, "biggest_recent_change": 4.2964681660865836 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-792.64, "episode_reward_trend_value": 0.09365521783402832, "biggest_recent_change": 3.9959126591793392 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-793.61, "episode_reward_trend_value": 0.05593059422051157, "biggest_recent_change": 3.9959126591793392 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-795.03, "episode_reward_trend_value": 0.06378008428870821, "biggest_recent_change": 3.9959126591793392 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-795.19, "episode_reward_trend_value": 0.07185934479184274, "biggest_recent_change": 3.9959126591793392 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-795.22, "episode_reward_trend_value": 0.03221537638214993, "biggest_recent_change": 3.9959126591793392 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-794.43, "episode_reward_trend_value": -0.003360882972540619, "biggest_recent_change": 2.1313760198816 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-793.97, "episode_reward_trend_value": 0.0036371653542295866, "biggest_recent_change": 2.1313760198816 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-794.53, "episode_reward_trend_value": 0.004207622855595774, "biggest_recent_change": 2.1313760198816 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-796.37, "episode_reward_trend_value": -0.017682419867675332, "biggest_recent_change": 2.1313760198816 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-797.04, "episode_reward_trend_value": -0.048819760976769225, "biggest_recent_change": 1.8394763683057818 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-797.23, "episode_reward_trend_value": -0.04024596686307784, "biggest_recent_change": 1.8394763683057818 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-797.81, "episode_reward_trend_value": -0.03091032904784849, "biggest_recent_change": 1.8394763683057818 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-799.79, "episode_reward_trend_value": -0.051143688151450484, "biggest_recent_change": 1.9790277955434021 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-798.43, "episode_reward_trend_value": -0.035642675045107805, "biggest_recent_change": 1.9790277955434021 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-796.38, "episode_reward_trend_value": -0.02161936146882605, "biggest_recent_change": 2.0561475391225486 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-796.30, "episode_reward_trend_value": -0.02585848510441085, "biggest_recent_change": 2.0561475391225486 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-796.23, "episode_reward_trend_value": -0.01893210861180579, "biggest_recent_change": 2.0561475391225486 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-796.50, "episode_reward_trend_value": -0.0014882390225756758, "biggest_recent_change": 2.0561475391225486 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-798.47, "episode_reward_trend_value": -0.015882556141717588, "biggest_recent_change": 2.0561475391225486 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-798.85, "episode_reward_trend_value": -0.018011023388171704, "biggest_recent_change": 2.0561475391225486 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-798.20, "episode_reward_trend_value": -0.004349514967318808, "biggest_recent_change": 2.0561475391225486 },
{"total_number_of_episodes":440, "number_of_timesteps":440000, "per_episode_reward":-797.26, "episode_reward_trend_value": 0.028107346807375207, "biggest_recent_change": 2.0561475391225486 },
{"total_number_of_episodes":450, "number_of_timesteps":450000, "per_episode_reward":-798.90, "episode_reward_trend_value": -0.0051987513105650075, "biggest_recent_change": 2.0561475391225486 },
{"total_number_of_episodes":460, "number_of_timesteps":460000, "per_episode_reward":-797.77, "episode_reward_trend_value": -0.0154659611340498, "biggest_recent_change": 1.9664732206596227 },
{"total_number_of_episodes":470, "number_of_timesteps":470000, "per_episode_reward":-798.80, "episode_reward_trend_value": -0.02782088125289748, "biggest_recent_change": 1.9664732206596227 },
{"total_number_of_episodes":480, "number_of_timesteps":480000, "per_episode_reward":-798.43, "episode_reward_trend_value": -0.024485258801250135, "biggest_recent_change": 1.9664732206596227 },
{"total_number_of_episodes":490, "number_of_timesteps":490000, "per_episode_reward":-798.75, "episode_reward_trend_value": -0.0250496803863699, "biggest_recent_change": 1.9664732206596227 },
{"total_number_of_episodes":500, "number_of_timesteps":500000, "per_episode_reward":-799.49, "episode_reward_trend_value": -0.01134184418716207, "biggest_recent_change": 1.6409973568132727 },
{"total_number_of_episodes":510, "number_of_timesteps":510000, "per_episode_reward":-797.81, "episode_reward_trend_value": 0.011602667999322119, "biggest_recent_change": 1.676342020102993 },
{"total_number_of_episodes":520, "number_of_timesteps":520000, "per_episode_reward":-798.22, "episode_reward_trend_value": -0.0002559442906961825, "biggest_recent_change": 1.676342020102993 },
{"total_number_of_episodes":530, "number_of_timesteps":530000, "per_episode_reward":-797.80, "episode_reward_trend_value": -0.00597102249345072, "biggest_recent_change": 1.676342020102993 },
{"total_number_of_episodes":540, "number_of_timesteps":540000, "per_episode_reward":-797.75, "episode_reward_trend_value": 0.012820241064682453, "biggest_recent_change": 1.676342020102993 },
{"total_number_of_episodes":550, "number_of_timesteps":550000, "per_episode_reward":-798.55, "episode_reward_trend_value": -0.008679846838283033, "biggest_recent_change": 1.676342020102993 },
{"total_number_of_episodes":560, "number_of_timesteps":560000, "per_episode_reward":-798.10, "episode_reward_trend_value": 0.007808251328901861, "biggest_recent_change": 1.676342020102993 },
{"total_number_of_episodes":570, "number_of_timesteps":570000, "per_episode_reward":-798.37, "episode_reward_trend_value": 0.0006765502195397251, "biggest_recent_change": 1.676342020102993 },
{"total_number_of_episodes":580, "number_of_timesteps":580000, "per_episode_reward":-797.75, "episode_reward_trend_value": 0.011178698574440205, "biggest_recent_change": 1.676342020102993 },
{"total_number_of_episodes":590, "number_of_timesteps":590000, "per_episode_reward":-796.61, "episode_reward_trend_value": 0.03193139985192677, "biggest_recent_change": 1.676342020102993 },
{"total_number_of_episodes":600, "number_of_timesteps":600000, "per_episode_reward":-796.10, "episode_reward_trend_value": 0.019005325337772976, "biggest_recent_change": 1.1349751522428733 },
{"total_number_of_episodes":610, "number_of_timesteps":610000, "per_episode_reward":-796.04, "episode_reward_trend_value": 0.02422026164014268, "biggest_recent_change": 1.1349751522428733 },
{"total_number_of_episodes":620, "number_of_timesteps":620000, "per_episode_reward":-796.43, "episode_reward_trend_value": 0.015149372939217504, "biggest_recent_change": 1.1349751522428733 },
{"total_number_of_episodes":630, "number_of_timesteps":630000, "per_episode_reward":-796.82, "episode_reward_trend_value": 0.010272032253159421, "biggest_recent_change": 1.1349751522428733 },
{"total_number_of_episodes":640, "number_of_timesteps":640000, "per_episode_reward":-796.54, "episode_reward_trend_value": 0.022272406434171898, "biggest_recent_change": 1.1349751522428733 },
{"total_number_of_episodes":650, "number_of_timesteps":650000, "per_episode_reward":-797.39, "episode_reward_trend_value": 0.007859478649092783, "biggest_recent_change": 1.1349751522428733 },
{"total_number_of_episodes":660, "number_of_timesteps":660000, "per_episode_reward":-798.96, "episode_reward_trend_value": -0.0065575714627458285, "biggest_recent_change": 1.5680936472375606 },
{"total_number_of_episodes":670, "number_of_timesteps":670000, "per_episode_reward":-797.66, "episode_reward_trend_value": 0.0009828862828814837, "biggest_recent_change": 1.5680936472375606 },
{"total_number_of_episodes":680, "number_of_timesteps":680000, "per_episode_reward":-799.24, "episode_reward_trend_value": -0.029238092155443002, "biggest_recent_change": 1.5849129072063306 },
{"total_number_of_episodes":690, "number_of_timesteps":690000, "per_episode_reward":-797.88, "episode_reward_trend_value": -0.01976880902131673, "biggest_recent_change": 1.5849129072063306 },
{"total_number_of_episodes":700, "number_of_timesteps":700000, "per_episode_reward":-797.67, "episode_reward_trend_value": -0.01802459455097581, "biggest_recent_change": 1.5849129072063306 },
{"total_number_of_episodes":710, "number_of_timesteps":710000, "per_episode_reward":-797.47, "episode_reward_trend_value": -0.011478919387949798, "biggest_recent_change": 1.5849129072063306 },
{"total_number_of_episodes":720, "number_of_timesteps":720000, "per_episode_reward":-798.64, "episode_reward_trend_value": -0.020213694541991522, "biggest_recent_change": 1.5849129072063306 },
{"total_number_of_episodes":730, "number_of_timesteps":730000, "per_episode_reward":-798.64, "episode_reward_trend_value": -0.02327151724811832, "biggest_recent_change": 1.5849129072063306 },
{"total_number_of_episodes":740, "number_of_timesteps":740000, "per_episode_reward":-798.97, "episode_reward_trend_value": -0.017504177708224232, "biggest_recent_change": 1.5849129072063306 },
{"total_number_of_episodes":750, "number_of_timesteps":750000, "per_episode_reward":-799.68, "episode_reward_trend_value": -0.007949822793369145, "biggest_recent_change": 1.5849129072063306 },
{"total_number_of_episodes":760, "number_of_timesteps":760000, "per_episode_reward":-799.88, "episode_reward_trend_value": -0.02462613721503683, "biggest_recent_change": 1.5849129072063306 },
{"total_number_of_episodes":770, "number_of_timesteps":770000, "per_episode_reward":-799.24, "episode_reward_trend_value": 3.696488925521003e-06, "biggest_recent_change": 1.3652307959005157 },
{"total_number_of_episodes":780, "number_of_timesteps":780000, "per_episode_reward":-800.15, "episode_reward_trend_value": -0.025189223954835898, "biggest_recent_change": 1.1748740621902698 },
Hit early stopping because per_episode_reward: -800.146012139713 < -800




Process Process-105:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-103:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-110:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-104:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-109:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-101:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:06:28,956][0m Trial 10 finished with value: -800.0587627631023 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 3, 'permaban_threshold': 0}. Best is trial 1 with value: -706.6358055663736.[0m








Process Process-112:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-114:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-782.54, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-780.00, "episode_reward_trend_value": 0.25367409783859785, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-797.37, "episode_reward_trend_value": -0.7419701010073538, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-807.26, "episode_reward_trend_value": -0.8240763904199866, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-802.62, "episode_reward_trend_value": -0.5021016217836148, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-809.52, "episode_reward_trend_value": -0.5396957089772491, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-812.94, "episode_reward_trend_value": -0.5067787063459472, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-811.40, "episode_reward_trend_value": -0.41236881112528023, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-809.74, "episode_reward_trend_value": -0.3400314574379223, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-810.53, "episode_reward_trend_value": -0.3110526353247047, "biggest_recent_change": 17.376142998533055 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-804.07, "episode_reward_trend_value": -0.26747437945113006, "biggest_recent_change": 17.376142998533055 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-805.49, "episode_reward_trend_value": -0.09021825518158519, "biggest_recent_change": 9.882889692452522 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-802.84, "episode_reward_trend_value": 0.049089444421397524, "biggest_recent_change": 6.900720577517859 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-795.27, "episode_reward_trend_value": 0.08165819930756647, "biggest_recent_change": 7.5694147810102095 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-802.42, "episode_reward_trend_value": 0.07887727295714311, "biggest_recent_change": 7.5694147810102095 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-806.88, "episode_reward_trend_value": 0.0673820582194796, "biggest_recent_change": 7.5694147810102095 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-808.09, "episode_reward_trend_value": 0.03680523688018916, "biggest_recent_change": 7.5694147810102095 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-805.38, "episode_reward_trend_value": 0.0483803084998221, "biggest_recent_change": 7.5694147810102095 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-805.29, "episode_reward_trend_value": 0.058176705826354515, "biggest_recent_change": 7.5694147810102095 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-804.37, "episode_reward_trend_value": -0.00329189838101532, "biggest_recent_change": 7.5694147810102095 },
Hit early stopping because per_episode_reward: -804.3677108103464 < -800




Process Process-118:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-113:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-111:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:06:50,281][0m Trial 11 finished with value: -803.2047898146832 and parameters: {'learning_rate': 2, 'variance_scaling_factor': 0, 'permaban_threshold': 0}. Best is trial 1 with value: -706.6358055663736.[0m




Process Process-129:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-122:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-128:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-795.55, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-805.06, "episode_reward_trend_value": -0.9508273309261085, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-819.78, "episode_reward_trend_value": -1.2114404082656562, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-804.76, "episode_reward_trend_value": -0.30695399002047075, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-795.94, "episode_reward_trend_value": -0.009762710772264427, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-798.50, "episode_reward_trend_value": -0.05893540583179174, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-799.71, "episode_reward_trend_value": -0.06929652653435028, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-806.79, "episode_reward_trend_value": -0.1605552059288763, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-803.85, "episode_reward_trend_value": -0.10373190430830022, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-802.25, "episode_reward_trend_value": -0.07442309061017972, "biggest_recent_change": 15.020188464699004 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-805.33, "episode_reward_trend_value": -0.0029983182467289123, "biggest_recent_change": 15.020188464699004 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-807.64, "episode_reward_trend_value": 0.13487576152279088, "biggest_recent_change": 15.020188464699004 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-803.62, "episode_reward_trend_value": 0.012640821606118558, "biggest_recent_change": 8.818111269723545 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-801.55, "episode_reward_trend_value": -0.062322323207705366, "biggest_recent_change": 7.081072822960323 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-801.78, "episode_reward_trend_value": -0.03647407506603031, "biggest_recent_change": 7.081072822960323 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-803.95, "episode_reward_trend_value": -0.047132866623911064, "biggest_recent_change": 7.081072822960323 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-801.88, "episode_reward_trend_value": 0.05456072603291836, "biggest_recent_change": 4.019043872198495 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-801.64, "episode_reward_trend_value": 0.02456161338200218, "biggest_recent_change": 4.019043872198495 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-802.25, "episode_reward_trend_value": 1.1557295048684562e-05, "biggest_recent_change": 4.019043872198495 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-801.56, "episode_reward_trend_value": 0.04184796907297318, "biggest_recent_change": 4.019043872198495 },
Hit early stopping because per_episode_reward: -801.5601240881144 < -800




Process Process-121:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:07:11,258][0m Trial 12 finished with value: -800.7782125073772 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 0, 'permaban_threshold': 0}. Best is trial 1 with value: -706.6358055663736.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-804.90, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-778.02, "episode_reward_trend_value": 2.6877307104989767, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-792.51, "episode_reward_trend_value": 0.6196511877994226, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-797.64, "episode_reward_trend_value": 0.24194170790126462, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-796.24, "episode_reward_trend_value": 0.21643088798687699, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-806.91, "episode_reward_trend_value": -0.040256593202175284, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-796.97, "episode_reward_trend_value": 0.13209189022618376, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-797.19, "episode_reward_trend_value": 0.11019139597709682, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-800.84, "episode_reward_trend_value": 0.05069105861858816, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-802.19, "episode_reward_trend_value": 0.030117840012285846, "biggest_recent_change": 26.877307104989768 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-804.99, "episode_reward_trend_value": -0.29967499277152, "biggest_recent_change": 14.484283349001316 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-802.89, "episode_reward_trend_value": -0.11540079423587787, "biggest_recent_change": 10.670065179583844 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-800.39, "episode_reward_trend_value": -0.030597636567409204, "biggest_recent_change": 10.670065179583844 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-795.35, "episode_reward_trend_value": 0.009909699569604729, "biggest_recent_change": 10.670065179583844 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-795.86, "episode_reward_trend_value": 0.122791536986996, "biggest_recent_change": 9.938343073679789 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-800.46, "episode_reward_trend_value": -0.038755934060831505, "biggest_recent_change": 5.044644534768395 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-801.81, "episode_reward_trend_value": -0.05137503255176247, "biggest_recent_change": 5.044644534768395 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-800.34, "episode_reward_trend_value": 0.00562391033758129, "biggest_recent_change": 5.044644534768395 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-800.77, "episode_reward_trend_value": 0.015708450404016125, "biggest_recent_change": 5.044644534768395 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-794.80, "episode_reward_trend_value": 0.11323829446675215, "biggest_recent_change": 5.973638120093483 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-795.92, "episode_reward_trend_value": 0.07748344805995153, "biggest_recent_change": 5.973638120093483 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-796.24, "episode_reward_trend_value": 0.04619507897148449, "biggest_recent_change": 5.973638120093483 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-795.29, "episode_reward_trend_value": 0.0006479109862198129, "biggest_recent_change": 5.973638120093483 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-796.98, "episode_reward_trend_value": -0.012442233368652625, "biggest_recent_change": 5.973638120093483 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-795.39, "episode_reward_trend_value": 0.05634339243453673, "biggest_recent_change": 5.973638120093483 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-793.50, "episode_reward_trend_value": 0.09230426148521802, "biggest_recent_change": 5.973638120093483 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-793.05, "episode_reward_trend_value": 0.08099237121098087, "biggest_recent_change": 5.973638120093483 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-792.50, "episode_reward_trend_value": 0.09189860702079815, "biggest_recent_change": 5.973638120093483 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-792.85, "episode_reward_trend_value": 0.021718718938265942, "biggest_recent_change": 1.888643655203282 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-793.13, "episode_reward_trend_value": 0.03094970707498861, "biggest_recent_change": 1.888643655203282 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-794.14, "episode_reward_trend_value": 0.023293479368852683, "biggest_recent_change": 1.888643655203282 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-793.27, "episode_reward_trend_value": 0.022470527162216865, "biggest_recent_change": 1.888643655203282 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-793.51, "episode_reward_trend_value": 0.03860053486849387, "biggest_recent_change": 1.888643655203282 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-792.67, "episode_reward_trend_value": 0.0301941566722601, "biggest_recent_change": 1.888643655203282 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-793.85, "episode_reward_trend_value": -0.003873818751350831, "biggest_recent_change": 1.1774741329217022 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-794.39, "episode_reward_trend_value": -0.014935649687448756, "biggest_recent_change": 1.1774741329217022 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-793.70, "episode_reward_trend_value": -0.013339927040495544, "biggest_recent_change": 1.1774741329217022 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-795.06, "episode_reward_trend_value": -0.02456529091982323, "biggest_recent_change": 1.3528345564739084 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-794.51, "episode_reward_trend_value": -0.015327289605886562, "biggest_recent_change": 1.3528345564739084 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-795.37, "episode_reward_trend_value": -0.01363060114361916, "biggest_recent_change": 1.3528345564739084 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-795.75, "episode_reward_trend_value": -0.027598720726487298, "biggest_recent_change": 1.3528345564739084 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-795.50, "episode_reward_trend_value": -0.022111210818758942, "biggest_recent_change": 1.3528345564739084 },
{"total_number_of_episodes":440, "number_of_timesteps":440000, "per_episode_reward":-795.44, "episode_reward_trend_value": -0.03070701138410009, "biggest_recent_change": 1.3528345564739084 },




Process Process-132:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-139:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":450, "number_of_timesteps":450000, "per_episode_reward":-796.25, "episode_reward_trend_value": -0.02661099963784434, "biggest_recent_change": 1.3528345564739084 },
{"total_number_of_episodes":460, "number_of_timesteps":460000, "per_episode_reward":-796.10, "episode_reward_trend_value": -0.018986822741440947, "biggest_recent_change": 1.3528345564739084 },
{"total_number_of_episodes":470, "number_of_timesteps":470000, "per_episode_reward":-795.25, "episode_reward_trend_value": -0.01722264898299449, "biggest_recent_change": 1.3528345564739084 },
{"total_number_of_episodes":480, "number_of_timesteps":480000, "per_episode_reward":-796.03, "episode_reward_trend_value": -0.010822819320825754, "biggest_recent_change": 0.8548000786985313 },
{"total_number_of_episodes":490, "number_of_timesteps":490000, "per_episode_reward":-796.09, "episode_reward_trend_value": -0.017497796620361643, "biggest_recent_change": 0.8548000786985313 },
{"total_number_of_episodes":500, "number_of_timesteps":500000, "per_episode_reward":-797.40, "episode_reward_trend_value": -0.022549362186695513, "biggest_recent_change": 1.3094409796685795 },
{"total_number_of_episodes":510, "number_of_timesteps":510000, "per_episode_reward":-798.35, "episode_reward_trend_value": -0.02880685203313078, "biggest_recent_change": 1.3094409796685795 },
{"total_number_of_episodes":520, "number_of_timesteps":520000, "per_episode_reward":-799.01, "episode_reward_trend_value": -0.03904432213855671, "biggest_recent_change": 1.3094409796685795 },
{"total_number_of_episodes":530, "number_of_timesteps":530000, "per_episode_reward":-800.01, "episode_reward_trend_value": -0.05086865838538111, "biggest_recent_change": 1.3094409796685795 },
Hit early stopping because per_episode_reward: -800.0148673393775 < -800




Process Process-131:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-135:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:08:03,456][0m Trial 13 finished with value: -799.9062133600586 and parameters: {'learning_rate': 1, 'variance_scaling_factor': 1, 'permaban_threshold': 0}. Best is trial 1 with value: -706.6358055663736.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-762.96, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-781.35, "episode_reward_trend_value": -1.8394074877672097, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-784.13, "episode_reward_trend_value": -1.0585035254811601, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-795.36, "episode_reward_trend_value": -1.0801218421155985, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-796.80, "episode_reward_trend_value": -0.8460443370256001, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-797.26, "episode_reward_trend_value": -0.6860505533581455, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-800.49, "episode_reward_trend_value": -0.6255014520663867, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-806.56, "episode_reward_trend_value": -0.6229146614021829, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-802.36, "episode_reward_trend_value": -0.4924883832118908, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-799.68, "episode_reward_trend_value": -0.40798392601615835, "biggest_recent_change": 18.394074877672097 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-796.93, "episode_reward_trend_value": -0.17308427102348484, "biggest_recent_change": 11.233584753844752 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-793.26, "episode_reward_trend_value": -0.10147906950988447, "biggest_recent_change": 11.233584753844752 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-794.64, "episode_reward_trend_value": 0.008019750907972896, "biggest_recent_change": 6.073939174169595 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-795.15, "episode_reward_trend_value": 0.01830741555058795, "biggest_recent_change": 6.073939174169595 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-797.69, "episode_reward_trend_value": -0.004718998619268354, "biggest_recent_change": 6.073939174169595 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-796.77, "episode_reward_trend_value": 0.04129331458709784, "biggest_recent_change": 6.073939174169595 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-797.78, "episode_reward_trend_value": 0.09760319825916212, "biggest_recent_change": 4.204955641201536 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-797.85, "episode_reward_trend_value": 0.05010314182202617, "biggest_recent_change": 3.668472504272927 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-797.40, "episode_reward_trend_value": 0.025348628586896717, "biggest_recent_change": 3.668472504272927 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-801.44, "episode_reward_trend_value": -0.050118682730827556, "biggest_recent_change": 4.045163946926664 },
Hit early stopping because per_episode_reward: -801.4416246089008 < -800




Process Process-142:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-145:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-143:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-141:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-146:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:08:24,505][0m Trial 14 finished with value: -799.3394030759772 and parameters: {'learning_rate': 1, 'variance_scaling_factor': 2, 'permaban_threshold': 4}. Best is trial 1 with value: -706.6358055663736.[0m




Process Process-153:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-769.46, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-789.10, "episode_reward_trend_value": -1.9640562051323514, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-797.87, "episode_reward_trend_value": -1.4204214342740955, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-805.41, "episode_reward_trend_value": -1.198331366703034, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-804.32, "episode_reward_trend_value": -0.8713154057491238, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-809.81, "episode_reward_trend_value": -0.8070221296460727, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-810.04, "episode_reward_trend_value": -0.67631578349052, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-804.72, "episode_reward_trend_value": -0.5036730542257244, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-803.66, "episode_reward_trend_value": -0.42741413404752393, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-804.86, "episode_reward_trend_value": -0.3932488680150098, "biggest_recent_change": 19.640562051323514 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-799.49, "episode_reward_trend_value": -0.11535918276914192, "biggest_recent_change": 8.767866634158395 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-801.02, "episode_reward_trend_value": -0.03495981569155144, "biggest_recent_change": 7.541512315609111 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-798.98, "episode_reward_trend_value": 0.07151206321100163, "biggest_recent_change": 5.498490252338684 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-799.06, "episode_reward_trend_value": 0.05843588360535755, "biggest_recent_change": 5.498490252338684 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-799.93, "episode_reward_trend_value": 0.10986191705505159, "biggest_recent_change": 5.3695096208045925 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-800.61, "episode_reward_trend_value": 0.10486080126534288, "biggest_recent_change": 5.3695096208045925 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-799.82, "episode_reward_trend_value": 0.05442286954109173, "biggest_recent_change": 5.3695096208045925 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-798.29, "episode_reward_trend_value": 0.05959414235356033, "biggest_recent_change": 5.3695096208045925 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-800.79, "episode_reward_trend_value": 0.04513903932436531, "biggest_recent_change": 5.3695096208045925 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-799.34, "episode_reward_trend_value": 0.0016490156579392432, "biggest_recent_change": 2.5002266701765166 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-799.39, "episode_reward_trend_value": 0.01807744026520418, "biggest_recent_change": 2.5002266701765166 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-799.61, "episode_reward_trend_value": -0.007023658266271266, "biggest_recent_change": 2.5002266701765166 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-798.74, "episode_reward_trend_value": 0.003473569508581578, "biggest_recent_change": 2.5002266701765166 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-800.03, "episode_reward_trend_value": -0.00118060081343098, "biggest_recent_change": 2.5002266701765166 },
Hit early stopping because per_episode_reward: -800.0335486767344 < -800




Process Process-156:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-160:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-159:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:08:49,278][0m Trial 15 finished with value: -800.6365231675583 and parameters: {'learning_rate': 1, 'variance_scaling_factor': 3, 'permaban_threshold': 0}. Best is trial 1 with value: -706.6358055663736.[0m




Process Process-163:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-166:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-168:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-788.60, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-790.23, "episode_reward_trend_value": -0.16306283636150737, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-805.99, "episode_reward_trend_value": -0.8695834104542826, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-810.99, "episode_reward_trend_value": -0.7464669513866208, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-811.52, "episode_reward_trend_value": -0.5729991235276145, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-805.51, "episode_reward_trend_value": -0.33816310333841554, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-807.78, "episode_reward_trend_value": -0.3197820056183142, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-797.64, "episode_reward_trend_value": -0.12919401974589195, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-800.63, "episode_reward_trend_value": -0.1504487208245749, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-800.94, "episode_reward_trend_value": -0.13711047924607683, "biggest_recent_change": 15.761039845470577 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-801.09, "episode_reward_trend_value": -0.12065849107164392, "biggest_recent_change": 15.761039845470577 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-802.20, "episode_reward_trend_value": 0.04215357524490931, "biggest_recent_change": 10.143338954886417 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-801.17, "episode_reward_trend_value": 0.10917055421720837, "biggest_recent_change": 10.143338954886417 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-802.77, "episode_reward_trend_value": 0.09720230055925563, "biggest_recent_change": 10.143338954886417 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-803.66, "episode_reward_trend_value": 0.02046817910887613, "biggest_recent_change": 10.143338954886417 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-803.88, "episode_reward_trend_value": 0.04343457872790471, "biggest_recent_change": 10.143338954886417 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-801.90, "episode_reward_trend_value": -0.047301321307759284, "biggest_recent_change": 2.992316283753553 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-806.58, "episode_reward_trend_value": -0.0660624466889039, "biggest_recent_change": 4.680817568056568 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-804.33, "episode_reward_trend_value": -0.03764383794246239, "biggest_recent_change": 4.680817568056568 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-803.80, "episode_reward_trend_value": -0.03017721276153477, "biggest_recent_change": 4.680817568056568 },
Hit early stopping because per_episode_reward: -803.8034189257362 < -800




Process Process-170:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-161:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:09:10,333][0m Trial 16 finished with value: -801.6039884902168 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 1, 'permaban_threshold': 3}. Best is trial 1 with value: -706.6358055663736.[0m




Process Process-176:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-172:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-174:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-180:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-175:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-171:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-769.65, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-794.36, "episode_reward_trend_value": -2.470753669291662, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-785.97, "episode_reward_trend_value": -0.8159383161689391, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-793.60, "episode_reward_trend_value": -0.7985519564406256, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-791.45, "episode_reward_trend_value": -0.5449571887448116, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-793.76, "episode_reward_trend_value": -0.48227556587455866, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-794.31, "episode_reward_trend_value": -0.4109754044550452, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-793.61, "episode_reward_trend_value": -0.3422745518865765, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-795.52, "episode_reward_trend_value": -0.32335574895255237, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-797.73, "episode_reward_trend_value": -0.31196482273067483, "biggest_recent_change": 24.70753669291662 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-797.72, "episode_reward_trend_value": -0.03739376569835081, "biggest_recent_change": 8.388770369537838 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-794.70, "episode_reward_trend_value": -0.09697770490122493, "biggest_recent_change": 7.637792369839985 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-795.26, "episode_reward_trend_value": -0.018406434390995704, "biggest_recent_change": 3.0262158412791678 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-794.28, "episode_reward_trend_value": -0.03151753399245081, "biggest_recent_change": 3.0262158412791678 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-796.13, "episode_reward_trend_value": -0.02635106487953433, "biggest_recent_change": 3.0262158412791678 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-793.21, "episode_reward_trend_value": 0.012223055058910227, "biggest_recent_change": 3.0262158412791678 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-792.65, "episode_reward_trend_value": 0.010678744375905404, "biggest_recent_change": 3.0262158412791678 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-793.58, "episode_reward_trend_value": 0.021517076640563775, "biggest_recent_change": 3.0262158412791678 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-794.44, "episode_reward_trend_value": 0.036553058334197884, "biggest_recent_change": 3.0262158412791678 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-794.72, "episode_reward_trend_value": 0.03332136278117635, "biggest_recent_change": 3.0262158412791678 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-795.36, "episode_reward_trend_value": -0.0074183264588466745, "biggest_recent_change": 2.926924820885233 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-795.06, "episode_reward_trend_value": 0.002286069120157208, "biggest_recent_change": 2.926924820885233 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-794.80, "episode_reward_trend_value": -0.005730446566653629, "biggest_recent_change": 2.926924820885233 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-796.60, "episode_reward_trend_value": -0.005216621947257257, "biggest_recent_change": 2.926924820885233 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-797.48, "episode_reward_trend_value": -0.04751299399813939, "biggest_recent_change": 1.8042643080273137 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-798.88, "episode_reward_trend_value": -0.06930129151958732, "biggest_recent_change": 1.8042643080273137 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-797.23, "episode_reward_trend_value": -0.040522916761272475, "biggest_recent_change": 1.8042643080273137 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-797.42, "episode_reward_trend_value": -0.033107805397873516, "biggest_recent_change": 1.8042643080273137 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-797.63, "episode_reward_trend_value": -0.0323267948102979, "biggest_recent_change": 1.8042643080273137 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-799.55, "episode_reward_trend_value": -0.04648295673568252, "biggest_recent_change": 1.9144107636075205 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-799.02, "episode_reward_trend_value": -0.044082439399840544, "biggest_recent_change": 1.9144107636075205 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-798.29, "episode_reward_trend_value": -0.03879573425065878, "biggest_recent_change": 1.9144107636075205 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-799.28, "episode_reward_trend_value": -0.0297934386414896, "biggest_recent_change": 1.9144107636075205 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-799.05, "episode_reward_trend_value": -0.017429482141488328, "biggest_recent_change": 1.9144107636075205 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-799.44, "episode_reward_trend_value": -0.006226375706523868, "biggest_recent_change": 1.9144107636075205 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-799.81, "episode_reward_trend_value": -0.028737524191459824, "biggest_recent_change": 1.9144107636075205 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-800.14, "episode_reward_trend_value": -0.03031323621396622, "biggest_recent_change": 1.9144107636075205 },
Hit early stopping because per_episode_reward: -800.1432494662815 < -800
[32m[I 2022-10-23 17:09:47,510][0m Trial 17 finished with value: -800.1035420128105 and parameters: {'learning_rate': 1, 'variance_scaling_factor': 0, 'permaban_threshold': 1}. Best is trial 1 with value: -706.6358055663736.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-772.84, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-768.12, "episode_reward_trend_value": 0.4720001912927046, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-792.98, "episode_reward_trend_value": -1.0070063309691135, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-794.14, "episode_reward_trend_value": -0.7101927063964808, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-797.79, "episode_reward_trend_value": -0.6238054380894624, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-795.79, "episode_reward_trend_value": -0.4591011609291513, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-794.84, "episode_reward_trend_value": -0.36670874528351194, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-797.61, "episode_reward_trend_value": -0.35393333375492864, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-796.26, "episode_reward_trend_value": -0.2927383586779143, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-796.39, "episode_reward_trend_value": -0.261635125764902, "biggest_recent_change": 24.860128532309318 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-793.34, "episode_reward_trend_value": -0.28021726832252575, "biggest_recent_change": 24.860128532309318 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-793.07, "episode_reward_trend_value": -0.0010122548032932678, "biggest_recent_change": 3.6464363316840718 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-794.07, "episode_reward_trend_value": 0.0008612306435502937, "biggest_recent_change": 3.6464363316840718 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-791.94, "episode_reward_trend_value": 0.06497579562574048, "biggest_recent_change": 3.0476090827409053 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-794.22, "episode_reward_trend_value": 0.017512972743008958, "biggest_recent_change": 3.0476090827409053 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-793.58, "episode_reward_trend_value": 0.014014225382312714, "biggest_recent_change": 3.0476090827409053 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-792.12, "episode_reward_trend_value": 0.06108466538608531, "biggest_recent_change": 3.0476090827409053 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-790.89, "episode_reward_trend_value": 0.059583038248162114, "biggest_recent_change": 3.0476090827409053 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-793.09, "episode_reward_trend_value": 0.03660328155739813, "biggest_recent_change": 3.0476090827409053 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-792.54, "episode_reward_trend_value": 0.008810594369778199, "biggest_recent_change": 2.2744945823249054 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-793.58, "episode_reward_trend_value": -0.005624882790214593, "biggest_recent_change": 2.2744945823249054 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-792.95, "episode_reward_trend_value": 0.012351672903626978, "biggest_recent_change": 2.2744945823249054 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-795.92, "episode_reward_trend_value": -0.04413753065933229, "biggest_recent_change": 2.9601538039532898 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-797.26, "episode_reward_trend_value": -0.03375692321953794, "biggest_recent_change": 2.9601538039532898 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-795.75, "episode_reward_trend_value": -0.02414792956189255, "biggest_recent_change": 2.9601538039532898 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-797.82, "episode_reward_trend_value": -0.06340868250583753, "biggest_recent_change": 2.9601538039532898 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-796.23, "episode_reward_trend_value": -0.05928743185532741, "biggest_recent_change": 2.9601538039532898 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-797.22, "episode_reward_trend_value": -0.045822803133979864, "biggest_recent_change": 2.9601538039532898 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-796.88, "episode_reward_trend_value": -0.048216370717983, "biggest_recent_change": 2.9601538039532898 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-797.30, "episode_reward_trend_value": -0.04138605184199251, "biggest_recent_change": 2.9601538039532898 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-796.81, "episode_reward_trend_value": -0.042855670117479705, "biggest_recent_change": 2.9601538039532898 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-795.23, "episode_reward_trend_value": 0.0076255197695180256, "biggest_recent_change": 2.0699368104498035 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-796.18, "episode_reward_trend_value": 0.01197685662581307, "biggest_recent_change": 2.0699368104498035 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-796.87, "episode_reward_trend_value": -0.012454029935754028, "biggest_recent_change": 2.0699368104498035 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-796.69, "episode_reward_trend_value": 0.012641415013414795, "biggest_recent_change": 1.5920307847446793 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-797.19, "episode_reward_trend_value": -0.010697383286709636, "biggest_recent_change": 1.5831532858765058 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-796.22, "episode_reward_trend_value": 0.01103199983646669, "biggest_recent_change": 1.5831532858765058 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-795.16, "episode_reward_trend_value": 0.019141538338135432, "biggest_recent_change": 1.5831532858765058 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-796.64, "episode_reward_trend_value": 0.007374730266140735, "biggest_recent_change": 1.5831532858765058 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-797.31, "episode_reward_trend_value": -0.005582513940814554, "biggest_recent_change": 1.5831532858765058 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-796.70, "episode_reward_trend_value": -0.016341143060145037, "biggest_recent_change": 1.4751542876181247 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-796.31, "episode_reward_trend_value": -0.0014963182525118807, "biggest_recent_change": 1.4751542876181247 },
{"total_number_of_episodes":440, "number_of_timesteps":440000, "per_episode_reward":-795.92, "episode_reward_trend_value": 0.01062210402719125, "biggest_recent_change": 1.4751542876181247 },
{"total_number_of_episodes":450, "number_of_timesteps":450000, "per_episode_reward":-795.59, "episode_reward_trend_value": 0.012123344962086929, "biggest_recent_change": 1.4751542876181247 },
{"total_number_of_episodes":460, "number_of_timesteps":460000, "per_episode_reward":-794.74, "episode_reward_trend_value": 0.027302758319155553, "biggest_recent_change": 1.4751542876181247 },
{"total_number_of_episodes":470, "number_of_timesteps":470000, "per_episode_reward":-795.77, "episode_reward_trend_value": 0.005029823853038175, "biggest_recent_change": 1.4751542876181247 },
{"total_number_of_episodes":480, "number_of_timesteps":480000, "per_episode_reward":-795.46, "episode_reward_trend_value": -0.003337721467890889, "biggest_recent_change": 1.4751542876181247 },
{"total_number_of_episodes":490, "number_of_timesteps":490000, "per_episode_reward":-795.08, "episode_reward_trend_value": 0.01728474810418245, "biggest_recent_change": 1.0333737627202026 },
{"total_number_of_episodes":500, "number_of_timesteps":500000, "per_episode_reward":-795.54, "episode_reward_trend_value": 0.01973237397799671, "biggest_recent_change": 1.0333737627202026 },
{"total_number_of_episodes":510, "number_of_timesteps":510000, "per_episode_reward":-794.98, "episode_reward_trend_value": 0.019118811159552884, "biggest_recent_change": 1.0333737627202026 },
{"total_number_of_episodes":520, "number_of_timesteps":520000, "per_episode_reward":-795.29, "episode_reward_trend_value": 0.011350192546449813, "biggest_recent_change": 1.0333737627202026 },
{"total_number_of_episodes":530, "number_of_timesteps":530000, "per_episode_reward":-794.96, "episode_reward_trend_value": 0.010644632612940717, "biggest_recent_change": 1.0333737627202026 },
{"total_number_of_episodes":540, "number_of_timesteps":540000, "per_episode_reward":-794.86, "episode_reward_trend_value": 0.008177249770288756, "biggest_recent_change": 1.0333737627202026 },
{"total_number_of_episodes":550, "number_of_timesteps":550000, "per_episode_reward":-794.17, "episode_reward_trend_value": 0.00627111580634442, "biggest_recent_change": 1.0333737627202026 },
{"total_number_of_episodes":560, "number_of_timesteps":560000, "per_episode_reward":-794.42, "episode_reward_trend_value": 0.014949581339751855, "biggest_recent_change": 0.6861340831146663 },
{"total_number_of_episodes":570, "number_of_timesteps":570000, "per_episode_reward":-794.36, "episode_reward_trend_value": 0.012241199641792062, "biggest_recent_change": 0.6861340831146663 },
{"total_number_of_episodes":580, "number_of_timesteps":580000, "per_episode_reward":-795.10, "episode_reward_trend_value": -0.00022081341073013996, "biggest_recent_change": 0.7407132008585222 },
{"total_number_of_episodes":590, "number_of_timesteps":590000, "per_episode_reward":-795.11, "episode_reward_trend_value": 0.004787396168188076, "biggest_recent_change": 0.7407132008585222 },
{"total_number_of_episodes":600, "number_of_timesteps":600000, "per_episode_reward":-796.19, "episode_reward_trend_value": -0.013410133197410485, "biggest_recent_change": 1.0781216314270523 },
{"total_number_of_episodes":610, "number_of_timesteps":610000, "per_episode_reward":-796.64, "episode_reward_trend_value": -0.01502280466078471, "biggest_recent_change": 1.0781216314270523 },
{"total_number_of_episodes":620, "number_of_timesteps":620000, "per_episode_reward":-796.18, "episode_reward_trend_value": -0.01354923288276066, "biggest_recent_change": 1.0781216314270523 },
{"total_number_of_episodes":630, "number_of_timesteps":630000, "per_episode_reward":-796.39, "episode_reward_trend_value": -0.017042769533852857, "biggest_recent_change": 1.0781216314270523 },
{"total_number_of_episodes":640, "number_of_timesteps":640000, "per_episode_reward":-795.95, "episode_reward_trend_value": -0.019804469701519643, "biggest_recent_change": 1.0781216314270523 },
{"total_number_of_episodes":650, "number_of_timesteps":650000, "per_episode_reward":-796.84, "episode_reward_trend_value": -0.026893318112413706, "biggest_recent_change": 1.0781216314270523 },
{"total_number_of_episodes":660, "number_of_timesteps":660000, "per_episode_reward":-796.54, "episode_reward_trend_value": -0.02416955315809067, "biggest_recent_change": 1.0781216314270523 },
{"total_number_of_episodes":670, "number_of_timesteps":670000, "per_episode_reward":-796.29, "episode_reward_trend_value": -0.013241798796850113, "biggest_recent_change": 1.0781216314270523 },
{"total_number_of_episodes":680, "number_of_timesteps":680000, "per_episode_reward":-795.71, "episode_reward_trend_value": -0.006741218175709997, "biggest_recent_change": 1.0781216314270523 },
{"total_number_of_episodes":690, "number_of_timesteps":690000, "per_episode_reward":-795.95, "episode_reward_trend_value": 0.002633802189502098, "biggest_recent_change": 0.8903082216939993 },
{"total_number_of_episodes":700, "number_of_timesteps":700000, "per_episode_reward":-795.30, "episode_reward_trend_value": 0.014894019868780913, "biggest_recent_change": 0.8903082216939993 },
{"total_number_of_episodes":710, "number_of_timesteps":710000, "per_episode_reward":-795.09, "episode_reward_trend_value": 0.012072264879992013, "biggest_recent_change": 0.8903082216939993 },
{"total_number_of_episodes":720, "number_of_timesteps":720000, "per_episode_reward":-794.77, "episode_reward_trend_value": 0.018044046178099255, "biggest_recent_change": 0.8903082216939993 },
{"total_number_of_episodes":730, "number_of_timesteps":730000, "per_episode_reward":-794.78, "episode_reward_trend_value": 0.013055361559304401, "biggest_recent_change": 0.8903082216939993 },
{"total_number_of_episodes":740, "number_of_timesteps":740000, "per_episode_reward":-793.26, "episode_reward_trend_value": 0.03987827992792113, "biggest_recent_change": 1.5237544314815068 },
{"total_number_of_episodes":750, "number_of_timesteps":750000, "per_episode_reward":-793.11, "episode_reward_trend_value": 0.03802140025615144, "biggest_recent_change": 1.5237544314815068 },
{"total_number_of_episodes":760, "number_of_timesteps":760000, "per_episode_reward":-792.55, "episode_reward_trend_value": 0.04160362351987917, "biggest_recent_change": 1.5237544314815068 },
{"total_number_of_episodes":770, "number_of_timesteps":770000, "per_episode_reward":-793.21, "episode_reward_trend_value": 0.027799619726369608, "biggest_recent_change": 1.5237544314815068 },
{"total_number_of_episodes":780, "number_of_timesteps":780000, "per_episode_reward":-793.26, "episode_reward_trend_value": 0.029849752420262932, "biggest_recent_change": 1.5237544314815068 },
{"total_number_of_episodes":790, "number_of_timesteps":790000, "per_episode_reward":-793.66, "episode_reward_trend_value": 0.018216035877113276, "biggest_recent_change": 1.5237544314815068 },
{"total_number_of_episodes":800, "number_of_timesteps":800000, "per_episode_reward":-793.39, "episode_reward_trend_value": 0.018967391418050282, "biggest_recent_change": 1.5237544314815068 },
{"total_number_of_episodes":810, "number_of_timesteps":810000, "per_episode_reward":-794.48, "episode_reward_trend_value": 0.00319076717986238, "biggest_recent_change": 1.5237544314815068 },
{"total_number_of_episodes":820, "number_of_timesteps":820000, "per_episode_reward":-793.86, "episode_reward_trend_value": 0.010172462971461805, "biggest_recent_change": 1.5237544314815068 },
{"total_number_of_episodes":830, "number_of_timesteps":830000, "per_episode_reward":-793.79, "episode_reward_trend_value": -0.005983797363670521, "biggest_recent_change": 1.0951536999282325 },
{"total_number_of_episodes":840, "number_of_timesteps":840000, "per_episode_reward":-793.78, "episode_reward_trend_value": -0.007378015769866882, "biggest_recent_change": 1.0951536999282325 },
{"total_number_of_episodes":850, "number_of_timesteps":850000, "per_episode_reward":-793.71, "episode_reward_trend_value": -0.012909782953203022, "biggest_recent_change": 1.0951536999282325 },
{"total_number_of_episodes":860, "number_of_timesteps":860000, "per_episode_reward":-794.48, "episode_reward_trend_value": -0.014139000728747912, "biggest_recent_change": 1.0951536999282325 },
{"total_number_of_episodes":870, "number_of_timesteps":870000, "per_episode_reward":-794.37, "episode_reward_trend_value": -0.012262569583864054, "biggest_recent_change": 1.0951536999282325 },
{"total_number_of_episodes":880, "number_of_timesteps":880000, "per_episode_reward":-794.16, "episode_reward_trend_value": -0.005474372010904618, "biggest_recent_change": 1.0951536999282325 },
{"total_number_of_episodes":890, "number_of_timesteps":890000, "per_episode_reward":-794.20, "episode_reward_trend_value": -0.00909436898868105, "biggest_recent_change": 1.0951536999282325 },
{"total_number_of_episodes":900, "number_of_timesteps":900000, "per_episode_reward":-793.84, "episode_reward_trend_value": 0.007133830173069731, "biggest_recent_change": 0.7744809878366823 },
{"total_number_of_episodes":910, "number_of_timesteps":910000, "per_episode_reward":-793.24, "episode_reward_trend_value": 0.006918155461327539, "biggest_recent_change": 0.7744809878366823 },
{"total_number_of_episodes":920, "number_of_timesteps":920000, "per_episode_reward":-793.22, "episode_reward_trend_value": 0.006411633094489844, "biggest_recent_change": 0.7744809878366823 },
{"total_number_of_episodes":930, "number_of_timesteps":930000, "per_episode_reward":-793.12, "episode_reward_trend_value": 0.007284341550535424, "biggest_recent_change": 0.7744809878366823 },
{"total_number_of_episodes":940, "number_of_timesteps":940000, "per_episode_reward":-793.19, "episode_reward_trend_value": 0.005752791401386679, "biggest_recent_change": 0.7744809878366823 },
{"total_number_of_episodes":950, "number_of_timesteps":950000, "per_episode_reward":-793.08, "episode_reward_trend_value": 0.015647539000772464, "biggest_recent_change": 0.5975413495202702 },
{"total_number_of_episodes":960, "number_of_timesteps":960000, "per_episode_reward":-792.43, "episode_reward_trend_value": 0.021477991867235837, "biggest_recent_change": 0.6437617049136861 },
{"total_number_of_episodes":970, "number_of_timesteps":970000, "per_episode_reward":-792.80, "episode_reward_trend_value": 0.015030514733364775, "biggest_recent_change": 0.6437617049136861 },
{"total_number_of_episodes":980, "number_of_timesteps":980000, "per_episode_reward":-793.02, "episode_reward_trend_value": 0.013106018000576114, "biggest_recent_change": 0.6437617049136861 },
{"total_number_of_episodes":990, "number_of_timesteps":990000, "per_episode_reward":-792.55, "episode_reward_trend_value": 0.0143460127759757, "biggest_recent_change": 0.6437617049136861 },
{"total_number_of_episodes":1000, "number_of_timesteps":1000000, "per_episode_reward":-792.11, "episode_reward_trend_value": 0.01253113442678821, "biggest_recent_change": 0.6437617049136861 },
{"total_number_of_episodes":1010, "number_of_timesteps":1010000, "per_episode_reward":-792.17, "episode_reward_trend_value": 0.011613381926748743, "biggest_recent_change": 0.6437617049136861 },
{"total_number_of_episodes":1020, "number_of_timesteps":1020000, "per_episode_reward":-792.09, "episode_reward_trend_value": 0.011465226333252252, "biggest_recent_change": 0.6437617049136861 },
{"total_number_of_episodes":1030, "number_of_timesteps":1030000, "per_episode_reward":-791.75, "episode_reward_trend_value": 0.016064228246770857, "biggest_recent_change": 0.6437617049136861 },
{"total_number_of_episodes":1040, "number_of_timesteps":1040000, "per_episode_reward":-791.70, "episode_reward_trend_value": 0.01532427276965791, "biggest_recent_change": 0.6437617049136861 },
{"total_number_of_episodes":1050, "number_of_timesteps":1050000, "per_episode_reward":-791.19, "episode_reward_trend_value": 0.01376727034842386, "biggest_recent_change": 0.5036314870026217 },
{"total_number_of_episodes":1060, "number_of_timesteps":1060000, "per_episode_reward":-791.41, "episode_reward_trend_value": 0.01552270981573732, "biggest_recent_change": 0.5036314870026217 },
{"total_number_of_episodes":1070, "number_of_timesteps":1070000, "per_episode_reward":-791.58, "episode_reward_trend_value": 0.016002700882812505, "biggest_recent_change": 0.5036314870026217 },
{"total_number_of_episodes":1080, "number_of_timesteps":1080000, "per_episode_reward":-791.87, "episode_reward_trend_value": 0.007485810844886804, "biggest_recent_change": 0.5036314870026217 },
{"total_number_of_episodes":1090, "number_of_timesteps":1090000, "per_episode_reward":-791.21, "episode_reward_trend_value": 0.01001104747156584, "biggest_recent_change": 0.6614735944945096 },
{"total_number_of_episodes":1100, "number_of_timesteps":1100000, "per_episode_reward":-714.25, "episode_reward_trend_value": 0.8658037819235347, "biggest_recent_change": 76.96285236397785 },
{"total_number_of_episodes":1110, "number_of_timesteps":1110000, "per_episode_reward":-714.34, "episode_reward_trend_value": 0.8639280652925259, "biggest_recent_change": 76.96285236397785 },
{"total_number_of_episodes":1120, "number_of_timesteps":1120000, "per_episode_reward":-714.36, "episode_reward_trend_value": 0.8598812900811545, "biggest_recent_change": 76.96285236397785 },
{"total_number_of_episodes":1130, "number_of_timesteps":1130000, "per_episode_reward":-714.31, "episode_reward_trend_value": 0.859893782752575, "biggest_recent_change": 76.96285236397785 },
{"total_number_of_episodes":1140, "number_of_timesteps":1140000, "per_episode_reward":-714.21, "episode_reward_trend_value": 0.8554291413472292, "biggest_recent_change": 76.96285236397785 },
{"total_number_of_episodes":1150, "number_of_timesteps":1150000, "per_episode_reward":-714.38, "episode_reward_trend_value": 0.8558557487768755, "biggest_recent_change": 76.96285236397785 },
{"total_number_of_episodes":1160, "number_of_timesteps":1160000, "per_episode_reward":-714.33, "episode_reward_trend_value": 0.8583515313310436, "biggest_recent_change": 76.96285236397785 },
{"total_number_of_episodes":1170, "number_of_timesteps":1170000, "per_episode_reward":-714.43, "episode_reward_trend_value": 0.8605343194777764, "biggest_recent_change": 76.96285236397785 },
{"total_number_of_episodes":1180, "number_of_timesteps":1180000, "per_episode_reward":-713.61, "episode_reward_trend_value": 0.8622468693404762, "biggest_recent_change": 76.96285236397785 },
{"total_number_of_episodes":1190, "number_of_timesteps":1190000, "per_episode_reward":-713.90, "episode_reward_trend_value": 0.0038335399202082954, "biggest_recent_change": 0.8156030821374998 },
{"total_number_of_episodes":1200, "number_of_timesteps":1200000, "per_episode_reward":-713.88, "episode_reward_trend_value": 0.005098918953590075, "biggest_recent_change": 0.8156030821374998 },
{"total_number_of_episodes":1210, "number_of_timesteps":1210000, "per_episode_reward":-714.54, "episode_reward_trend_value": -0.0019758465136154274, "biggest_recent_change": 0.8156030821374998 },
{"total_number_of_episodes":1220, "number_of_timesteps":1220000, "per_episode_reward":-714.58, "episode_reward_trend_value": -0.0030692060480532746, "biggest_recent_change": 0.8156030821374998 },
{"total_number_of_episodes":1230, "number_of_timesteps":1230000, "per_episode_reward":-714.65, "episode_reward_trend_value": -0.004926834462494551, "biggest_recent_change": 0.8156030821374998 },
{"total_number_of_episodes":1240, "number_of_timesteps":1240000, "per_episode_reward":-714.51, "episode_reward_trend_value": -0.001435427873834922, "biggest_recent_change": 0.8156030821374998 },
{"total_number_of_episodes":1250, "number_of_timesteps":1250000, "per_episode_reward":-714.68, "episode_reward_trend_value": -0.003888045967149386, "biggest_recent_change": 0.8156030821374998 },
{"total_number_of_episodes":1260, "number_of_timesteps":1260000, "per_episode_reward":-714.71, "episode_reward_trend_value": -0.0031500073459937994, "biggest_recent_change": 0.8156030821374998 },
{"total_number_of_episodes":1270, "number_of_timesteps":1270000, "per_episode_reward":-714.28, "episode_reward_trend_value": -0.0074451168153359506, "biggest_recent_change": 0.657542263390269 },
{"total_number_of_episodes":1280, "number_of_timesteps":1280000, "per_episode_reward":-714.21, "episode_reward_trend_value": -0.0033656494640088874, "biggest_recent_change": 0.657542263390269 },
{"total_number_of_episodes":1290, "number_of_timesteps":1290000, "per_episode_reward":-714.05, "episode_reward_trend_value": -0.0019110786550982084, "biggest_recent_change": 0.657542263390269 },
{"total_number_of_episodes":1300, "number_of_timesteps":1300000, "per_episode_reward":-714.07, "episode_reward_trend_value": 0.005215191687394357, "biggest_recent_change": 0.4290432298967062 },
{"total_number_of_episodes":1310, "number_of_timesteps":1310000, "per_episode_reward":-714.16, "episode_reward_trend_value": 0.0046503060909369905, "biggest_recent_change": 0.4290432298967062 },
{"total_number_of_episodes":1320, "number_of_timesteps":1320000, "per_episode_reward":-714.13, "episode_reward_trend_value": 0.005743777665221236, "biggest_recent_change": 0.4290432298967062 },
{"total_number_of_episodes":1330, "number_of_timesteps":1330000, "per_episode_reward":-714.09, "episode_reward_trend_value": 0.004599426261222561, "biggest_recent_change": 0.4290432298967062 },
{"total_number_of_episodes":1340, "number_of_timesteps":1340000, "per_episode_reward":-714.46, "episode_reward_trend_value": 0.0024620659141659316, "biggest_recent_change": 0.4290432298967062 },
{"total_number_of_episodes":1350, "number_of_timesteps":1350000, "per_episode_reward":-714.73, "episode_reward_trend_value": -0.0002161127339326817, "biggest_recent_change": 0.4290432298967062 },
{"total_number_of_episodes":1360, "number_of_timesteps":1360000, "per_episode_reward":-714.88, "episode_reward_trend_value": -0.006641843318864933, "biggest_recent_change": 0.3671640411681665 },
{"total_number_of_episodes":1370, "number_of_timesteps":1370000, "per_episode_reward":-714.96, "episode_reward_trend_value": -0.008389861964020737, "biggest_recent_change": 0.3671640411681665 },
{"total_number_of_episodes":1380, "number_of_timesteps":1380000, "per_episode_reward":-715.05, "episode_reward_trend_value": -0.011137865670562203, "biggest_recent_change": 0.3671640411681665 },
{"total_number_of_episodes":1390, "number_of_timesteps":1390000, "per_episode_reward":-715.15, "episode_reward_trend_value": -0.01202614201370251, "biggest_recent_change": 0.3671640411681665 },
{"total_number_of_episodes":1400, "number_of_timesteps":1400000, "per_episode_reward":-715.03, "episode_reward_trend_value": -0.009626275986172711, "biggest_recent_change": 0.3671640411681665 },
{"total_number_of_episodes":1410, "number_of_timesteps":1410000, "per_episode_reward":-714.89, "episode_reward_trend_value": -0.008427770615999936, "biggest_recent_change": 0.3671640411681665 },
{"total_number_of_episodes":1420, "number_of_timesteps":1420000, "per_episode_reward":-714.80, "episode_reward_trend_value": -0.00789343377589628, "biggest_recent_change": 0.3671640411681665 },
{"total_number_of_episodes":1430, "number_of_timesteps":1430000, "per_episode_reward":-714.88, "episode_reward_trend_value": -0.004638916979500765, "biggest_recent_change": 0.2676980182169473 },
{"total_number_of_episodes":1440, "number_of_timesteps":1440000, "per_episode_reward":-715.11, "episode_reward_trend_value": -0.004271587887948651, "biggest_recent_change": 0.234638399977257 },
{"total_number_of_episodes":1450, "number_of_timesteps":1450000, "per_episode_reward":-714.90, "episode_reward_trend_value": -0.0001995142527170578, "biggest_recent_change": 0.234638399977257 },
{"total_number_of_episodes":1460, "number_of_timesteps":1460000, "per_episode_reward":-715.43, "episode_reward_trend_value": -0.005208457864014488, "biggest_recent_change": 0.5353218253076193 },
{"total_number_of_episodes":1470, "number_of_timesteps":1470000, "per_episode_reward":-715.83, "episode_reward_trend_value": -0.008613229609621461, "biggest_recent_change": 0.5353218253076193 },
{"total_number_of_episodes":1480, "number_of_timesteps":1480000, "per_episode_reward":-715.27, "episode_reward_trend_value": -0.0013717455875128812, "biggest_recent_change": 0.5556107585412065 },
{"total_number_of_episodes":1490, "number_of_timesteps":1490000, "per_episode_reward":-715.46, "episode_reward_trend_value": -0.004785677964976761, "biggest_recent_change": 0.5556107585412065 },
{"total_number_of_episodes":1500, "number_of_timesteps":1500000, "per_episode_reward":-715.44, "episode_reward_trend_value": -0.0060862405065032965, "biggest_recent_change": 0.5556107585412065 },
{"total_number_of_episodes":1510, "number_of_timesteps":1510000, "per_episode_reward":-715.80, "episode_reward_trend_value": -0.011092966215037197, "biggest_recent_change": 0.5556107585412065 },
{"total_number_of_episodes":1520, "number_of_timesteps":1520000, "per_episode_reward":-716.16, "episode_reward_trend_value": -0.014294695205603603, "biggest_recent_change": 0.5556107585412065 },
{"total_number_of_episodes":1530, "number_of_timesteps":1530000, "per_episode_reward":-716.21, "episode_reward_trend_value": -0.012165073220206876, "biggest_recent_change": 0.5556107585412065 },
{"total_number_of_episodes":1540, "number_of_timesteps":1540000, "per_episode_reward":-716.18, "episode_reward_trend_value": -0.014291756336298376, "biggest_recent_change": 0.5556107585412065 },
{"total_number_of_episodes":1550, "number_of_timesteps":1550000, "per_episode_reward":-715.88, "episode_reward_trend_value": -0.005010114609002711, "biggest_recent_change": 0.5556107585412065 },
{"total_number_of_episodes":1560, "number_of_timesteps":1560000, "per_episode_reward":-715.81, "episode_reward_trend_value": 0.00019724204224126472, "biggest_recent_change": 0.5556107585412065 },
{"total_number_of_episodes":1570, "number_of_timesteps":1570000, "per_episode_reward":-716.13, "episode_reward_trend_value": -0.009502417883853316, "biggest_recent_change": 0.36474733891611777 },
{"total_number_of_episodes":1580, "number_of_timesteps":1580000, "per_episode_reward":-716.37, "episode_reward_trend_value": -0.010076705876525063, "biggest_recent_change": 0.36474733891611777 },
{"total_number_of_episodes":1590, "number_of_timesteps":1590000, "per_episode_reward":-717.04, "episode_reward_trend_value": -0.017832180512995264, "biggest_recent_change": 0.6741382177967807 },
{"total_number_of_episodes":1600, "number_of_timesteps":1600000, "per_episode_reward":-716.84, "episode_reward_trend_value": -0.011537694256705639, "biggest_recent_change": 0.6741382177967807 },
{"total_number_of_episodes":1610, "number_of_timesteps":1610000, "per_episode_reward":-716.22, "episode_reward_trend_value": -0.0006307276517898369, "biggest_recent_change": 0.6741382177967807 },
{"total_number_of_episodes":1620, "number_of_timesteps":1620000, "per_episode_reward":-716.42, "episode_reward_trend_value": -0.0023344181499245073, "biggest_recent_change": 0.6741382177967807 },
{"total_number_of_episodes":1630, "number_of_timesteps":1630000, "per_episode_reward":-716.66, "episode_reward_trend_value": -0.0052791507535908345, "biggest_recent_change": 0.6741382177967807 },
{"total_number_of_episodes":1640, "number_of_timesteps":1640000, "per_episode_reward":-717.04, "episode_reward_trend_value": -0.01286914765893496, "biggest_recent_change": 0.6741382177967807 },
{"total_number_of_episodes":1650, "number_of_timesteps":1650000, "per_episode_reward":-717.02, "episode_reward_trend_value": -0.013493687770250696, "biggest_recent_change": 0.6741382177967807 },
{"total_number_of_episodes":1660, "number_of_timesteps":1660000, "per_episode_reward":-717.29, "episode_reward_trend_value": -0.012933427354544669, "biggest_recent_change": 0.6741382177967807 },
{"total_number_of_episodes":1670, "number_of_timesteps":1670000, "per_episode_reward":-717.16, "episode_reward_trend_value": -0.008830140527201517, "biggest_recent_change": 0.6741382177967807 },
{"total_number_of_episodes":1680, "number_of_timesteps":1680000, "per_episode_reward":-716.97, "episode_reward_trend_value": 0.0008168540127687467, "biggest_recent_change": 0.6192138557988756 },
{"total_number_of_episodes":1690, "number_of_timesteps":1690000, "per_episode_reward":-716.87, "episode_reward_trend_value": -0.00029300851071083244, "biggest_recent_change": 0.6192138557988756 },
{"total_number_of_episodes":1700, "number_of_timesteps":1700000, "per_episode_reward":-716.83, "episode_reward_trend_value": -0.006792358821321108, "biggest_recent_change": 0.38307379133198083 },
{"total_number_of_episodes":1710, "number_of_timesteps":1710000, "per_episode_reward":-716.96, "episode_reward_trend_value": -0.005978752861455582, "biggest_recent_change": 0.38307379133198083 },
{"total_number_of_episodes":1720, "number_of_timesteps":1720000, "per_episode_reward":-716.50, "episode_reward_trend_value": 0.0017609870885571075, "biggest_recent_change": 0.45736328514658453 },
{"total_number_of_episodes":1730, "number_of_timesteps":1730000, "per_episode_reward":-716.47, "episode_reward_trend_value": 0.0063215710352576724, "biggest_recent_change": 0.45736328514658453 },
{"total_number_of_episodes":1740, "number_of_timesteps":1740000, "per_episode_reward":-716.60, "episode_reward_trend_value": 0.004682638106861406, "biggest_recent_change": 0.45736328514658453 },
{"total_number_of_episodes":1750, "number_of_timesteps":1750000, "per_episode_reward":-716.59, "episode_reward_trend_value": 0.007781852493014362, "biggest_recent_change": 0.45736328514658453 },
{"total_number_of_episodes":1760, "number_of_timesteps":1760000, "per_episode_reward":-716.56, "episode_reward_trend_value": 0.006739925323224977, "biggest_recent_change": 0.45736328514658453 },
{"total_number_of_episodes":1770, "number_of_timesteps":1770000, "per_episode_reward":-716.86, "episode_reward_trend_value": 0.0012202834938357732, "biggest_recent_change": 0.45736328514658453 },
{"total_number_of_episodes":1780, "number_of_timesteps":1780000, "per_episode_reward":-716.85, "episode_reward_trend_value": 0.00022454716276393405, "biggest_recent_change": 0.45736328514658453 },
{"total_number_of_episodes":1790, "number_of_timesteps":1790000, "per_episode_reward":-716.66, "episode_reward_trend_value": 0.0018710168927327687, "biggest_recent_change": 0.45736328514658453 },
{"total_number_of_episodes":1800, "number_of_timesteps":1800000, "per_episode_reward":-716.94, "episode_reward_trend_value": 0.00018614089535604864, "biggest_recent_change": 0.45736328514658453 },
{"total_number_of_episodes":1810, "number_of_timesteps":1810000, "per_episode_reward":-716.80, "episode_reward_trend_value": -0.0033780396162607453, "biggest_recent_change": 0.30267647384448537 },
{"total_number_of_episodes":1820, "number_of_timesteps":1820000, "per_episode_reward":-716.88, "episode_reward_trend_value": -0.004565164339721327, "biggest_recent_change": 0.30267647384448537 },
{"total_number_of_episodes":1830, "number_of_timesteps":1830000, "per_episode_reward":-716.87, "episode_reward_trend_value": -0.0030065041425435387, "biggest_recent_change": 0.30267647384448537 },
{"total_number_of_episodes":1840, "number_of_timesteps":1840000, "per_episode_reward":-717.27, "episode_reward_trend_value": -0.007515906556538237, "biggest_recent_change": 0.3938521198995204 },
{"total_number_of_episodes":1850, "number_of_timesteps":1850000, "per_episode_reward":-717.49, "episode_reward_trend_value": -0.010408327262880195, "biggest_recent_change": 0.3938521198995204 },
{"total_number_of_episodes":1860, "number_of_timesteps":1860000, "per_episode_reward":-717.34, "episode_reward_trend_value": -0.0053731357147424, "biggest_recent_change": 0.3938521198995204 },
{"total_number_of_episodes":1870, "number_of_timesteps":1870000, "per_episode_reward":-717.34, "episode_reward_trend_value": -0.005457821114792852, "biggest_recent_change": 0.3938521198995204 },
{"total_number_of_episodes":1880, "number_of_timesteps":1880000, "per_episode_reward":-717.32, "episode_reward_trend_value": -0.007310773531235605, "biggest_recent_change": 0.3938521198995204 },
{"total_number_of_episodes":1890, "number_of_timesteps":1890000, "per_episode_reward":-717.35, "episode_reward_trend_value": -0.004558885422750336, "biggest_recent_change": 0.3938521198995204 },
{"total_number_of_episodes":1900, "number_of_timesteps":1900000, "per_episode_reward":-717.61, "episode_reward_trend_value": -0.009022478886576361, "biggest_recent_change": 0.3938521198995204 },
{"total_number_of_episodes":1910, "number_of_timesteps":1910000, "per_episode_reward":-717.76, "episode_reward_trend_value": -0.00974833162325442, "biggest_recent_change": 0.3938521198995204 },
{"total_number_of_episodes":1920, "number_of_timesteps":1920000, "per_episode_reward":-717.75, "episode_reward_trend_value": -0.00972081273559474, "biggest_recent_change": 0.3938521198995204 },
{"total_number_of_episodes":1930, "number_of_timesteps":1930000, "per_episode_reward":-717.61, "episode_reward_trend_value": -0.003831504364244312, "biggest_recent_change": 0.26513637264326917 },
{"total_number_of_episodes":1940, "number_of_timesteps":1940000, "per_episode_reward":-717.55, "episode_reward_trend_value": -0.0006526311911506531, "biggest_recent_change": 0.26513637264326917 },
{"total_number_of_episodes":1950, "number_of_timesteps":1950000, "per_episode_reward":-717.63, "episode_reward_trend_value": -0.003217508786508461, "biggest_recent_change": 0.26513637264326917 },
{"total_number_of_episodes":1960, "number_of_timesteps":1960000, "per_episode_reward":-717.62, "episode_reward_trend_value": -0.0030997209842464853, "biggest_recent_change": 0.26513637264326917 },
{"total_number_of_episodes":1970, "number_of_timesteps":1970000, "per_episode_reward":-717.56, "episode_reward_trend_value": -0.002608753954571815, "biggest_recent_change": 0.26513637264326917 },
{"total_number_of_episodes":1980, "number_of_timesteps":1980000, "per_episode_reward":-717.65, "episode_reward_trend_value": -0.003363889471898397, "biggest_recent_change": 0.26513637264326917 },
{"total_number_of_episodes":1990, "number_of_timesteps":1990000, "per_episode_reward":-717.66, "episode_reward_trend_value": -0.0005378349410875671, "biggest_recent_change": 0.14478920754140745 },
{"total_number_of_episodes":2000, "number_of_timesteps":2000000, "per_episode_reward":-717.81, "episode_reward_trend_value": -0.0005539126867208526, "biggest_recent_change": 0.14623620464840315 },
{"total_number_of_episodes":2010, "number_of_timesteps":2010000, "per_episode_reward":-717.83, "episode_reward_trend_value": -0.0009005809011316361, "biggest_recent_change": 0.14623620464840315 },
{"total_number_of_episodes":2020, "number_of_timesteps":2020000, "per_episode_reward":-717.70, "episode_reward_trend_value": -0.0009755258765431386, "biggest_recent_change": 0.14623620464840315 },
{"total_number_of_episodes":2030, "number_of_timesteps":2030000, "per_episode_reward":-717.73, "episode_reward_trend_value": -0.0019265165298242611, "biggest_recent_change": 0.14623620464840315 },
{"total_number_of_episodes":2040, "number_of_timesteps":2040000, "per_episode_reward":-717.74, "episode_reward_trend_value": -0.001197064908540647, "biggest_recent_change": 0.14623620464840315 },
{"total_number_of_episodes":2050, "number_of_timesteps":2050000, "per_episode_reward":-717.91, "episode_reward_trend_value": -0.003215117711686667, "biggest_recent_change": 0.16639300884378372 },
{"total_number_of_episodes":2060, "number_of_timesteps":2060000, "per_episode_reward":-717.74, "episode_reward_trend_value": -0.002080053218071498, "biggest_recent_change": 0.16639300884378372 },
{"total_number_of_episodes":2070, "number_of_timesteps":2070000, "per_episode_reward":-717.66, "episode_reward_trend_value": -0.00010093704682326967, "biggest_recent_change": 0.16639300884378372 },
{"total_number_of_episodes":2080, "number_of_timesteps":2080000, "per_episode_reward":-717.71, "episode_reward_trend_value": -0.0005156384780826152, "biggest_recent_change": 0.16639300884378372 },
{"total_number_of_episodes":2090, "number_of_timesteps":2090000, "per_episode_reward":-717.76, "episode_reward_trend_value": 0.0006038440124989898, "biggest_recent_change": 0.16639300884378372 },
{"total_number_of_episodes":2100, "number_of_timesteps":2100000, "per_episode_reward":-717.66, "episode_reward_trend_value": 0.0018734834053096467, "biggest_recent_change": 0.16639300884378372 },








Process Process-190:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-186:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":2110, "number_of_timesteps":2110000, "per_episode_reward":-717.76, "episode_reward_trend_value": -0.0006518046306913069, "biggest_recent_change": 0.16639300884378372 },
{"total_number_of_episodes":2120, "number_of_timesteps":2120000, "per_episode_reward":-717.86, "episode_reward_trend_value": -0.0014515416958336877, "biggest_recent_change": 0.16639300884378372 },
{"total_number_of_episodes":2130, "number_of_timesteps":2130000, "per_episode_reward":-717.91, "episode_reward_trend_value": -0.0018654454975565687, "biggest_recent_change": 0.16639300884378372 },
{"total_number_of_episodes":2140, "number_of_timesteps":2140000, "per_episode_reward":-717.98, "episode_reward_trend_value": -0.000779535756167762, "biggest_recent_change": 0.16203172315738357 },
Hit early stopping because biggest_recent_change: 0.09788171248999333 < 0.1
{"total_number_of_episodes":2150, "number_of_timesteps":2150000, "per_episode_reward":-718.03, "episode_reward_trend_value": -0.0031383651784330545, "biggest_recent_change": 0.09788171248999333 },
[32m[I 2022-10-23 17:13:16,575][0m Trial 18 finished with value: -718.2747787702062 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 1, 'permaban_threshold': 0}. Best is trial 1 with value: -706.6358055663736.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-775.07, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-781.92, "episode_reward_trend_value": -0.6842202486457836, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-799.22, "episode_reward_trend_value": -1.2073861530505894, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-803.88, "episode_reward_trend_value": -0.9603837013956991, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-798.17, "episode_reward_trend_value": -0.5774758913892015, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-797.20, "episode_reward_trend_value": -0.4424608392324922, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-798.81, "episode_reward_trend_value": -0.39553989691508684, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-803.10, "episode_reward_trend_value": -0.4003740875769056, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-801.06, "episode_reward_trend_value": -0.32486809745083034, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-803.84, "episode_reward_trend_value": -0.3196594528426052, "biggest_recent_change": 17.305520574553952 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-799.55, "episode_reward_trend_value": -0.19598354903530563, "biggest_recent_change": 17.305520574553952 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-796.99, "episode_reward_trend_value": 0.024795399286646595, "biggest_recent_change": 5.712475386302913 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-798.52, "episode_reward_trend_value": 0.05963988313621813, "biggest_recent_change": 5.712475386302913 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-794.86, "episode_reward_trend_value": 0.03677425611284156, "biggest_recent_change": 4.293792315478186 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-798.08, "episode_reward_trend_value": -0.009839637910919111, "biggest_recent_change": 4.293792315478186 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-801.55, "episode_reward_trend_value": -0.030463117141312068, "biggest_recent_change": 4.293792315478186 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-799.88, "episode_reward_trend_value": 0.03571865185487013, "biggest_recent_change": 4.288628856199125 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-797.03, "episode_reward_trend_value": 0.044806205006960274, "biggest_recent_change": 4.288628856199125 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-795.43, "episode_reward_trend_value": 0.09351775428089013, "biggest_recent_change": 4.288628856199125 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-797.90, "episode_reward_trend_value": 0.018389046314726126, "biggest_recent_change": 3.6545689541990214 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-796.53, "episode_reward_trend_value": 0.005118847961623234, "biggest_recent_change": 3.6545689541990214 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-795.05, "episode_reward_trend_value": 0.0384794295246997, "biggest_recent_change": 3.6545689541990214 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-796.43, "episode_reward_trend_value": -0.01736983369398912, "biggest_recent_change": 3.4654649840159664 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-795.68, "episode_reward_trend_value": 0.02668818319831543, "biggest_recent_change": 3.4654649840159664 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-793.08, "episode_reward_trend_value": 0.09407846693683647, "biggest_recent_change": 2.854618118005078 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-793.10, "episode_reward_trend_value": 0.07541273512009639, "biggest_recent_change": 2.854618118005078 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-792.42, "episode_reward_trend_value": 0.05119262003131174, "biggest_recent_change": 2.5996605524509278 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-791.83, "episode_reward_trend_value": 0.039957192763223605, "biggest_recent_change": 2.5996605524509278 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-791.77, "episode_reward_trend_value": 0.06806467741431182, "biggest_recent_change": 2.5996605524509278 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-791.94, "episode_reward_trend_value": 0.05096920571874206, "biggest_recent_change": 2.5996605524509278 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-792.10, "episode_reward_trend_value": 0.032807131516383634, "biggest_recent_change": 2.5996605524509278 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-792.17, "episode_reward_trend_value": 0.047231452070514025, "biggest_recent_change": 2.5996605524509278 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-792.38, "episode_reward_trend_value": 0.0366419066623343, "biggest_recent_change": 2.5996605524509278 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-792.09, "episode_reward_trend_value": 0.010951631375907152, "biggest_recent_change": 0.6748077600144597 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-792.23, "episode_reward_trend_value": 0.009623143649625692, "biggest_recent_change": 0.6748077600144597 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-791.46, "episode_reward_trend_value": 0.010667455411607484, "biggest_recent_change": 0.768795818592821 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-791.12, "episode_reward_trend_value": 0.007863967502318145, "biggest_recent_change": 0.768795818592821 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-792.29, "episode_reward_trend_value": -0.005781397866908517, "biggest_recent_change": 1.171364125388095 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-792.73, "episode_reward_trend_value": -0.008762264863198903, "biggest_recent_change": 1.171364125388095 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-793.54, "episode_reward_trend_value": -0.015960216799888054, "biggest_recent_change": 1.171364125388095 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-793.13, "episode_reward_trend_value": -0.010608215575568163, "biggest_recent_change": 1.171364125388095 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-792.84, "episode_reward_trend_value": -0.0050884556897573124, "biggest_recent_change": 1.171364125388095 },
{"total_number_of_episodes":440, "number_of_timesteps":440000, "per_episode_reward":-791.03, "episode_reward_trend_value": 0.011856762717997703, "biggest_recent_change": 1.812605433370436 },
{"total_number_of_episodes":450, "number_of_timesteps":450000, "per_episode_reward":-789.99, "episode_reward_trend_value": 0.024906257800153576, "biggest_recent_change": 1.812605433370436 },
{"total_number_of_episodes":460, "number_of_timesteps":460000, "per_episode_reward":-790.48, "episode_reward_trend_value": 0.010936934298384434, "biggest_recent_change": 1.812605433370436 },
{"total_number_of_episodes":470, "number_of_timesteps":470000, "per_episode_reward":-790.91, "episode_reward_trend_value": 0.0023328378115302765, "biggest_recent_change": 1.812605433370436 },
{"total_number_of_episodes":480, "number_of_timesteps":480000, "per_episode_reward":-792.26, "episode_reward_trend_value": 0.0003906592871519226, "biggest_recent_change": 1.812605433370436 },
{"total_number_of_episodes":490, "number_of_timesteps":490000, "per_episode_reward":-791.54, "episode_reward_trend_value": 0.013164031817511083, "biggest_recent_change": 1.812605433370436 },
{"total_number_of_episodes":500, "number_of_timesteps":500000, "per_episode_reward":-791.08, "episode_reward_trend_value": 0.027262639612227857, "biggest_recent_change": 1.812605433370436 },
{"total_number_of_episodes":510, "number_of_timesteps":510000, "per_episode_reward":-791.48, "episode_reward_trend_value": 0.018365168921995214, "biggest_recent_change": 1.812605433370436 },
{"total_number_of_episodes":520, "number_of_timesteps":520000, "per_episode_reward":-791.82, "episode_reward_trend_value": 0.011300519161831188, "biggest_recent_change": 1.812605433370436 },
{"total_number_of_episodes":530, "number_of_timesteps":530000, "per_episode_reward":-790.84, "episode_reward_trend_value": 0.0020416266823644947, "biggest_recent_change": 1.346160192582147 },
{"total_number_of_episodes":540, "number_of_timesteps":540000, "per_episode_reward":-791.77, "episode_reward_trend_value": -0.019786959576847746, "biggest_recent_change": 1.346160192582147 },
{"total_number_of_episodes":550, "number_of_timesteps":550000, "per_episode_reward":-792.79, "episode_reward_trend_value": -0.02563978671467516, "biggest_recent_change": 1.346160192582147 },
{"total_number_of_episodes":560, "number_of_timesteps":560000, "per_episode_reward":-792.75, "episode_reward_trend_value": -0.02037750104399139, "biggest_recent_change": 1.346160192582147 },
{"total_number_of_episodes":570, "number_of_timesteps":570000, "per_episode_reward":-792.54, "episode_reward_trend_value": -0.0031297223225604082, "biggest_recent_change": 1.015197738970869 },
{"total_number_of_episodes":580, "number_of_timesteps":580000, "per_episode_reward":-793.03, "episode_reward_trend_value": -0.01650829354619974, "biggest_recent_change": 1.015197738970869 },
{"total_number_of_episodes":590, "number_of_timesteps":590000, "per_episode_reward":-793.44, "episode_reward_trend_value": -0.02620522854164417, "biggest_recent_change": 1.015197738970869 },
{"total_number_of_episodes":600, "number_of_timesteps":600000, "per_episode_reward":-793.08, "episode_reward_trend_value": -0.017777288521536625, "biggest_recent_change": 1.015197738970869 },
{"total_number_of_episodes":610, "number_of_timesteps":610000, "per_episode_reward":-792.52, "episode_reward_trend_value": -0.007739394298008999, "biggest_recent_change": 1.015197738970869 },
{"total_number_of_episodes":620, "number_of_timesteps":620000, "per_episode_reward":-792.90, "episode_reward_trend_value": -0.02283071704923461, "biggest_recent_change": 1.015197738970869 },
{"total_number_of_episodes":630, "number_of_timesteps":630000, "per_episode_reward":-793.82, "episode_reward_trend_value": -0.022755889430892594, "biggest_recent_change": 1.015197738970869 },
{"total_number_of_episodes":640, "number_of_timesteps":640000, "per_episode_reward":-792.57, "episode_reward_trend_value": 0.002424267229270703, "biggest_recent_change": 1.2510163604438276 },
{"total_number_of_episodes":650, "number_of_timesteps":650000, "per_episode_reward":-792.73, "episode_reward_trend_value": 0.00015983327113089116, "biggest_recent_change": 1.2510163604438276 },
{"total_number_of_episodes":660, "number_of_timesteps":660000, "per_episode_reward":-793.14, "episode_reward_trend_value": -0.006705358712328133, "biggest_recent_change": 1.2510163604438276 },
{"total_number_of_episodes":670, "number_of_timesteps":670000, "per_episode_reward":-793.41, "episode_reward_trend_value": -0.004254957920693414, "biggest_recent_change": 1.2510163604438276 },
{"total_number_of_episodes":680, "number_of_timesteps":680000, "per_episode_reward":-793.70, "episode_reward_trend_value": -0.002907715903533396, "biggest_recent_change": 1.2510163604438276 },
{"total_number_of_episodes":690, "number_of_timesteps":690000, "per_episode_reward":-793.75, "episode_reward_trend_value": -0.007511352263041874, "biggest_recent_change": 1.2510163604438276 },
{"total_number_of_episodes":700, "number_of_timesteps":700000, "per_episode_reward":-792.95, "episode_reward_trend_value": -0.004780253459376935, "biggest_recent_change": 1.2510163604438276 },
{"total_number_of_episodes":710, "number_of_timesteps":710000, "per_episode_reward":-792.64, "episode_reward_trend_value": 0.002813264673987861, "biggest_recent_change": 1.2510163604438276 },
{"total_number_of_episodes":720, "number_of_timesteps":720000, "per_episode_reward":-793.41, "episode_reward_trend_value": 0.004495372555887419, "biggest_recent_change": 1.2510163604438276 },
{"total_number_of_episodes":730, "number_of_timesteps":730000, "per_episode_reward":-793.80, "episode_reward_trend_value": -0.013730294252894104, "biggest_recent_change": 0.8030749491317692 },
{"total_number_of_episodes":740, "number_of_timesteps":740000, "per_episode_reward":-793.79, "episode_reward_trend_value": -0.011710171634317451, "biggest_recent_change": 0.8030749491317692 },
{"total_number_of_episodes":750, "number_of_timesteps":750000, "per_episode_reward":-793.23, "episode_reward_trend_value": -0.0009713836357605032, "biggest_recent_change": 0.8030749491317692 },
{"total_number_of_episodes":760, "number_of_timesteps":760000, "per_episode_reward":-793.50, "episode_reward_trend_value": -0.000976874174350289, "biggest_recent_change": 0.8030749491317692 },
{"total_number_of_episodes":770, "number_of_timesteps":770000, "per_episode_reward":-793.93, "episode_reward_trend_value": -0.0024746918993388237, "biggest_recent_change": 0.8030749491317692 },
{"total_number_of_episodes":780, "number_of_timesteps":780000, "per_episode_reward":-793.49, "episode_reward_trend_value": 0.0028672342047886762, "biggest_recent_change": 0.8030749491317692 },
{"total_number_of_episodes":790, "number_of_timesteps":790000, "per_episode_reward":-793.25, "episode_reward_trend_value": -0.0033062796077451316, "biggest_recent_change": 0.7689068756070583 },
{"total_number_of_episodes":800, "number_of_timesteps":800000, "per_episode_reward":-793.79, "episode_reward_trend_value": -0.01271001329930641, "biggest_recent_change": 0.7689068756070583 },
{"total_number_of_episodes":810, "number_of_timesteps":810000, "per_episode_reward":-793.99, "episode_reward_trend_value": -0.006428996973484875, "biggest_recent_change": 0.5547635337054544 },
{"total_number_of_episodes":820, "number_of_timesteps":820000, "per_episode_reward":-793.69, "episode_reward_trend_value": 0.001254997960530899, "biggest_recent_change": 0.5547635337054544 },
{"total_number_of_episodes":830, "number_of_timesteps":830000, "per_episode_reward":-793.40, "episode_reward_trend_value": 0.004232996506884016, "biggest_recent_change": 0.5547635337054544 },
{"total_number_of_episodes":840, "number_of_timesteps":840000, "per_episode_reward":-793.68, "episode_reward_trend_value": -0.0049491081036207, "biggest_recent_change": 0.541833337629555 },
{"total_number_of_episodes":850, "number_of_timesteps":850000, "per_episode_reward":-793.75, "episode_reward_trend_value": -0.002769188165735967, "biggest_recent_change": 0.541833337629555 },
{"total_number_of_episodes":860, "number_of_timesteps":860000, "per_episode_reward":-794.17, "episode_reward_trend_value": -0.002704276162497384, "biggest_recent_change": 0.541833337629555 },
{"total_number_of_episodes":870, "number_of_timesteps":870000, "per_episode_reward":-793.97, "episode_reward_trend_value": -0.00524930898216351, "biggest_recent_change": 0.541833337629555 },
{"total_number_of_episodes":880, "number_of_timesteps":880000, "per_episode_reward":-794.00, "episode_reward_trend_value": -0.008352732448287901, "biggest_recent_change": 0.541833337629555 },
{"total_number_of_episodes":890, "number_of_timesteps":890000, "per_episode_reward":-794.01, "episode_reward_trend_value": -0.002411368323237184, "biggest_recent_change": 0.4192936277137278 },
{"total_number_of_episodes":900, "number_of_timesteps":900000, "per_episode_reward":-793.37, "episode_reward_trend_value": 0.006910727689051379, "biggest_recent_change": 0.6353732348228505 },
{"total_number_of_episodes":910, "number_of_timesteps":910000, "per_episode_reward":-793.62, "episode_reward_trend_value": 0.0007676328070576548, "biggest_recent_change": 0.6353732348228505 },
{"total_number_of_episodes":920, "number_of_timesteps":920000, "per_episode_reward":-793.86, "episode_reward_trend_value": -0.005065293944897399, "biggest_recent_change": 0.6353732348228505 },
{"total_number_of_episodes":930, "number_of_timesteps":930000, "per_episode_reward":-794.19, "episode_reward_trend_value": -0.00574395949229635, "biggest_recent_change": 0.6353732348228505 },
{"total_number_of_episodes":940, "number_of_timesteps":940000, "per_episode_reward":-794.13, "episode_reward_trend_value": -0.004184692156762695, "biggest_recent_change": 0.6353732348228505 },
{"total_number_of_episodes":950, "number_of_timesteps":950000, "per_episode_reward":-794.71, "episode_reward_trend_value": -0.006015972958445874, "biggest_recent_change": 0.6353732348228505 },
{"total_number_of_episodes":960, "number_of_timesteps":960000, "per_episode_reward":-794.56, "episode_reward_trend_value": -0.006561130795777597, "biggest_recent_change": 0.6353732348228505 },
{"total_number_of_episodes":970, "number_of_timesteps":970000, "per_episode_reward":-794.57, "episode_reward_trend_value": -0.006372221706830159, "biggest_recent_change": 0.6353732348228505 },
{"total_number_of_episodes":980, "number_of_timesteps":980000, "per_episode_reward":-794.61, "episode_reward_trend_value": -0.0067621814868756595, "biggest_recent_change": 0.6353732348228505 },
{"total_number_of_episodes":990, "number_of_timesteps":990000, "per_episode_reward":-794.63, "episode_reward_trend_value": -0.013947923772318266, "biggest_recent_change": 0.584108899865214 },
{"total_number_of_episodes":1000, "number_of_timesteps":1000000, "per_episode_reward":-794.34, "episode_reward_trend_value": -0.007951223973447618, "biggest_recent_change": 0.584108899865214 },
{"total_number_of_episodes":1010, "number_of_timesteps":1010000, "per_episode_reward":-794.74, "episode_reward_trend_value": -0.009797367837341477, "biggest_recent_change": 0.584108899865214 },
{"total_number_of_episodes":1020, "number_of_timesteps":1020000, "per_episode_reward":-794.79, "episode_reward_trend_value": -0.006593809074317708, "biggest_recent_change": 0.584108899865214 },
{"total_number_of_episodes":1030, "number_of_timesteps":1030000, "per_episode_reward":-794.85, "episode_reward_trend_value": -0.008059480749784598, "biggest_recent_change": 0.584108899865214 },
{"total_number_of_episodes":1040, "number_of_timesteps":1040000, "per_episode_reward":-794.98, "episode_reward_trend_value": -0.002937376813635082, "biggest_recent_change": 0.40521337134896385 },
{"total_number_of_episodes":1050, "number_of_timesteps":1050000, "per_episode_reward":-794.94, "episode_reward_trend_value": -0.004215821374162311, "biggest_recent_change": 0.40521337134896385 },
{"total_number_of_episodes":1060, "number_of_timesteps":1060000, "per_episode_reward":-795.09, "episode_reward_trend_value": -0.005721666851095758, "biggest_recent_change": 0.40521337134896385 },
{"total_number_of_episodes":1070, "number_of_timesteps":1070000, "per_episode_reward":-794.84, "episode_reward_trend_value": -0.0025380112219952227, "biggest_recent_change": 0.40521337134896385 },
{"total_number_of_episodes":1080, "number_of_timesteps":1080000, "per_episode_reward":-794.60, "episode_reward_trend_value": 0.0003321573504144403, "biggest_recent_change": 0.40521337134896385 },
{"total_number_of_episodes":1090, "number_of_timesteps":1090000, "per_episode_reward":-794.70, "episode_reward_trend_value": -0.004031805125994955, "biggest_recent_change": 0.40521337134896385 },
{"total_number_of_episodes":1100, "number_of_timesteps":1100000, "per_episode_reward":-716.16, "episode_reward_trend_value": 0.873177956947619, "biggest_recent_change": 78.54366521527629 },
{"total_number_of_episodes":1110, "number_of_timesteps":1110000, "per_episode_reward":-715.88, "episode_reward_trend_value": 0.8767549111557805, "biggest_recent_change": 78.54366521527629 },
{"total_number_of_episodes":1120, "number_of_timesteps":1120000, "per_episode_reward":-715.53, "episode_reward_trend_value": 0.8813188672452068, "biggest_recent_change": 78.54366521527629 },
{"total_number_of_episodes":1130, "number_of_timesteps":1130000, "per_episode_reward":-715.91, "episode_reward_trend_value": 0.8785463134943257, "biggest_recent_change": 78.54366521527629 },
{"total_number_of_episodes":1140, "number_of_timesteps":1140000, "per_episode_reward":-715.33, "episode_reward_trend_value": 0.8844657559372093, "biggest_recent_change": 78.54366521527629 },
{"total_number_of_episodes":1150, "number_of_timesteps":1150000, "per_episode_reward":-715.24, "episode_reward_trend_value": 0.88717300822311, "biggest_recent_change": 78.54366521527629 },
{"total_number_of_episodes":1160, "number_of_timesteps":1160000, "per_episode_reward":-715.10, "episode_reward_trend_value": 0.8860830170598535, "biggest_recent_change": 78.54366521527629 },
{"total_number_of_episodes":1170, "number_of_timesteps":1170000, "per_episode_reward":-715.23, "episode_reward_trend_value": 0.8818943260477858, "biggest_recent_change": 78.54366521527629 },
{"total_number_of_episodes":1180, "number_of_timesteps":1180000, "per_episode_reward":-714.63, "episode_reward_trend_value": 0.889627677360252, "biggest_recent_change": 78.54366521527629 },
{"total_number_of_episodes":1190, "number_of_timesteps":1190000, "per_episode_reward":-714.28, "episode_reward_trend_value": 0.0208264895369982, "biggest_recent_change": 0.5923353294789422 },
{"total_number_of_episodes":1200, "number_of_timesteps":1200000, "per_episode_reward":-714.24, "episode_reward_trend_value": 0.01815802024488878, "biggest_recent_change": 0.5923353294789422 },
{"total_number_of_episodes":1210, "number_of_timesteps":1210000, "per_episode_reward":-714.34, "episode_reward_trend_value": 0.0133042725165789, "biggest_recent_change": 0.5923353294789422 },
{"total_number_of_episodes":1220, "number_of_timesteps":1220000, "per_episode_reward":-713.97, "episode_reward_trend_value": 0.021487687241259816, "biggest_recent_change": 0.5923353294789422 },
{"total_number_of_episodes":1230, "number_of_timesteps":1230000, "per_episode_reward":-713.72, "episode_reward_trend_value": 0.01794860496879917, "biggest_recent_change": 0.5923353294789422 },
{"total_number_of_episodes":1240, "number_of_timesteps":1240000, "per_episode_reward":-714.06, "episode_reward_trend_value": 0.013135144192777818, "biggest_recent_change": 0.5923353294789422 },
{"total_number_of_episodes":1250, "number_of_timesteps":1250000, "per_episode_reward":-714.37, "episode_reward_trend_value": 0.008065899297942753, "biggest_recent_change": 0.5923353294789422 },
{"total_number_of_episodes":1260, "number_of_timesteps":1260000, "per_episode_reward":-713.98, "episode_reward_trend_value": 0.013864866401137179, "biggest_recent_change": 0.5923353294789422 },
{"total_number_of_episodes":1270, "number_of_timesteps":1270000, "per_episode_reward":-713.81, "episode_reward_trend_value": 0.009192098131509535, "biggest_recent_change": 0.3918964488512984 },
{"total_number_of_episodes":1280, "number_of_timesteps":1280000, "per_episode_reward":-713.77, "episode_reward_trend_value": 0.005714208509875284, "biggest_recent_change": 0.3918964488512984 },
{"total_number_of_episodes":1290, "number_of_timesteps":1290000, "per_episode_reward":-713.83, "episode_reward_trend_value": 0.004621412821979397, "biggest_recent_change": 0.3918964488512984 },
{"total_number_of_episodes":1300, "number_of_timesteps":1300000, "per_episode_reward":-713.77, "episode_reward_trend_value": 0.006248096136086031, "biggest_recent_change": 0.3918964488512984 },
{"total_number_of_episodes":1310, "number_of_timesteps":1310000, "per_episode_reward":-713.60, "episode_reward_trend_value": 0.004178695814851431, "biggest_recent_change": 0.3918964488512984 },
{"total_number_of_episodes":1320, "number_of_timesteps":1320000, "per_episode_reward":-713.26, "episode_reward_trend_value": 0.005080924132108875, "biggest_recent_change": 0.3918964488512984 },
{"total_number_of_episodes":1330, "number_of_timesteps":1330000, "per_episode_reward":-713.34, "episode_reward_trend_value": 0.00803487496583178, "biggest_recent_change": 0.3918964488512984 },
{"total_number_of_episodes":1340, "number_of_timesteps":1340000, "per_episode_reward":-713.66, "episode_reward_trend_value": 0.007897174135108445, "biggest_recent_change": 0.3918964488512984 },
{"total_number_of_episodes":1350, "number_of_timesteps":1350000, "per_episode_reward":-713.46, "episode_reward_trend_value": 0.005759065308849484, "biggest_recent_change": 0.33444833559599374 },
{"total_number_of_episodes":1360, "number_of_timesteps":1360000, "per_episode_reward":-713.42, "episode_reward_trend_value": 0.004287861845139357, "biggest_recent_change": 0.33444833559599374 },
{"total_number_of_episodes":1370, "number_of_timesteps":1370000, "per_episode_reward":-713.19, "episode_reward_trend_value": 0.00639431491929372, "biggest_recent_change": 0.33444833559599374 },
{"total_number_of_episodes":1380, "number_of_timesteps":1380000, "per_episode_reward":-713.15, "episode_reward_trend_value": 0.007522332518194869, "biggest_recent_change": 0.33444833559599374 },
{"total_number_of_episodes":1390, "number_of_timesteps":1390000, "per_episode_reward":-713.60, "episode_reward_trend_value": 0.0019256171525209235, "biggest_recent_change": 0.4497972475710412 },
{"total_number_of_episodes":1400, "number_of_timesteps":1400000, "per_episode_reward":-713.47, "episode_reward_trend_value": 0.0014571856087627565, "biggest_recent_change": 0.4497972475710412 },
{"total_number_of_episodes":1410, "number_of_timesteps":1410000, "per_episode_reward":-713.84, "episode_reward_trend_value": -0.006403965661192818, "biggest_recent_change": 0.4497972475710412 },
{"total_number_of_episodes":1420, "number_of_timesteps":1420000, "per_episode_reward":-714.14, "episode_reward_trend_value": -0.008948164852749112, "biggest_recent_change": 0.4497972475710412 },
{"total_number_of_episodes":1430, "number_of_timesteps":1430000, "per_episode_reward":-714.34, "episode_reward_trend_value": -0.007587621254482554, "biggest_recent_change": 0.4497972475710412 },
{"total_number_of_episodes":1440, "number_of_timesteps":1440000, "per_episode_reward":-714.29, "episode_reward_trend_value": -0.00917925429252288, "biggest_recent_change": 0.4497972475710412 },
{"total_number_of_episodes":1450, "number_of_timesteps":1450000, "per_episode_reward":-713.88, "episode_reward_trend_value": -0.005132025097884707, "biggest_recent_change": 0.4497972475710412 },
{"total_number_of_episodes":1460, "number_of_timesteps":1460000, "per_episode_reward":-713.77, "episode_reward_trend_value": -0.00643427123711591, "biggest_recent_change": 0.4497972475710412 },
{"total_number_of_episodes":1470, "number_of_timesteps":1470000, "per_episode_reward":-713.76, "episode_reward_trend_value": -0.0067439518765266045, "biggest_recent_change": 0.4497972475710412 },
{"total_number_of_episodes":1480, "number_of_timesteps":1480000, "per_episode_reward":-713.89, "episode_reward_trend_value": -0.0031791383584921585, "biggest_recent_change": 0.40362850099597836 },
{"total_number_of_episodes":1490, "number_of_timesteps":1490000, "per_episode_reward":-713.84, "episode_reward_trend_value": -0.004211411863560796, "biggest_recent_change": 0.40362850099597836 },
{"total_number_of_episodes":1500, "number_of_timesteps":1500000, "per_episode_reward":-714.13, "episode_reward_trend_value": -0.0032184379405116995, "biggest_recent_change": 0.40362850099597836 },
{"total_number_of_episodes":1510, "number_of_timesteps":1510000, "per_episode_reward":-713.98, "episode_reward_trend_value": 0.0018240111529040505, "biggest_recent_change": 0.40362850099597836 },
{"total_number_of_episodes":1520, "number_of_timesteps":1520000, "per_episode_reward":-714.26, "episode_reward_trend_value": 0.0009192685584253013, "biggest_recent_change": 0.40362850099597836 },
{"total_number_of_episodes":1530, "number_of_timesteps":1530000, "per_episode_reward":-714.55, "episode_reward_trend_value": -0.0029772739934552797, "biggest_recent_change": 0.40362850099597836 },
{"total_number_of_episodes":1540, "number_of_timesteps":1540000, "per_episode_reward":-714.27, "episode_reward_trend_value": -0.004333816763409636, "biggest_recent_change": 0.2944691486048896 },
{"total_number_of_episodes":1550, "number_of_timesteps":1550000, "per_episode_reward":-714.38, "episode_reward_trend_value": -0.006754227216989268, "biggest_recent_change": 0.2944691486048896 },
{"total_number_of_episodes":1560, "number_of_timesteps":1560000, "per_episode_reward":-714.37, "episode_reward_trend_value": -0.006744807267364194, "biggest_recent_change": 0.2944691486048896 },
{"total_number_of_episodes":1570, "number_of_timesteps":1570000, "per_episode_reward":-714.46, "episode_reward_trend_value": -0.006328216899397729, "biggest_recent_change": 0.2944691486048896 },
{"total_number_of_episodes":1580, "number_of_timesteps":1580000, "per_episode_reward":-714.65, "episode_reward_trend_value": -0.008960415763632075, "biggest_recent_change": 0.2944691486048896 },
{"total_number_of_episodes":1590, "number_of_timesteps":1590000, "per_episode_reward":-714.94, "episode_reward_trend_value": -0.008987865115142945, "biggest_recent_change": 0.2944691486048896 },
{"total_number_of_episodes":1600, "number_of_timesteps":1600000, "per_episode_reward":-715.01, "episode_reward_trend_value": -0.011501201654698433, "biggest_recent_change": 0.2944691486048896 },
{"total_number_of_episodes":1610, "number_of_timesteps":1610000, "per_episode_reward":-714.91, "episode_reward_trend_value": -0.007206067888479437, "biggest_recent_change": 0.2944691486048896 },
{"total_number_of_episodes":1620, "number_of_timesteps":1620000, "per_episode_reward":-715.02, "episode_reward_trend_value": -0.005215708187342393, "biggest_recent_change": 0.28615806726156734 },
{"total_number_of_episodes":1630, "number_of_timesteps":1630000, "per_episode_reward":-715.08, "episode_reward_trend_value": -0.009014616491252684, "biggest_recent_change": 0.28615806726156734 },
{"total_number_of_episodes":1640, "number_of_timesteps":1640000, "per_episode_reward":-715.04, "episode_reward_trend_value": -0.00737817699609751, "biggest_recent_change": 0.28615806726156734 },
{"total_number_of_episodes":1650, "number_of_timesteps":1650000, "per_episode_reward":-714.95, "episode_reward_trend_value": -0.006495416220781812, "biggest_recent_change": 0.28615806726156734 },
{"total_number_of_episodes":1660, "number_of_timesteps":1660000, "per_episode_reward":-714.81, "episode_reward_trend_value": -0.003930153405113086, "biggest_recent_change": 0.28615806726156734 },
{"total_number_of_episodes":1670, "number_of_timesteps":1670000, "per_episode_reward":-714.43, "episode_reward_trend_value": 0.002471402534771035, "biggest_recent_change": 0.3817905955331753 },
{"total_number_of_episodes":1680, "number_of_timesteps":1680000, "per_episode_reward":-714.64, "episode_reward_trend_value": 0.0033094951342213285, "biggest_recent_change": 0.3817905955331753 },
{"total_number_of_episodes":1690, "number_of_timesteps":1690000, "per_episode_reward":-714.80, "episode_reward_trend_value": 0.002374088422929314, "biggest_recent_change": 0.3817905955331753 },
{"total_number_of_episodes":1700, "number_of_timesteps":1700000, "per_episode_reward":-714.67, "episode_reward_trend_value": 0.002642233663396433, "biggest_recent_change": 0.3817905955331753 },
{"total_number_of_episodes":1710, "number_of_timesteps":1710000, "per_episode_reward":-714.90, "episode_reward_trend_value": 0.0013579582505814061, "biggest_recent_change": 0.3817905955331753 },
{"total_number_of_episodes":1720, "number_of_timesteps":1720000, "per_episode_reward":-714.85, "episode_reward_trend_value": 0.0026346745216756064, "biggest_recent_change": 0.3817905955331753 },
{"total_number_of_episodes":1730, "number_of_timesteps":1730000, "per_episode_reward":-714.85, "episode_reward_trend_value": 0.0021764193702147674, "biggest_recent_change": 0.3817905955331753 },
{"total_number_of_episodes":1740, "number_of_timesteps":1740000, "per_episode_reward":-715.04, "episode_reward_trend_value": -0.000987106659073965, "biggest_recent_change": 0.3817905955331753 },
{"total_number_of_episodes":1750, "number_of_timesteps":1750000, "per_episode_reward":-714.98, "episode_reward_trend_value": -0.0019296056015870515, "biggest_recent_change": 0.3817905955331753 },
{"total_number_of_episodes":1760, "number_of_timesteps":1760000, "per_episode_reward":-715.10, "episode_reward_trend_value": -0.007437137704241398, "biggest_recent_change": 0.23092156265590802 },
{"total_number_of_episodes":1770, "number_of_timesteps":1770000, "per_episode_reward":-715.26, "episode_reward_trend_value": -0.006864812239478447, "biggest_recent_change": 0.23092156265590802 },
{"total_number_of_episodes":1780, "number_of_timesteps":1780000, "per_episode_reward":-715.10, "episode_reward_trend_value": -0.003395991773310117, "biggest_recent_change": 0.23092156265590802 },
{"total_number_of_episodes":1790, "number_of_timesteps":1790000, "per_episode_reward":-715.08, "episode_reward_trend_value": -0.004552877069234759, "biggest_recent_change": 0.23092156265590802 },
{"total_number_of_episodes":1800, "number_of_timesteps":1800000, "per_episode_reward":-715.15, "episode_reward_trend_value": -0.0027942226298313874, "biggest_recent_change": 0.19174421233685734 },
{"total_number_of_episodes":1810, "number_of_timesteps":1810000, "per_episode_reward":-714.99, "episode_reward_trend_value": -0.0016491443244528153, "biggest_recent_change": 0.19174421233685734 },
{"total_number_of_episodes":1820, "number_of_timesteps":1820000, "per_episode_reward":-715.08, "episode_reward_trend_value": -0.0026385299411584937, "biggest_recent_change": 0.19174421233685734 },
{"total_number_of_episodes":1830, "number_of_timesteps":1830000, "per_episode_reward":-715.00, "episode_reward_trend_value": 0.000484514744624044, "biggest_recent_change": 0.15922044148237546 },
{"total_number_of_episodes":1840, "number_of_timesteps":1840000, "per_episode_reward":-714.92, "episode_reward_trend_value": 0.00074454351399835, "biggest_recent_change": 0.15922044148237546 },
{"total_number_of_episodes":1850, "number_of_timesteps":1850000, "per_episode_reward":-714.61, "episode_reward_trend_value": 0.005451198858867833, "biggest_recent_change": 0.3097116873325376 },
{"total_number_of_episodes":1860, "number_of_timesteps":1860000, "per_episode_reward":-714.68, "episode_reward_trend_value": 0.00643736999746933, "biggest_recent_change": 0.3097116873325376 },
{"total_number_of_episodes":1870, "number_of_timesteps":1870000, "per_episode_reward":-714.57, "episode_reward_trend_value": 0.005979314342363927, "biggest_recent_change": 0.3097116873325376 },
{"total_number_of_episodes":1880, "number_of_timesteps":1880000, "per_episode_reward":-714.72, "episode_reward_trend_value": 0.003950324971418064, "biggest_recent_change": 0.3097116873325376 },
{"total_number_of_episodes":1890, "number_of_timesteps":1890000, "per_episode_reward":-714.62, "episode_reward_trend_value": 0.005873737635770087, "biggest_recent_change": 0.3097116873325376 },
{"total_number_of_episodes":1900, "number_of_timesteps":1900000, "per_episode_reward":-714.56, "episode_reward_trend_value": 0.004773874342743056, "biggest_recent_change": 0.3097116873325376 },
{"total_number_of_episodes":1910, "number_of_timesteps":1910000, "per_episode_reward":-714.60, "episode_reward_trend_value": 0.005408204231266861, "biggest_recent_change": 0.3097116873325376 },
{"total_number_of_episodes":1920, "number_of_timesteps":1920000, "per_episode_reward":-714.67, "episode_reward_trend_value": 0.0035812438333778117, "biggest_recent_change": 0.3097116873325376 },
{"total_number_of_episodes":1930, "number_of_timesteps":1930000, "per_episode_reward":-714.55, "episode_reward_trend_value": 0.004086038338421834, "biggest_recent_change": 0.3097116873325376 },
{"total_number_of_episodes":1940, "number_of_timesteps":1940000, "per_episode_reward":-714.68, "episode_reward_trend_value": -0.0007749705865712208, "biggest_recent_change": 0.1574137790290706 },
{"total_number_of_episodes":1950, "number_of_timesteps":1950000, "per_episode_reward":-714.68, "episode_reward_trend_value": -2.896575504109933e-05, "biggest_recent_change": 0.1574137790290706 },
{"total_number_of_episodes":1960, "number_of_timesteps":1960000, "per_episode_reward":-714.68, "episode_reward_trend_value": -0.0012387068714967123, "biggest_recent_change": 0.1574137790290706 },
{"total_number_of_episodes":1970, "number_of_timesteps":1970000, "per_episode_reward":-714.86, "episode_reward_trend_value": -0.0014949041153588243, "biggest_recent_change": 0.1804715309766607 },
{"total_number_of_episodes":1980, "number_of_timesteps":1980000, "per_episode_reward":-714.79, "episode_reward_trend_value": -0.0018510683276089265, "biggest_recent_change": 0.1804715309766607 },
{"total_number_of_episodes":1990, "number_of_timesteps":1990000, "per_episode_reward":-714.59, "episode_reward_trend_value": -0.000278755007315744, "biggest_recent_change": 0.2001199186846634 },
{"total_number_of_episodes":2000, "number_of_timesteps":2000000, "per_episode_reward":-714.52, "episode_reward_trend_value": 0.0008205685630905969, "biggest_recent_change": 0.2001199186846634 },
{"total_number_of_episodes":2010, "number_of_timesteps":2010000, "per_episode_reward":-714.33, "episode_reward_trend_value": 0.003800189667449811, "biggest_recent_change": 0.2001199186846634 },
{"total_number_of_episodes":2020, "number_of_timesteps":2020000, "per_episode_reward":-714.23, "episode_reward_trend_value": 0.0035987581762469746, "biggest_recent_change": 0.2001199186846634 },
{"total_number_of_episodes":2030, "number_of_timesteps":2030000, "per_episode_reward":-714.28, "episode_reward_trend_value": 0.004410745809793247, "biggest_recent_change": 0.2001199186846634 },
{"total_number_of_episodes":2040, "number_of_timesteps":2040000, "per_episode_reward":-714.08, "episode_reward_trend_value": 0.00662892824589739, "biggest_recent_change": 0.2001199186846634 },
{"total_number_of_episodes":2050, "number_of_timesteps":2050000, "per_episode_reward":-714.23, "episode_reward_trend_value": 0.004977014440597335, "biggest_recent_change": 0.2001199186846634 },
{"total_number_of_episodes":2060, "number_of_timesteps":2060000, "per_episode_reward":-713.94, "episode_reward_trend_value": 0.010212875357721411, "biggest_recent_change": 0.2907559515645062 },
{"total_number_of_episodes":2070, "number_of_timesteps":2070000, "per_episode_reward":-713.74, "episode_reward_trend_value": 0.011683076108826957, "biggest_recent_change": 0.2907559515645062 },
{"total_number_of_episodes":2080, "number_of_timesteps":2080000, "per_episode_reward":-713.90, "episode_reward_trend_value": 0.007657825847878838, "biggest_recent_change": 0.2907559515645062 },
{"total_number_of_episodes":2090, "number_of_timesteps":2090000, "per_episode_reward":-713.90, "episode_reward_trend_value": 0.006977186441454503, "biggest_recent_change": 0.2907559515645062 },
{"total_number_of_episodes":2100, "number_of_timesteps":2100000, "per_episode_reward":-713.91, "episode_reward_trend_value": 0.00465874969291791, "biggest_recent_change": 0.2907559515645062 },
{"total_number_of_episodes":2110, "number_of_timesteps":2110000, "per_episode_reward":-714.20, "episode_reward_trend_value": 0.00033040344454396593, "biggest_recent_change": 0.2907559515645062 },
{"total_number_of_episodes":2120, "number_of_timesteps":2120000, "per_episode_reward":-713.99, "episode_reward_trend_value": 0.00322382418569431, "biggest_recent_change": 0.2907559515645062 },
{"total_number_of_episodes":2130, "number_of_timesteps":2130000, "per_episode_reward":-714.17, "episode_reward_trend_value": -0.0009373312575651956, "biggest_recent_change": 0.2907559515645062 },
{"total_number_of_episodes":2140, "number_of_timesteps":2140000, "per_episode_reward":-714.23, "episode_reward_trend_value": -1.0765461411210708e-07, "biggest_recent_change": 0.2907559515645062 },
{"total_number_of_episodes":2150, "number_of_timesteps":2150000, "per_episode_reward":-714.29, "episode_reward_trend_value": -0.003891494231550041, "biggest_recent_change": 0.2842680511112121 },
{"total_number_of_episodes":2160, "number_of_timesteps":2160000, "per_episode_reward":-714.06, "episode_reward_trend_value": -0.0036002165209816266, "biggest_recent_change": 0.2842680511112121 },
{"total_number_of_episodes":2170, "number_of_timesteps":2170000, "per_episode_reward":-713.91, "episode_reward_trend_value": -5.980168148324891e-05, "biggest_recent_change": 0.2842680511112121 },
{"total_number_of_episodes":2180, "number_of_timesteps":2180000, "per_episode_reward":-714.06, "episode_reward_trend_value": -0.0017949025838434965, "biggest_recent_change": 0.2842680511112121 },
{"total_number_of_episodes":2190, "number_of_timesteps":2190000, "per_episode_reward":-714.03, "episode_reward_trend_value": -0.001321895924791584, "biggest_recent_change": 0.2842680511112121 },
{"total_number_of_episodes":2200, "number_of_timesteps":2200000, "per_episode_reward":-713.96, "episode_reward_trend_value": 0.0025892176375224255, "biggest_recent_change": 0.22694275913022466 },
{"total_number_of_episodes":2210, "number_of_timesteps":2210000, "per_episode_reward":-713.92, "episode_reward_trend_value": 0.0007493468494494159, "biggest_recent_change": 0.22694275913022466 },
{"total_number_of_episodes":2220, "number_of_timesteps":2220000, "per_episode_reward":-713.96, "episode_reward_trend_value": 0.002270822138393669, "biggest_recent_change": 0.22694275913022466 },
{"total_number_of_episodes":2230, "number_of_timesteps":2230000, "per_episode_reward":-714.07, "episode_reward_trend_value": 0.0017724672054568046, "biggest_recent_change": 0.22694275913022466 },
{"total_number_of_episodes":2240, "number_of_timesteps":2240000, "per_episode_reward":-714.10, "episode_reward_trend_value": 0.0021091739019096977, "biggest_recent_change": 0.22694275913022466 },
{"total_number_of_episodes":2250, "number_of_timesteps":2250000, "per_episode_reward":-714.11, "episode_reward_trend_value": -0.0005669518598400221, "biggest_recent_change": 0.15648473075418678 },
{"total_number_of_episodes":2260, "number_of_timesteps":2260000, "per_episode_reward":-714.08, "episode_reward_trend_value": -0.001885354420542424, "biggest_recent_change": 0.15130600250063253 },
{"total_number_of_episodes":2270, "number_of_timesteps":2270000, "per_episode_reward":-713.92, "episode_reward_trend_value": 0.0015409221327685272, "biggest_recent_change": 0.15705888729735307 },
{"total_number_of_episodes":2280, "number_of_timesteps":2280000, "per_episode_reward":-714.10, "episode_reward_trend_value": -0.000769331374690157, "biggest_recent_change": 0.18094225075901704 },
{"total_number_of_episodes":2290, "number_of_timesteps":2290000, "per_episode_reward":-714.01, "episode_reward_trend_value": -0.0005578433338933387, "biggest_recent_change": 0.18094225075901704 },
{"total_number_of_episodes":2300, "number_of_timesteps":2300000, "per_episode_reward":-713.89, "episode_reward_trend_value": 0.00037121825556596377, "biggest_recent_change": 0.18094225075901704 },
{"total_number_of_episodes":2310, "number_of_timesteps":2310000, "per_episode_reward":-714.02, "episode_reward_trend_value": -0.0006771345260833388, "biggest_recent_change": 0.18094225075901704 },
{"total_number_of_episodes":2320, "number_of_timesteps":2320000, "per_episode_reward":-713.59, "episode_reward_trend_value": 0.005360615753016241, "biggest_recent_change": 0.4366943241069521 },
{"total_number_of_episodes":2330, "number_of_timesteps":2330000, "per_episode_reward":-713.91, "episode_reward_trend_value": 0.002123401462132632, "biggest_recent_change": 0.4366943241069521 },
{"total_number_of_episodes":2340, "number_of_timesteps":2340000, "per_episode_reward":-713.24, "episode_reward_trend_value": 0.009762292335286322, "biggest_recent_change": 0.673591619156582 },
{"total_number_of_episodes":2350, "number_of_timesteps":2350000, "per_episode_reward":-713.31, "episode_reward_trend_value": 0.00856408172288386, "biggest_recent_change": 0.673591619156582 },
{"total_number_of_episodes":2360, "number_of_timesteps":2360000, "per_episode_reward":-713.32, "episode_reward_trend_value": 0.0066705225355677715, "biggest_recent_change": 0.673591619156582 },
{"total_number_of_episodes":2370, "number_of_timesteps":2370000, "per_episode_reward":-713.48, "episode_reward_trend_value": 0.006886178882770208, "biggest_recent_change": 0.673591619156582 },
{"total_number_of_episodes":2380, "number_of_timesteps":2380000, "per_episode_reward":-713.34, "episode_reward_trend_value": 0.007424082109424843, "biggest_recent_change": 0.673591619156582 },
{"total_number_of_episodes":2390, "number_of_timesteps":2390000, "per_episode_reward":-713.08, "episode_reward_trend_value": 0.008960840266758573, "biggest_recent_change": 0.673591619156582 },
{"total_number_of_episodes":2400, "number_of_timesteps":2400000, "per_episode_reward":-713.15, "episode_reward_trend_value": 0.009770428672119478, "biggest_recent_change": 0.673591619156582 },
{"total_number_of_episodes":2410, "number_of_timesteps":2410000, "per_episode_reward":-712.91, "episode_reward_trend_value": 0.007528018418492542, "biggest_recent_change": 0.673591619156582 },
{"total_number_of_episodes":2420, "number_of_timesteps":2420000, "per_episode_reward":-712.86, "episode_reward_trend_value": 0.01163399844608269, "biggest_recent_change": 0.673591619156582 },
{"total_number_of_episodes":2430, "number_of_timesteps":2430000, "per_episode_reward":-712.82, "episode_reward_trend_value": 0.0046522711482604485, "biggest_recent_change": 0.26204304409066026 },
{"total_number_of_episodes":2440, "number_of_timesteps":2440000, "per_episode_reward":-712.81, "episode_reward_trend_value": 0.005554226902731645, "biggest_recent_change": 0.26204304409066026 },
{"total_number_of_episodes":2450, "number_of_timesteps":2450000, "per_episode_reward":-712.62, "episode_reward_trend_value": 0.007731917663347253, "biggest_recent_change": 0.26204304409066026 },
{"total_number_of_episodes":2460, "number_of_timesteps":2460000, "per_episode_reward":-712.82, "episode_reward_trend_value": 0.007364687208552015, "biggest_recent_change": 0.26204304409066026 },
{"total_number_of_episodes":2470, "number_of_timesteps":2470000, "per_episode_reward":-712.61, "episode_reward_trend_value": 0.008179776670023228, "biggest_recent_change": 0.26204304409066026 },
{"total_number_of_episodes":2480, "number_of_timesteps":2480000, "per_episode_reward":-712.90, "episode_reward_trend_value": 0.002080158579642304, "biggest_recent_change": 0.2869225840436229 },
{"total_number_of_episodes":2490, "number_of_timesteps":2490000, "per_episode_reward":-712.72, "episode_reward_trend_value": 0.004675666232612406, "biggest_recent_change": 0.2869225840436229 },
{"total_number_of_episodes":2500, "number_of_timesteps":2500000, "per_episode_reward":-713.11, "episode_reward_trend_value": -0.002242597372148541, "biggest_recent_change": 0.38776632314795734 },
{"total_number_of_episodes":2510, "number_of_timesteps":2510000, "per_episode_reward":-712.87, "episode_reward_trend_value": -9.761193662648617e-05, "biggest_recent_change": 0.38776632314795734 },
{"total_number_of_episodes":2520, "number_of_timesteps":2520000, "per_episode_reward":-712.73, "episode_reward_trend_value": 0.0009222997716897731, "biggest_recent_change": 0.38776632314795734 },
{"total_number_of_episodes":2530, "number_of_timesteps":2530000, "per_episode_reward":-712.64, "episode_reward_trend_value": 0.0018247094974652908, "biggest_recent_change": 0.38776632314795734 },
{"total_number_of_episodes":2540, "number_of_timesteps":2540000, "per_episode_reward":-712.61, "episode_reward_trend_value": 0.00013879687233561022, "biggest_recent_change": 0.38776632314795734 },
{"total_number_of_episodes":2550, "number_of_timesteps":2550000, "per_episode_reward":-712.53, "episode_reward_trend_value": 0.0031365917850495056, "biggest_recent_change": 0.38776632314795734 },
{"total_number_of_episodes":2560, "number_of_timesteps":2560000, "per_episode_reward":-712.49, "episode_reward_trend_value": 0.0013232612988897726, "biggest_recent_change": 0.38776632314795734 },
{"total_number_of_episodes":2570, "number_of_timesteps":2570000, "per_episode_reward":-712.52, "episode_reward_trend_value": 0.004187614766844186, "biggest_recent_change": 0.38776632314795734 },
{"total_number_of_episodes":2580, "number_of_timesteps":2580000, "per_episode_reward":-712.60, "episode_reward_trend_value": 0.0014365621002411899, "biggest_recent_change": 0.38776632314795734 },
{"total_number_of_episodes":2590, "number_of_timesteps":2590000, "per_episode_reward":-712.61, "episode_reward_trend_value": 0.005540777155267228, "biggest_recent_change": 0.24207236782160635 },
{"total_number_of_episodes":2600, "number_of_timesteps":2600000, "per_episode_reward":-712.64, "episode_reward_trend_value": 0.0025697412081588785, "biggest_recent_change": 0.13702821610104365 },
Hit early stopping because biggest_recent_change: 0.09238243839695315 < 0.1
{"total_number_of_episodes":2610, "number_of_timesteps":2610000, "per_episode_reward":-712.64, "episode_reward_trend_value": 0.0009912197451184411, "biggest_recent_change": 0.09238243839695315 },
[32m[I 2022-10-23 17:17:26,011][0m Trial 19 finished with value: -712.5164533545487 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 1, 'permaban_threshold': 0}. Best is trial 1 with value: -706.6358055663736.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-787.27, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-798.92, "episode_reward_trend_value": -1.1653950904205659, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-799.94, "episode_reward_trend_value": -0.6337459118063521, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-805.52, "episode_reward_trend_value": -0.608435311261087, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-801.60, "episode_reward_trend_value": -0.3583207842493493, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-809.35, "episode_reward_trend_value": -0.4415514545789188, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-812.36, "episode_reward_trend_value": -0.4181649367141195, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-811.98, "episode_reward_trend_value": -0.3529785951901918, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-813.32, "episode_reward_trend_value": -0.32560448452566815, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-807.38, "episode_reward_trend_value": -0.2235105890396047, "biggest_recent_change": 11.653950904205658 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-809.94, "episode_reward_trend_value": -0.1224624844157145, "biggest_recent_change": 7.7447413589719645 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-804.28, "episode_reward_trend_value": -0.048208253181815056, "biggest_recent_change": 7.7447413589719645 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-804.08, "episode_reward_trend_value": 0.015971161550915745, "biggest_recent_change": 7.7447413589719645 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-797.09, "episode_reward_trend_value": 0.05010834874230012, "biggest_recent_change": 7.7447413589719645 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-796.81, "episode_reward_trend_value": 0.1392716856831473, "biggest_recent_change": 6.9925748150832305 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-796.97, "episode_reward_trend_value": 0.17099872440802022, "biggest_recent_change": 6.9925748150832305 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-799.12, "episode_reward_trend_value": 0.14281205064416252, "biggest_recent_change": 6.9925748150832305 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-799.47, "episode_reward_trend_value": 0.15381852928547787, "biggest_recent_change": 6.9925748150832305 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-798.56, "episode_reward_trend_value": 0.09808840527451315, "biggest_recent_change": 6.9925748150832305 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-798.84, "episode_reward_trend_value": 0.12338610260101177, "biggest_recent_change": 6.9925748150832305 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-797.07, "episode_reward_trend_value": 0.08017917450603262, "biggest_recent_change": 6.9925748150832305 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-798.08, "episode_reward_trend_value": 0.0667454891125241, "biggest_recent_change": 6.9925748150832305 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-798.64, "episode_reward_trend_value": -0.017171944117141416, "biggest_recent_change": 2.155406099213451 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-798.76, "episode_reward_trend_value": -0.021660704255108007, "biggest_recent_change": 2.155406099213451 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-798.98, "episode_reward_trend_value": -0.022304402025929447, "biggest_recent_change": 2.155406099213451 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-798.76, "episode_reward_trend_value": 0.004040612381936625, "biggest_recent_change": 1.773289950581443 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-798.02, "episode_reward_trend_value": 0.016118346157185846, "biggest_recent_change": 1.773289950581443 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-798.85, "episode_reward_trend_value": -0.0032418767253135335, "biggest_recent_change": 1.773289950581443 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-798.08, "episode_reward_trend_value": 0.008445203130613916, "biggest_recent_change": 1.773289950581443 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-797.17, "episode_reward_trend_value": -0.0011516183480542472, "biggest_recent_change": 1.0110254611755636 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-797.77, "episode_reward_trend_value": 0.0033687984271194987, "biggest_recent_change": 0.9095760175013083 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-796.95, "episode_reward_trend_value": 0.018693066714096554, "biggest_recent_change": 0.9095760175013083 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-795.29, "episode_reward_trend_value": 0.03860646961813675, "biggest_recent_change": 1.6681768146509057 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-794.85, "episode_reward_trend_value": 0.04585020227228951, "biggest_recent_change": 1.6681768146509057 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-795.17, "episode_reward_trend_value": 0.03985797653232213, "biggest_recent_change": 1.6681768146509057 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-794.22, "episode_reward_trend_value": 0.042236874245326, "biggest_recent_change": 1.6681768146509057 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-795.34, "episode_reward_trend_value": 0.039023315732302104, "biggest_recent_change": 1.6681768146509057 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-796.03, "episode_reward_trend_value": 0.022731042447943158, "biggest_recent_change": 1.6681768146509057 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-794.88, "episode_reward_trend_value": 0.025442727416076293, "biggest_recent_change": 1.6681768146509057 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-795.90, "episode_reward_trend_value": 0.02082944732995252, "biggest_recent_change": 1.6681768146509057 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-795.63, "episode_reward_trend_value": 0.01466777521448345, "biggest_recent_change": 1.6681768146509057 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-795.28, "episode_reward_trend_value": 8.321001713890534e-05, "biggest_recent_change": 1.1536276646332908 },
{"total_number_of_episodes":440, "number_of_timesteps":440000, "per_episode_reward":-795.59, "episode_reward_trend_value": -0.008206733670430014, "biggest_recent_change": 1.1536276646332908 },
{"total_number_of_episodes":450, "number_of_timesteps":450000, "per_episode_reward":-794.78, "episode_reward_trend_value": 0.004325431670948395, "biggest_recent_change": 1.1536276646332908 },
{"total_number_of_episodes":460, "number_of_timesteps":460000, "per_episode_reward":-794.31, "episode_reward_trend_value": -0.0010206111729164351, "biggest_recent_change": 1.1536276646332908 },
{"total_number_of_episodes":470, "number_of_timesteps":470000, "per_episode_reward":-793.75, "episode_reward_trend_value": 0.017569348118951843, "biggest_recent_change": 1.1536276646332908 },
{"total_number_of_episodes":480, "number_of_timesteps":480000, "per_episode_reward":-794.76, "episode_reward_trend_value": 0.014100567321300637, "biggest_recent_change": 1.1536276646332908 },
{"total_number_of_episodes":490, "number_of_timesteps":490000, "per_episode_reward":-794.81, "episode_reward_trend_value": 0.0007597317983873431, "biggest_recent_change": 1.019383159161066 },
{"total_number_of_episodes":500, "number_of_timesteps":500000, "per_episode_reward":-793.47, "episode_reward_trend_value": 0.027022314394572634, "biggest_recent_change": 1.3442492744956098 },
{"total_number_of_episodes":510, "number_of_timesteps":510000, "per_episode_reward":-793.35, "episode_reward_trend_value": 0.025433612311741804, "biggest_recent_change": 1.3442492744956098 },
{"total_number_of_episodes":520, "number_of_timesteps":520000, "per_episode_reward":-794.00, "episode_reward_trend_value": 0.014158431368019187, "biggest_recent_change": 1.3442492744956098 },
{"total_number_of_episodes":530, "number_of_timesteps":530000, "per_episode_reward":-793.54, "episode_reward_trend_value": 0.02272407352210798, "biggest_recent_change": 1.3442492744956098 },
{"total_number_of_episodes":540, "number_of_timesteps":540000, "per_episode_reward":-792.20, "episode_reward_trend_value": 0.02867039731442749, "biggest_recent_change": 1.3442492744956098 },
{"total_number_of_episodes":550, "number_of_timesteps":550000, "per_episode_reward":-792.53, "episode_reward_trend_value": 0.019790529179642313, "biggest_recent_change": 1.3442492744956098 },
{"total_number_of_episodes":560, "number_of_timesteps":560000, "per_episode_reward":-792.22, "episode_reward_trend_value": 0.017068679018781393, "biggest_recent_change": 1.3442492744956098 },
{"total_number_of_episodes":570, "number_of_timesteps":570000, "per_episode_reward":-792.89, "episode_reward_trend_value": 0.020772735561653613, "biggest_recent_change": 1.3442492744956098 },
{"total_number_of_episodes":580, "number_of_timesteps":580000, "per_episode_reward":-792.39, "episode_reward_trend_value": 0.02685052232290875, "biggest_recent_change": 1.3442492744956098 },
{"total_number_of_episodes":590, "number_of_timesteps":590000, "per_episode_reward":-792.37, "episode_reward_trend_value": 0.012136565861698676, "biggest_recent_change": 1.339408902930245 },
{"total_number_of_episodes":600, "number_of_timesteps":600000, "per_episode_reward":-791.91, "episode_reward_trend_value": 0.015896405019239206, "biggest_recent_change": 1.339408902930245 },
{"total_number_of_episodes":610, "number_of_timesteps":610000, "per_episode_reward":-791.46, "episode_reward_trend_value": 0.028268048451937172, "biggest_recent_change": 1.339408902930245 },
{"total_number_of_episodes":620, "number_of_timesteps":620000, "per_episode_reward":-791.61, "episode_reward_trend_value": 0.02144617220705843, "biggest_recent_change": 1.339408902930245 },
{"total_number_of_episodes":630, "number_of_timesteps":630000, "per_episode_reward":-791.04, "episode_reward_trend_value": 0.012908292923776165, "biggest_recent_change": 0.6761213201596092 },
{"total_number_of_episodes":640, "number_of_timesteps":640000, "per_episode_reward":-791.39, "episode_reward_trend_value": 0.012733195341686748, "biggest_recent_change": 0.6761213201596092 },
{"total_number_of_episodes":650, "number_of_timesteps":650000, "per_episode_reward":-792.20, "episode_reward_trend_value": 0.0002565350123278323, "biggest_recent_change": 0.8097153459465289 },
{"total_number_of_episodes":660, "number_of_timesteps":660000, "per_episode_reward":-791.87, "episode_reward_trend_value": 0.011405588046449743, "biggest_recent_change": 0.8097153459465289 },
{"total_number_of_episodes":670, "number_of_timesteps":670000, "per_episode_reward":-791.51, "episode_reward_trend_value": 0.009782763807498264, "biggest_recent_change": 0.8097153459465289 },
{"total_number_of_episodes":680, "number_of_timesteps":680000, "per_episode_reward":-790.97, "episode_reward_trend_value": 0.015628106820119884, "biggest_recent_change": 0.8097153459465289 },
{"total_number_of_episodes":690, "number_of_timesteps":690000, "per_episode_reward":-790.72, "episode_reward_trend_value": 0.013300733103215456, "biggest_recent_change": 0.8097153459465289 },
{"total_number_of_episodes":700, "number_of_timesteps":700000, "per_episode_reward":-790.31, "episode_reward_trend_value": 0.012822589905724473, "biggest_recent_change": 0.8097153459465289 },
{"total_number_of_episodes":710, "number_of_timesteps":710000, "per_episode_reward":-789.86, "episode_reward_trend_value": 0.019520022388079875, "biggest_recent_change": 0.8097153459465289 },
{"total_number_of_episodes":720, "number_of_timesteps":720000, "per_episode_reward":-790.08, "episode_reward_trend_value": 0.010687721237978723, "biggest_recent_change": 0.8097153459465289 },
{"total_number_of_episodes":730, "number_of_timesteps":730000, "per_episode_reward":-790.05, "episode_reward_trend_value": 0.014873561966199609, "biggest_recent_change": 0.8097153459465289 },
{"total_number_of_episodes":740, "number_of_timesteps":740000, "per_episode_reward":-790.54, "episode_reward_trend_value": 0.018348684411148498, "biggest_recent_change": 0.5460740641226494 },
{"total_number_of_episodes":750, "number_of_timesteps":750000, "per_episode_reward":-790.31, "episode_reward_trend_value": 0.017286540155660328, "biggest_recent_change": 0.5460740641226494 },
{"total_number_of_episodes":760, "number_of_timesteps":760000, "per_episode_reward":-790.87, "episode_reward_trend_value": 0.007158205464515453, "biggest_recent_change": 0.5576510276246154 },
{"total_number_of_episodes":770, "number_of_timesteps":770000, "per_episode_reward":-791.86, "episode_reward_trend_value": -0.009940633346927068, "biggest_recent_change": 0.9928214289071775 },
{"total_number_of_episodes":780, "number_of_timesteps":780000, "per_episode_reward":-790.63, "episode_reward_trend_value": 0.00101123441641625, "biggest_recent_change": 1.2362462807524253 },
{"total_number_of_episodes":790, "number_of_timesteps":790000, "per_episode_reward":-789.90, "episode_reward_trend_value": 0.0045020033261696174, "biggest_recent_change": 1.2362462807524253 },
{"total_number_of_episodes":800, "number_of_timesteps":800000, "per_episode_reward":-789.83, "episode_reward_trend_value": 0.00024745943483266174, "biggest_recent_change": 1.2362462807524253 },
{"total_number_of_episodes":810, "number_of_timesteps":810000, "per_episode_reward":-789.19, "episode_reward_trend_value": 0.00990201739799785, "biggest_recent_change": 1.2362462807524253 },
{"total_number_of_episodes":820, "number_of_timesteps":820000, "per_episode_reward":-790.13, "episode_reward_trend_value": -0.0009651791871205104, "biggest_recent_change": 1.2362462807524253 },
{"total_number_of_episodes":830, "number_of_timesteps":830000, "per_episode_reward":-790.36, "episode_reward_trend_value": 0.00201407753226148, "biggest_recent_change": 1.2362462807524253 },
{"total_number_of_episodes":840, "number_of_timesteps":840000, "per_episode_reward":-790.85, "episode_reward_trend_value": -0.005999119980875397, "biggest_recent_change": 1.2362462807524253 },
{"total_number_of_episodes":850, "number_of_timesteps":850000, "per_episode_reward":-790.81, "episode_reward_trend_value": 0.0006378445897477528, "biggest_recent_change": 1.2362462807524253 },
{"total_number_of_episodes":860, "number_of_timesteps":860000, "per_episode_reward":-790.81, "episode_reward_trend_value": 0.01168320846241689, "biggest_recent_change": 1.2362462807524253 },
{"total_number_of_episodes":870, "number_of_timesteps":870000, "per_episode_reward":-790.58, "episode_reward_trend_value": 0.0005007610834127263, "biggest_recent_change": 0.945589984666185 },
{"total_number_of_episodes":880, "number_of_timesteps":880000, "per_episode_reward":-790.08, "episode_reward_trend_value": -0.001937894003319545, "biggest_recent_change": 0.945589984666185 },
{"total_number_of_episodes":890, "number_of_timesteps":890000, "per_episode_reward":-790.53, "episode_reward_trend_value": -0.0077321931356765265, "biggest_recent_change": 0.945589984666185 },
{"total_number_of_episodes":900, "number_of_timesteps":900000, "per_episode_reward":-790.39, "episode_reward_trend_value": -0.01339734346437101, "biggest_recent_change": 0.945589984666185 },
{"total_number_of_episodes":910, "number_of_timesteps":910000, "per_episode_reward":-790.13, "episode_reward_trend_value": 1.5166199347681363e-05, "biggest_recent_change": 0.5059049271953882 },
{"total_number_of_episodes":920, "number_of_timesteps":920000, "per_episode_reward":-790.12, "episode_reward_trend_value": 0.0027217346470062393, "biggest_recent_change": 0.5059049271953882 },
{"total_number_of_episodes":930, "number_of_timesteps":930000, "per_episode_reward":-790.06, "episode_reward_trend_value": 0.008794980489464404, "biggest_recent_change": 0.5059049271953882 },
{"total_number_of_episodes":940, "number_of_timesteps":940000, "per_episode_reward":-790.06, "episode_reward_trend_value": 0.008339739776705653, "biggest_recent_change": 0.5059049271953882 },
{"total_number_of_episodes":950, "number_of_timesteps":950000, "per_episode_reward":-790.38, "episode_reward_trend_value": 0.004809145676567065, "biggest_recent_change": 0.5059049271953882 },
{"total_number_of_episodes":960, "number_of_timesteps":960000, "per_episode_reward":-789.87, "episode_reward_trend_value": 0.007951638831289326, "biggest_recent_change": 0.512650400567054 },
{"total_number_of_episodes":970, "number_of_timesteps":970000, "per_episode_reward":-789.83, "episode_reward_trend_value": 0.0026838087301611167, "biggest_recent_change": 0.512650400567054 },
{"total_number_of_episodes":980, "number_of_timesteps":980000, "per_episode_reward":-789.63, "episode_reward_trend_value": 0.010018694665715581, "biggest_recent_change": 0.512650400567054 },
{"total_number_of_episodes":990, "number_of_timesteps":990000, "per_episode_reward":-789.13, "episode_reward_trend_value": 0.013997103103933265, "biggest_recent_change": 0.512650400567054 },
{"total_number_of_episodes":1000, "number_of_timesteps":1000000, "per_episode_reward":-788.93, "episode_reward_trend_value": 0.013375915299495014, "biggest_recent_change": 0.512650400567054 },
{"total_number_of_episodes":1010, "number_of_timesteps":1010000, "per_episode_reward":-789.03, "episode_reward_trend_value": 0.012092065723327829, "biggest_recent_change": 0.512650400567054 },
{"total_number_of_episodes":1020, "number_of_timesteps":1020000, "per_episode_reward":-789.09, "episode_reward_trend_value": 0.010770651533492683, "biggest_recent_change": 0.512650400567054 },
{"total_number_of_episodes":1030, "number_of_timesteps":1030000, "per_episode_reward":-789.03, "episode_reward_trend_value": 0.011471966399555337, "biggest_recent_change": 0.512650400567054 },
{"total_number_of_episodes":1040, "number_of_timesteps":1040000, "per_episode_reward":-789.05, "episode_reward_trend_value": 0.01471271213725787, "biggest_recent_change": 0.512650400567054 },
{"total_number_of_episodes":1050, "number_of_timesteps":1050000, "per_episode_reward":-788.48, "episode_reward_trend_value": 0.01540059055023701, "biggest_recent_change": 0.5745594577351767 },
{"total_number_of_episodes":1060, "number_of_timesteps":1060000, "per_episode_reward":-788.12, "episode_reward_trend_value": 0.019022504089079777, "biggest_recent_change": 0.5745594577351767 },
{"total_number_of_episodes":1070, "number_of_timesteps":1070000, "per_episode_reward":-788.00, "episode_reward_trend_value": 0.018091561388425565, "biggest_recent_change": 0.5745594577351767 },
{"total_number_of_episodes":1080, "number_of_timesteps":1080000, "per_episode_reward":-787.78, "episode_reward_trend_value": 0.015066292818748175, "biggest_recent_change": 0.5745594577351767 },
{"total_number_of_episodes":1090, "number_of_timesteps":1090000, "per_episode_reward":-787.40, "episode_reward_trend_value": 0.01700141719823023, "biggest_recent_change": 0.5745594577351767 },
{"total_number_of_episodes":1100, "number_of_timesteps":1100000, "per_episode_reward":-709.11, "episode_reward_trend_value": 0.8880448213700813, "biggest_recent_change": 78.29312985274407 },
{"total_number_of_episodes":1110, "number_of_timesteps":1110000, "per_episode_reward":-709.38, "episode_reward_trend_value": 0.8856720966413036, "biggest_recent_change": 78.29312985274407 },
{"total_number_of_episodes":1120, "number_of_timesteps":1120000, "per_episode_reward":-709.27, "episode_reward_trend_value": 0.8861648052471613, "biggest_recent_change": 78.29312985274407 },
{"total_number_of_episodes":1130, "number_of_timesteps":1130000, "per_episode_reward":-709.42, "episode_reward_trend_value": 0.8848584424448165, "biggest_recent_change": 78.29312985274407 },
{"total_number_of_episodes":1140, "number_of_timesteps":1140000, "per_episode_reward":-709.30, "episode_reward_trend_value": 0.8798206072723891, "biggest_recent_change": 78.29312985274407 },
{"total_number_of_episodes":1150, "number_of_timesteps":1150000, "per_episode_reward":-709.59, "episode_reward_trend_value": 0.8725953500745517, "biggest_recent_change": 78.29312985274407 },
{"total_number_of_episodes":1160, "number_of_timesteps":1160000, "per_episode_reward":-709.54, "episode_reward_trend_value": 0.8718161782533469, "biggest_recent_change": 78.29312985274407 },
{"total_number_of_episodes":1170, "number_of_timesteps":1170000, "per_episode_reward":-709.47, "episode_reward_trend_value": 0.8701292506941323, "biggest_recent_change": 78.29312985274407 },
{"total_number_of_episodes":1180, "number_of_timesteps":1180000, "per_episode_reward":-709.47, "episode_reward_trend_value": 0.8658428265148095, "biggest_recent_change": 78.29312985274407 },
{"total_number_of_episodes":1190, "number_of_timesteps":1190000, "per_episode_reward":-709.61, "episode_reward_trend_value": -0.00560969683107765, "biggest_recent_change": 0.29250071121566634 },
{"total_number_of_episodes":1200, "number_of_timesteps":1200000, "per_episode_reward":-709.62, "episode_reward_trend_value": -0.0027083661794070697, "biggest_recent_change": 0.29250071121566634 },
{"total_number_of_episodes":1210, "number_of_timesteps":1210000, "per_episode_reward":-709.14, "episode_reward_trend_value": 0.0014934922981372741, "biggest_recent_change": 0.48433349503500267 },
{"total_number_of_episodes":1220, "number_of_timesteps":1220000, "per_episode_reward":-709.42, "episode_reward_trend_value": 2.3083069747068898e-05, "biggest_recent_change": 0.48433349503500267 },
{"total_number_of_episodes":1230, "number_of_timesteps":1230000, "per_episode_reward":-709.81, "episode_reward_trend_value": -0.005766746980965662, "biggest_recent_change": 0.48433349503500267 },
{"total_number_of_episodes":1240, "number_of_timesteps":1240000, "per_episode_reward":-709.63, "episode_reward_trend_value": -0.000417965645672464, "biggest_recent_change": 0.48433349503500267 },
{"total_number_of_episodes":1250, "number_of_timesteps":1250000, "per_episode_reward":-709.48, "episode_reward_trend_value": 0.0006029633407517092, "biggest_recent_change": 0.48433349503500267 },
{"total_number_of_episodes":1260, "number_of_timesteps":1260000, "per_episode_reward":-709.31, "episode_reward_trend_value": 0.0016949515423814571, "biggest_recent_change": 0.48433349503500267 },
{"total_number_of_episodes":1270, "number_of_timesteps":1270000, "per_episode_reward":-708.81, "episode_reward_trend_value": 0.007401562458947966, "biggest_recent_change": 0.5076069831743553 },
{"total_number_of_episodes":1280, "number_of_timesteps":1280000, "per_episode_reward":-708.86, "episode_reward_trend_value": 0.0082855366571923, "biggest_recent_change": 0.5076069831743553 },
{"total_number_of_episodes":1290, "number_of_timesteps":1290000, "per_episode_reward":-708.76, "episode_reward_trend_value": 0.009625110435757101, "biggest_recent_change": 0.5076069831743553 },
{"total_number_of_episodes":1300, "number_of_timesteps":1300000, "per_episode_reward":-708.70, "episode_reward_trend_value": 0.004837203962333407, "biggest_recent_change": 0.5076069831743553 },
{"total_number_of_episodes":1310, "number_of_timesteps":1310000, "per_episode_reward":-708.64, "episode_reward_trend_value": 0.008619127156480822, "biggest_recent_change": 0.5076069831743553 },
{"total_number_of_episodes":1320, "number_of_timesteps":1320000, "per_episode_reward":-708.61, "episode_reward_trend_value": 0.013340641677282545, "biggest_recent_change": 0.5076069831743553 },
{"total_number_of_episodes":1330, "number_of_timesteps":1330000, "per_episode_reward":-709.16, "episode_reward_trend_value": 0.005137044157472953, "biggest_recent_change": 0.5494341678221417 },
{"total_number_of_episodes":1340, "number_of_timesteps":1340000, "per_episode_reward":-709.10, "episode_reward_trend_value": 0.004190288008822386, "biggest_recent_change": 0.5494341678221417 },
{"total_number_of_episodes":1350, "number_of_timesteps":1350000, "per_episode_reward":-709.22, "episode_reward_trend_value": 0.0010848587719730555, "biggest_recent_change": 0.5494341678221417 },
{"total_number_of_episodes":1360, "number_of_timesteps":1360000, "per_episode_reward":-708.82, "episode_reward_trend_value": -0.0001453012523117852, "biggest_recent_change": 0.5494341678221417 },
{"total_number_of_episodes":1370, "number_of_timesteps":1370000, "per_episode_reward":-708.32, "episode_reward_trend_value": 0.006101059404090847, "biggest_recent_change": 0.5494341678221417 },
{"total_number_of_episodes":1380, "number_of_timesteps":1380000, "per_episode_reward":-708.39, "episode_reward_trend_value": 0.0041291362069526055, "biggest_recent_change": 0.5494341678221417 },
{"total_number_of_episodes":1390, "number_of_timesteps":1390000, "per_episode_reward":-708.03, "episode_reward_trend_value": 0.007453495655153751, "biggest_recent_change": 0.5494341678221417 },
{"total_number_of_episodes":1400, "number_of_timesteps":1400000, "per_episode_reward":-708.04, "episode_reward_trend_value": 0.006682009536027383, "biggest_recent_change": 0.5494341678221417 },
{"total_number_of_episodes":1410, "number_of_timesteps":1410000, "per_episode_reward":-708.35, "episode_reward_trend_value": 0.0028883040119543086, "biggest_recent_change": 0.5494341678221417 },
{"total_number_of_episodes":1420, "number_of_timesteps":1420000, "per_episode_reward":-708.32, "episode_reward_trend_value": 0.009340195680159822, "biggest_recent_change": 0.5041328885324674 },
{"total_number_of_episodes":1430, "number_of_timesteps":1430000, "per_episode_reward":-708.47, "episode_reward_trend_value": 0.007053620117618367, "biggest_recent_change": 0.5041328885324674 },
{"total_number_of_episodes":1440, "number_of_timesteps":1440000, "per_episode_reward":-708.44, "episode_reward_trend_value": 0.008588013673075542, "biggest_recent_change": 0.5041328885324674 },
{"total_number_of_episodes":1450, "number_of_timesteps":1450000, "per_episode_reward":-708.31, "episode_reward_trend_value": 0.005640589636181579, "biggest_recent_change": 0.5041328885324674 },
{"total_number_of_episodes":1460, "number_of_timesteps":1460000, "per_episode_reward":-708.18, "episode_reward_trend_value": 0.0015405488786806624, "biggest_recent_change": 0.3526142627649733 },
{"total_number_of_episodes":1470, "number_of_timesteps":1470000, "per_episode_reward":-708.10, "episode_reward_trend_value": 0.003185291930157695, "biggest_recent_change": 0.3526142627649733 },
{"total_number_of_episodes":1480, "number_of_timesteps":1480000, "per_episode_reward":-708.24, "episode_reward_trend_value": -0.0023315402923546774, "biggest_recent_change": 0.316427602641852 },
{"total_number_of_episodes":1490, "number_of_timesteps":1490000, "per_episode_reward":-708.00, "episode_reward_trend_value": 0.0003982930534789375, "biggest_recent_change": 0.316427602641852 },
{"total_number_of_episodes":1500, "number_of_timesteps":1500000, "per_episode_reward":-708.09, "episode_reward_trend_value": 0.0029556317730516134, "biggest_recent_change": 0.24188982212456267 },
{"total_number_of_episodes":1510, "number_of_timesteps":1510000, "per_episode_reward":-708.22, "episode_reward_trend_value": 0.001171120432960985, "biggest_recent_change": 0.24188982212456267 },
{"total_number_of_episodes":1520, "number_of_timesteps":1520000, "per_episode_reward":-708.39, "episode_reward_trend_value": 0.0008820260690526993, "biggest_recent_change": 0.24188982212456267 },
{"total_number_of_episodes":1530, "number_of_timesteps":1530000, "per_episode_reward":-708.06, "episode_reward_trend_value": 0.004270488551141069, "biggest_recent_change": 0.3309458090767521 },
{"total_number_of_episodes":1540, "number_of_timesteps":1540000, "per_episode_reward":-708.49, "episode_reward_trend_value": -0.0019419319270064586, "biggest_recent_change": 0.42749342536501445 },
{"total_number_of_episodes":1550, "number_of_timesteps":1550000, "per_episode_reward":-708.59, "episode_reward_trend_value": -0.004592407283900634, "biggest_recent_change": 0.42749342536501445 },
{"total_number_of_episodes":1560, "number_of_timesteps":1560000, "per_episode_reward":-708.48, "episode_reward_trend_value": -0.004240383530784837, "biggest_recent_change": 0.42749342536501445 },
{"total_number_of_episodes":1570, "number_of_timesteps":1570000, "per_episode_reward":-708.33, "episode_reward_trend_value": -0.0009193496119286869, "biggest_recent_change": 0.42749342536501445 },
{"total_number_of_episodes":1580, "number_of_timesteps":1580000, "per_episode_reward":-708.33, "episode_reward_trend_value": -0.003622942766859675, "biggest_recent_change": 0.42749342536501445 },
{"total_number_of_episodes":1590, "number_of_timesteps":1590000, "per_episode_reward":-708.27, "episode_reward_trend_value": -0.0019725654048632652, "biggest_recent_change": 0.42749342536501445 },
{"total_number_of_episodes":1600, "number_of_timesteps":1600000, "per_episode_reward":-708.38, "episode_reward_trend_value": -0.001824976768581463, "biggest_recent_change": 0.42749342536501445 },
{"total_number_of_episodes":1610, "number_of_timesteps":1610000, "per_episode_reward":-708.33, "episode_reward_trend_value": 0.0006917839111376149, "biggest_recent_change": 0.42749342536501445 },
{"total_number_of_episodes":1620, "number_of_timesteps":1620000, "per_episode_reward":-708.16, "episode_reward_trend_value": -0.0011601649151392244, "biggest_recent_change": 0.42749342536501445 },
{"total_number_of_episodes":1630, "number_of_timesteps":1630000, "per_episode_reward":-708.06, "episode_reward_trend_value": 0.004785144918116657, "biggest_recent_change": 0.16427041471183657 },
{"total_number_of_episodes":1640, "number_of_timesteps":1640000, "per_episode_reward":-708.04, "episode_reward_trend_value": 0.006059956334268514, "biggest_recent_change": 0.16427041471183657 },
{"total_number_of_episodes":1650, "number_of_timesteps":1650000, "per_episode_reward":-708.01, "episode_reward_trend_value": 0.005223312559242762, "biggest_recent_change": 0.16427041471183657 },
{"total_number_of_episodes":1660, "number_of_timesteps":1660000, "per_episode_reward":-707.96, "episode_reward_trend_value": 0.004035519784912999, "biggest_recent_change": 0.16427041471183657 },
{"total_number_of_episodes":1670, "number_of_timesteps":1670000, "per_episode_reward":-707.76, "episode_reward_trend_value": 0.0063105737596111395, "biggest_recent_change": 0.20332129590360637 },
{"total_number_of_episodes":1680, "number_of_timesteps":1680000, "per_episode_reward":-707.89, "episode_reward_trend_value": 0.004129178239636783, "biggest_recent_change": 0.20332129590360637 },
{"total_number_of_episodes":1690, "number_of_timesteps":1690000, "per_episode_reward":-707.95, "episode_reward_trend_value": 0.0047634781362388716, "biggest_recent_change": 0.20332129590360637 },
{"total_number_of_episodes":1700, "number_of_timesteps":1700000, "per_episode_reward":-708.29, "episode_reward_trend_value": 0.00040653294724961596, "biggest_recent_change": 0.33819171451818875 },
{"total_number_of_episodes":1710, "number_of_timesteps":1710000, "per_episode_reward":-707.99, "episode_reward_trend_value": 0.0019683709617058613, "biggest_recent_change": 0.33819171451818875 },
{"total_number_of_episodes":1720, "number_of_timesteps":1720000, "per_episode_reward":-707.73, "episode_reward_trend_value": 0.0035931050576373815, "biggest_recent_change": 0.33819171451818875 },
{"total_number_of_episodes":1730, "number_of_timesteps":1730000, "per_episode_reward":-707.78, "episode_reward_trend_value": 0.002944289646056101, "biggest_recent_change": 0.33819171451818875 },
{"total_number_of_episodes":1740, "number_of_timesteps":1740000, "per_episode_reward":-707.81, "episode_reward_trend_value": 0.0022372370282823795, "biggest_recent_change": 0.33819171451818875 },
{"total_number_of_episodes":1750, "number_of_timesteps":1750000, "per_episode_reward":-707.68, "episode_reward_trend_value": 0.003100805007651767, "biggest_recent_change": 0.33819171451818875 },
{"total_number_of_episodes":1760, "number_of_timesteps":1760000, "per_episode_reward":-708.14, "episode_reward_trend_value": -0.004242667343270215, "biggest_recent_change": 0.457591215679372 },
{"total_number_of_episodes":1770, "number_of_timesteps":1770000, "per_episode_reward":-708.07, "episode_reward_trend_value": -0.0019021400860538052, "biggest_recent_change": 0.457591215679372 },
{"total_number_of_episodes":1780, "number_of_timesteps":1780000, "per_episode_reward":-708.07, "episode_reward_trend_value": -0.0013415048790054445, "biggest_recent_change": 0.457591215679372 },
{"total_number_of_episodes":1790, "number_of_timesteps":1790000, "per_episode_reward":-708.42, "episode_reward_trend_value": -0.00137395585083545, "biggest_recent_change": 0.457591215679372 },
{"total_number_of_episodes":1800, "number_of_timesteps":1800000, "per_episode_reward":-708.09, "episode_reward_trend_value": -0.0011143845298293956, "biggest_recent_change": 0.457591215679372 },
{"total_number_of_episodes":1810, "number_of_timesteps":1810000, "per_episode_reward":-707.96, "episode_reward_trend_value": -0.0024951060191432993, "biggest_recent_change": 0.457591215679372 },
{"total_number_of_episodes":1820, "number_of_timesteps":1820000, "per_episode_reward":-707.93, "episode_reward_trend_value": -0.0017091803100091358, "biggest_recent_change": 0.457591215679372 },
{"total_number_of_episodes":1830, "number_of_timesteps":1830000, "per_episode_reward":-708.10, "episode_reward_trend_value": -0.0032365257718197427, "biggest_recent_change": 0.457591215679372 },
{"total_number_of_episodes":1840, "number_of_timesteps":1840000, "per_episode_reward":-707.75, "episode_reward_trend_value": -0.0007313339927665968, "biggest_recent_change": 0.457591215679372 },
{"total_number_of_episodes":1850, "number_of_timesteps":1850000, "per_episode_reward":-708.30, "episode_reward_trend_value": -0.0017166344119219604, "biggest_recent_change": 0.5462682534033547 },
{"total_number_of_episodes":1860, "number_of_timesteps":1860000, "per_episode_reward":-708.41, "episode_reward_trend_value": -0.003862115674199787, "biggest_recent_change": 0.5462682534033547 },
{"total_number_of_episodes":1870, "number_of_timesteps":1870000, "per_episode_reward":-708.61, "episode_reward_trend_value": -0.005976951347232292, "biggest_recent_change": 0.5462682534033547 },
{"total_number_of_episodes":1880, "number_of_timesteps":1880000, "per_episode_reward":-708.55, "episode_reward_trend_value": -0.0014977533154794602, "biggest_recent_change": 0.5462682534033547 },
{"total_number_of_episodes":1890, "number_of_timesteps":1890000, "per_episode_reward":-708.53, "episode_reward_trend_value": -0.004924616952116089, "biggest_recent_change": 0.5462682534033547 },
{"total_number_of_episodes":1900, "number_of_timesteps":1900000, "per_episode_reward":-708.77, "episode_reward_trend_value": -0.009010508349638258, "biggest_recent_change": 0.5462682534033547 },
{"total_number_of_episodes":1910, "number_of_timesteps":1910000, "per_episode_reward":-709.11, "episode_reward_trend_value": -0.013030342726981165, "biggest_recent_change": 0.5462682534033547 },
{"total_number_of_episodes":1920, "number_of_timesteps":1920000, "per_episode_reward":-708.88, "episode_reward_trend_value": -0.008647688663354023, "biggest_recent_change": 0.5462682534033547 },
{"total_number_of_episodes":1930, "number_of_timesteps":1930000, "per_episode_reward":-708.98, "episode_reward_trend_value": -0.01364098450404122, "biggest_recent_change": 0.5462682534033547 },
{"total_number_of_episodes":1940, "number_of_timesteps":1940000, "per_episode_reward":-708.98, "episode_reward_trend_value": -0.007646516065882578, "biggest_recent_change": 0.3381257014905259 },
{"total_number_of_episodes":1950, "number_of_timesteps":1950000, "per_episode_reward":-709.00, "episode_reward_trend_value": -0.006546951411326063, "biggest_recent_change": 0.3381257014905259 },
{"total_number_of_episodes":1960, "number_of_timesteps":1960000, "per_episode_reward":-708.95, "episode_reward_trend_value": -0.0038025612147761317, "biggest_recent_change": 0.3381257014905259 },
{"total_number_of_episodes":1970, "number_of_timesteps":1970000, "per_episode_reward":-708.92, "episode_reward_trend_value": -0.0041039866427215405, "biggest_recent_change": 0.3381257014905259 },
{"total_number_of_episodes":1980, "number_of_timesteps":1980000, "per_episode_reward":-708.94, "episode_reward_trend_value": -0.004589593425072912, "biggest_recent_change": 0.3381257014905259 },
{"total_number_of_episodes":1990, "number_of_timesteps":1990000, "per_episode_reward":-708.97, "episode_reward_trend_value": -0.0022183933522038997, "biggest_recent_change": 0.3381257014905259 },
{"total_number_of_episodes":2000, "number_of_timesteps":2000000, "per_episode_reward":-709.10, "episode_reward_trend_value": 0.00012077410815310134, "biggest_recent_change": 0.22659473908481687 },
{"total_number_of_episodes":2010, "number_of_timesteps":2010000, "per_episode_reward":-709.00, "episode_reward_trend_value": -0.0012849846483580527, "biggest_recent_change": 0.12760063005839584 },
{"total_number_of_episodes":2020, "number_of_timesteps":2020000, "per_episode_reward":-708.84, "episode_reward_trend_value": 0.0015039181385860603, "biggest_recent_change": 0.15288406916738495 },
{"total_number_of_episodes":2030, "number_of_timesteps":2030000, "per_episode_reward":-708.78, "episode_reward_trend_value": 0.002240354633478445, "biggest_recent_change": 0.15288406916738495 },
{"total_number_of_episodes":2040, "number_of_timesteps":2040000, "per_episode_reward":-708.79, "episode_reward_trend_value": 0.0023697366757763423, "biggest_recent_change": 0.15288406916738495 },
{"total_number_of_episodes":2050, "number_of_timesteps":2050000, "per_episode_reward":-708.99, "episode_reward_trend_value": -0.0003966853671019837, "biggest_recent_change": 0.20086087844038047 },
{"total_number_of_episodes":2060, "number_of_timesteps":2060000, "per_episode_reward":-708.77, "episode_reward_trend_value": 0.0016941615495663126, "biggest_recent_change": 0.22306345485992551 },
{"total_number_of_episodes":2070, "number_of_timesteps":2070000, "per_episode_reward":-708.67, "episode_reward_trend_value": 0.002983029799991578, "biggest_recent_change": 0.22306345485992551 },
{"total_number_of_episodes":2080, "number_of_timesteps":2080000, "per_episode_reward":-708.62, "episode_reward_trend_value": 0.0038293005288993404, "biggest_recent_change": 0.22306345485992551 },
{"total_number_of_episodes":2090, "number_of_timesteps":2090000, "per_episode_reward":-708.88, "episode_reward_trend_value": 0.0024008939800170184, "biggest_recent_change": 0.2561572194578048 },
{"total_number_of_episodes":2100, "number_of_timesteps":2100000, "per_episode_reward":-708.68, "episode_reward_trend_value": 0.0035443710228277164, "biggest_recent_change": 0.2561572194578048 },
{"total_number_of_episodes":2110, "number_of_timesteps":2110000, "per_episode_reward":-708.86, "episode_reward_trend_value": -0.0001855684373190848, "biggest_recent_change": 0.2561572194578048 },
{"total_number_of_episodes":2120, "number_of_timesteps":2120000, "per_episode_reward":-708.58, "episode_reward_trend_value": 0.002201956855654114, "biggest_recent_change": 0.27439046693882574 },
{"total_number_of_episodes":2130, "number_of_timesteps":2130000, "per_episode_reward":-708.84, "episode_reward_trend_value": -0.0005627133820603882, "biggest_recent_change": 0.27439046693882574 },
{"total_number_of_episodes":2140, "number_of_timesteps":2140000, "per_episode_reward":-708.42, "episode_reward_trend_value": 0.0063631582412932, "biggest_recent_change": 0.42246756766144244 },
{"total_number_of_episodes":2150, "number_of_timesteps":2150000, "per_episode_reward":-708.02, "episode_reward_trend_value": 0.008299244642220554, "biggest_recent_change": 0.42246756766144244 },
{"total_number_of_episodes":2160, "number_of_timesteps":2160000, "per_episode_reward":-707.99, "episode_reward_trend_value": 0.007564320508525827, "biggest_recent_change": 0.42246756766144244 },
{"total_number_of_episodes":2170, "number_of_timesteps":2170000, "per_episode_reward":-707.97, "episode_reward_trend_value": 0.0072327622095662665, "biggest_recent_change": 0.42246756766144244 },
{"total_number_of_episodes":2180, "number_of_timesteps":2180000, "per_episode_reward":-707.96, "episode_reward_trend_value": 0.01018679262486507, "biggest_recent_change": 0.42246756766144244 },
{"total_number_of_episodes":2190, "number_of_timesteps":2190000, "per_episode_reward":-707.84, "episode_reward_trend_value": 0.009304542148861906, "biggest_recent_change": 0.42246756766144244 },
{"total_number_of_episodes":2200, "number_of_timesteps":2200000, "per_episode_reward":-707.82, "episode_reward_trend_value": 0.011492525685138037, "biggest_recent_change": 0.42246756766144244 },
{"total_number_of_episodes":2210, "number_of_timesteps":2210000, "per_episode_reward":-707.82, "episode_reward_trend_value": 0.008479960077180396, "biggest_recent_change": 0.42246756766144244 },
{"total_number_of_episodes":2220, "number_of_timesteps":2220000, "per_episode_reward":-707.79, "episode_reward_trend_value": 0.011716760202863144, "biggest_recent_change": 0.42246756766144244 },
{"total_number_of_episodes":2230, "number_of_timesteps":2230000, "per_episode_reward":-707.75, "episode_reward_trend_value": 0.007377686841896826, "biggest_recent_change": 0.3973112309433873 },
{"total_number_of_episodes":2240, "number_of_timesteps":2240000, "per_episode_reward":-707.75, "episode_reward_trend_value": 0.003005482790172209, "biggest_recent_change": 0.12358684201149117 },
{"total_number_of_episodes":2250, "number_of_timesteps":2250000, "per_episode_reward":-707.66, "episode_reward_trend_value": 0.0037572704475337558, "biggest_recent_change": 0.12358684201149117 },
{"total_number_of_episodes":2260, "number_of_timesteps":2260000, "per_episode_reward":-707.86, "episode_reward_trend_value": 0.0011964949346052992, "biggest_recent_change": 0.2089223024634066 },
{"total_number_of_episodes":2270, "number_of_timesteps":2270000, "per_episode_reward":-707.83, "episode_reward_trend_value": 0.0014490498003750772, "biggest_recent_change": 0.2089223024634066 },
{"total_number_of_episodes":2280, "number_of_timesteps":2280000, "per_episode_reward":-708.01, "episode_reward_trend_value": -0.0018692028648299836, "biggest_recent_change": 0.2089223024634066 },
{"total_number_of_episodes":2290, "number_of_timesteps":2290000, "per_episode_reward":-707.81, "episode_reward_trend_value": 0.0001683036877958936, "biggest_recent_change": 0.2089223024634066 },
{"total_number_of_episodes":2300, "number_of_timesteps":2300000, "per_episode_reward":-708.04, "episode_reward_trend_value": -0.0024286464421493293, "biggest_recent_change": 0.23046594947243193 },
{"total_number_of_episodes":2310, "number_of_timesteps":2310000, "per_episode_reward":-707.68, "episode_reward_trend_value": 0.0011257931977651323, "biggest_recent_change": 0.356491847672487 },
{"total_number_of_episodes":2320, "number_of_timesteps":2320000, "per_episode_reward":-707.67, "episode_reward_trend_value": 0.0009538765576937496, "biggest_recent_change": 0.356491847672487 },
{"total_number_of_episodes":2330, "number_of_timesteps":2330000, "per_episode_reward":-707.71, "episode_reward_trend_value": 0.00043735329275402065, "biggest_recent_change": 0.356491847672487 },
{"total_number_of_episodes":2340, "number_of_timesteps":2340000, "per_episode_reward":-707.72, "episode_reward_trend_value": -0.0007566897640534383, "biggest_recent_change": 0.356491847672487 },
{"total_number_of_episodes":2350, "number_of_timesteps":2350000, "per_episode_reward":-707.97, "episode_reward_trend_value": -0.001181457839661309, "biggest_recent_change": 0.356491847672487 },
{"total_number_of_episodes":2360, "number_of_timesteps":2360000, "per_episode_reward":-708.06, "episode_reward_trend_value": -0.002537044161684005, "biggest_recent_change": 0.356491847672487 },
{"total_number_of_episodes":2370, "number_of_timesteps":2370000, "per_episode_reward":-707.99, "episode_reward_trend_value": 0.00024227459363525364, "biggest_recent_change": 0.356491847672487 },
{"total_number_of_episodes":2380, "number_of_timesteps":2380000, "per_episode_reward":-707.80, "episode_reward_trend_value": 8.125477763643276e-05, "biggest_recent_change": 0.356491847672487 },
{"total_number_of_episodes":2390, "number_of_timesteps":2390000, "per_episode_reward":-707.67, "episode_reward_trend_value": 0.0041480329158553614, "biggest_recent_change": 0.356491847672487 },
{"total_number_of_episodes":2400, "number_of_timesteps":2400000, "per_episode_reward":-707.96, "episode_reward_trend_value": -0.003065934305297762, "biggest_recent_change": 0.2927652022312941 },
{"total_number_of_episodes":2410, "number_of_timesteps":2410000, "per_episode_reward":-707.99, "episode_reward_trend_value": -0.0036341334440534306, "biggest_recent_change": 0.2927652022312941 },
{"total_number_of_episodes":2420, "number_of_timesteps":2420000, "per_episode_reward":-707.91, "episode_reward_trend_value": -0.002231741912189338, "biggest_recent_change": 0.2927652022312941 },
{"total_number_of_episodes":2430, "number_of_timesteps":2430000, "per_episode_reward":-707.85, "episode_reward_trend_value": -0.001399842139020772, "biggest_recent_change": 0.2927652022312941 },
{"total_number_of_episodes":2440, "number_of_timesteps":2440000, "per_episode_reward":-707.81, "episode_reward_trend_value": 0.001842872235363302, "biggest_recent_change": 0.2927652022312941 },
{"total_number_of_episodes":2450, "number_of_timesteps":2450000, "per_episode_reward":-707.65, "episode_reward_trend_value": 0.004567026970415907, "biggest_recent_change": 0.2927652022312941 },
{"total_number_of_episodes":2460, "number_of_timesteps":2460000, "per_episode_reward":-707.79, "episode_reward_trend_value": 0.0021377624577136583, "biggest_recent_change": 0.2927652022312941 },
{"total_number_of_episodes":2470, "number_of_timesteps":2470000, "per_episode_reward":-707.97, "episode_reward_trend_value": -0.001901251133391371, "biggest_recent_change": 0.2927652022312941 },
{"total_number_of_episodes":2480, "number_of_timesteps":2480000, "per_episode_reward":-707.74, "episode_reward_trend_value": -0.0007797001551706571, "biggest_recent_change": 0.2927652022312941 },
{"total_number_of_episodes":2490, "number_of_timesteps":2490000, "per_episode_reward":-707.83, "episode_reward_trend_value": 0.0014863200996804457, "biggest_recent_change": 0.23648367100713585 },
{"total_number_of_episodes":2500, "number_of_timesteps":2500000, "per_episode_reward":-707.46, "episode_reward_trend_value": 0.005945278872314298, "biggest_recent_change": 0.3666468346170859 },
{"total_number_of_episodes":2510, "number_of_timesteps":2510000, "per_episode_reward":-707.16, "episode_reward_trend_value": 0.00830320092115547, "biggest_recent_change": 0.3666468346170859 },
{"total_number_of_episodes":2520, "number_of_timesteps":2520000, "per_episode_reward":-707.41, "episode_reward_trend_value": 0.004833178231478996, "biggest_recent_change": 0.3666468346170859 },
{"total_number_of_episodes":2530, "number_of_timesteps":2530000, "per_episode_reward":-707.13, "episode_reward_trend_value": 0.007539357023319503, "biggest_recent_change": 0.3666468346170859 },
{"total_number_of_episodes":2540, "number_of_timesteps":2540000, "per_episode_reward":-707.10, "episode_reward_trend_value": 0.006085790310073157, "biggest_recent_change": 0.3666468346170859 },
{"total_number_of_episodes":2550, "number_of_timesteps":2550000, "per_episode_reward":-707.55, "episode_reward_trend_value": 0.0026503574856267024, "biggest_recent_change": 0.4527399702216144 },
{"total_number_of_episodes":2560, "number_of_timesteps":2560000, "per_episode_reward":-707.23, "episode_reward_trend_value": 0.008295238288437506, "biggest_recent_change": 0.4527399702216144 },
{"total_number_of_episodes":2570, "number_of_timesteps":2570000, "per_episode_reward":-706.95, "episode_reward_trend_value": 0.008708933578842941, "biggest_recent_change": 0.4527399702216144 },
{"total_number_of_episodes":2580, "number_of_timesteps":2580000, "per_episode_reward":-706.90, "episode_reward_trend_value": 0.010243858961449507, "biggest_recent_change": 0.4527399702216144 },
{"total_number_of_episodes":2590, "number_of_timesteps":2590000, "per_episode_reward":-706.49, "episode_reward_trend_value": 0.01080637399104363, "biggest_recent_change": 0.4527399702216144 },
{"total_number_of_episodes":2600, "number_of_timesteps":2600000, "per_episode_reward":-706.43, "episode_reward_trend_value": 0.008134407241600961, "biggest_recent_change": 0.4527399702216144 },
{"total_number_of_episodes":2610, "number_of_timesteps":2610000, "per_episode_reward":-706.43, "episode_reward_trend_value": 0.01089369112132393, "biggest_recent_change": 0.4527399702216144 },
{"total_number_of_episodes":2620, "number_of_timesteps":2620000, "per_episode_reward":-706.61, "episode_reward_trend_value": 0.0057720136894216815, "biggest_recent_change": 0.4527399702216144 },
{"total_number_of_episodes":2630, "number_of_timesteps":2630000, "per_episode_reward":-706.88, "episode_reward_trend_value": 0.0024165820114692197, "biggest_recent_change": 0.4527399702216144 },
{"total_number_of_episodes":2640, "number_of_timesteps":2640000, "per_episode_reward":-706.67, "episode_reward_trend_value": 0.009879793148216878, "biggest_recent_change": 0.41727318728055707 },
{"total_number_of_episodes":2650, "number_of_timesteps":2650000, "per_episode_reward":-706.60, "episode_reward_trend_value": 0.006913418156520695, "biggest_recent_change": 0.41727318728055707 },
{"total_number_of_episodes":2660, "number_of_timesteps":2660000, "per_episode_reward":-707.01, "episode_reward_trend_value": -0.0005938189700246867, "biggest_recent_change": 0.41727318728055707 },
{"total_number_of_episodes":2670, "number_of_timesteps":2670000, "per_episode_reward":-706.70, "episode_reward_trend_value": 0.002261103315216436, "biggest_recent_change": 0.41727318728055707 },
{"total_number_of_episodes":2680, "number_of_timesteps":2680000, "per_episode_reward":-707.23, "episode_reward_trend_value": -0.008225885370239262, "biggest_recent_change": 0.5265557944104557 },
{"total_number_of_episodes":2690, "number_of_timesteps":2690000, "per_episode_reward":-707.12, "episode_reward_trend_value": -0.0076296872079562195, "biggest_recent_change": 0.5265557944104557 },
{"total_number_of_episodes":2700, "number_of_timesteps":2700000, "per_episode_reward":-707.29, "episode_reward_trend_value": -0.00953140353944314, "biggest_recent_change": 0.5265557944104557 },
{"total_number_of_episodes":2710, "number_of_timesteps":2710000, "per_episode_reward":-707.36, "episode_reward_trend_value": -0.008315310321332465, "biggest_recent_change": 0.5265557944104557 },
{"total_number_of_episodes":2720, "number_of_timesteps":2720000, "per_episode_reward":-707.34, "episode_reward_trend_value": -0.005025382684889084, "biggest_recent_change": 0.5265557944104557 },
{"total_number_of_episodes":2730, "number_of_timesteps":2730000, "per_episode_reward":-707.27, "episode_reward_trend_value": -0.006698546994034081, "biggest_recent_change": 0.5265557944104557 },
{"total_number_of_episodes":2740, "number_of_timesteps":2740000, "per_episode_reward":-707.22, "episode_reward_trend_value": -0.006803860077103411, "biggest_recent_change": 0.5265557944104557 },
{"total_number_of_episodes":2750, "number_of_timesteps":2750000, "per_episode_reward":-707.07, "episode_reward_trend_value": -0.0006749852766195848, "biggest_recent_change": 0.5265557944104557 },
{"total_number_of_episodes":2760, "number_of_timesteps":2760000, "per_episode_reward":-707.04, "episode_reward_trend_value": -0.0037692222449145956, "biggest_recent_change": 0.5265557944104557 },
{"total_number_of_episodes":2770, "number_of_timesteps":2770000, "per_episode_reward":-707.15, "episode_reward_trend_value": 0.00086950992770451, "biggest_recent_change": 0.17412308139432753 },
{"total_number_of_episodes":2780, "number_of_timesteps":2780000, "per_episode_reward":-707.17, "episode_reward_trend_value": -0.0005730778795143326, "biggest_recent_change": 0.17412308139432753 },
{"total_number_of_episodes":2790, "number_of_timesteps":2790000, "per_episode_reward":-706.96, "episode_reward_trend_value": 0.0037284146595879973, "biggest_recent_change": 0.21301124712488217 },
{"total_number_of_episodes":2800, "number_of_timesteps":2800000, "per_episode_reward":-707.07, "episode_reward_trend_value": 0.0031309068186513083, "biggest_recent_change": 0.21301124712488217 },
{"total_number_of_episodes":2810, "number_of_timesteps":2810000, "per_episode_reward":-707.10, "episode_reward_trend_value": 0.0026157835142422907, "biggest_recent_change": 0.21301124712488217 },
{"total_number_of_episodes":2820, "number_of_timesteps":2820000, "per_episode_reward":-707.38, "episode_reward_trend_value": -0.001295849403249526, "biggest_recent_change": 0.2836827183116384 },
{"total_number_of_episodes":2830, "number_of_timesteps":2830000, "per_episode_reward":-707.10, "episode_reward_trend_value": 0.001327956179976455, "biggest_recent_change": 0.2872104671304214 },
{"total_number_of_episodes":2840, "number_of_timesteps":2840000, "per_episode_reward":-707.19, "episode_reward_trend_value": -0.001337119417468734, "biggest_recent_change": 0.2872104671304214 },
{"total_number_of_episodes":2850, "number_of_timesteps":2850000, "per_episode_reward":-707.14, "episode_reward_trend_value": -0.0011094846786628902, "biggest_recent_change": 0.2872104671304214 },
{"total_number_of_episodes":2860, "number_of_timesteps":2860000, "per_episode_reward":-706.97, "episode_reward_trend_value": 0.0019374445580815011, "biggest_recent_change": 0.2872104671304214 },
{"total_number_of_episodes":2870, "number_of_timesteps":2870000, "per_episode_reward":-706.92, "episode_reward_trend_value": 0.002825521568523628, "biggest_recent_change": 0.2872104671304214 },
{"total_number_of_episodes":2880, "number_of_timesteps":2880000, "per_episode_reward":-706.93, "episode_reward_trend_value": 0.00028915205625202765, "biggest_recent_change": 0.2872104671304214 },
{"total_number_of_episodes":2890, "number_of_timesteps":2890000, "per_episode_reward":-707.01, "episode_reward_trend_value": 0.0007438748516026155, "biggest_recent_change": 0.2872104671304214 },
{"total_number_of_episodes":2900, "number_of_timesteps":2900000, "per_episode_reward":-706.93, "episode_reward_trend_value": 0.0019514021211926372, "biggest_recent_change": 0.2872104671304214 },
{"total_number_of_episodes":2910, "number_of_timesteps":2910000, "per_episode_reward":-706.73, "episode_reward_trend_value": 0.007298986516569686, "biggest_recent_change": 0.2872104671304214 },
{"total_number_of_episodes":2920, "number_of_timesteps":2920000, "per_episode_reward":-706.74, "episode_reward_trend_value": 0.003954826611324178, "biggest_recent_change": 0.197599877272296 },
{"total_number_of_episodes":2930, "number_of_timesteps":2930000, "per_episode_reward":-706.74, "episode_reward_trend_value": 0.004935954318413375, "biggest_recent_change": 0.197599877272296 },
{"total_number_of_episodes":2940, "number_of_timesteps":2940000, "per_episode_reward":-706.69, "episode_reward_trend_value": 0.005010538667526513, "biggest_recent_change": 0.197599877272296 },
{"total_number_of_episodes":2950, "number_of_timesteps":2950000, "per_episode_reward":-706.72, "episode_reward_trend_value": 0.002843361057153339, "biggest_recent_change": 0.197599877272296 },
{"total_number_of_episodes":2960, "number_of_timesteps":2960000, "per_episode_reward":-706.52, "episode_reward_trend_value": 0.004353736632871611, "biggest_recent_change": 0.197599877272296 },
{"total_number_of_episodes":2970, "number_of_timesteps":2970000, "per_episode_reward":-706.43, "episode_reward_trend_value": 0.005539303158981409, "biggest_recent_change": 0.197599877272296 },
{"total_number_of_episodes":2980, "number_of_timesteps":2980000, "per_episode_reward":-706.41, "episode_reward_trend_value": 0.0065970050123660764, "biggest_recent_change": 0.197599877272296 },
{"total_number_of_episodes":2990, "number_of_timesteps":2990000, "per_episode_reward":-706.34, "episode_reward_trend_value": 0.00647847650759685, "biggest_recent_change": 0.197599877272296 },
{"total_number_of_episodes":3000, "number_of_timesteps":3000000, "per_episode_reward":-706.32, "episode_reward_trend_value": 0.0045777161762885045, "biggest_recent_change": 0.1949626519674439 },
{"total_number_of_episodes":3010, "number_of_timesteps":3010000, "per_episode_reward":-706.37, "episode_reward_trend_value": 0.004076223834797727, "biggest_recent_change": 0.1949626519674439 },
{"total_number_of_episodes":3020, "number_of_timesteps":3020000, "per_episode_reward":-706.33, "episode_reward_trend_value": 0.004638471380025698, "biggest_recent_change": 0.1949626519674439 },
{"total_number_of_episodes":3030, "number_of_timesteps":3030000, "per_episode_reward":-706.19, "episode_reward_trend_value": 0.0055156176199969, "biggest_recent_change": 0.1949626519674439 },
{"total_number_of_episodes":3040, "number_of_timesteps":3040000, "per_episode_reward":-706.29, "episode_reward_trend_value": 0.004745875908026341, "biggest_recent_change": 0.1949626519674439 },
{"total_number_of_episodes":3050, "number_of_timesteps":3050000, "per_episode_reward":-706.26, "episode_reward_trend_value": 0.0028989731557961325, "biggest_recent_change": 0.1339244631751626 },
{"total_number_of_episodes":3060, "number_of_timesteps":3060000, "per_episode_reward":-706.38, "episode_reward_trend_value": 0.0006064286100708058, "biggest_recent_change": 0.1339244631751626 },
{"total_number_of_episodes":3070, "number_of_timesteps":3070000, "per_episode_reward":-706.37, "episode_reward_trend_value": 0.00046552997531661177, "biggest_recent_change": 0.1339244631751626 },
{"total_number_of_episodes":3080, "number_of_timesteps":3080000, "per_episode_reward":-706.47, "episode_reward_trend_value": -0.0013966426922113189, "biggest_recent_change": 0.1339244631751626 },
{"total_number_of_episodes":3090, "number_of_timesteps":3090000, "per_episode_reward":-706.32, "episode_reward_trend_value": -2.2073092562398718e-05, "biggest_recent_change": 0.15024271142294765 },
{"total_number_of_episodes":3100, "number_of_timesteps":3100000, "per_episode_reward":-706.24, "episode_reward_trend_value": 0.0015455219368125958, "biggest_recent_change": 0.15024271142294765 },
{"total_number_of_episodes":3110, "number_of_timesteps":3110000, "per_episode_reward":-706.06, "episode_reward_trend_value": 0.0029672199911854277, "biggest_recent_change": 0.17666343163011788 },
{"total_number_of_episodes":3120, "number_of_timesteps":3120000, "per_episode_reward":-705.99, "episode_reward_trend_value": 0.00221419602555165, "biggest_recent_change": 0.17666343163011788 },
{"total_number_of_episodes":3130, "number_of_timesteps":3130000, "per_episode_reward":-705.82, "episode_reward_trend_value": 0.005205974937894098, "biggest_recent_change": 0.17666343163011788 },
{"total_number_of_episodes":3140, "number_of_timesteps":3140000, "per_episode_reward":-705.79, "episode_reward_trend_value": 0.005204794529371762, "biggest_recent_change": 0.17666343163011788 },
{"total_number_of_episodes":3150, "number_of_timesteps":3150000, "per_episode_reward":-705.71, "episode_reward_trend_value": 0.007388064941020882, "biggest_recent_change": 0.17666343163011788 },
{"total_number_of_episodes":3160, "number_of_timesteps":3160000, "per_episode_reward":-705.68, "episode_reward_trend_value": 0.0076759729746552485, "biggest_recent_change": 0.17666343163011788 },
{"total_number_of_episodes":3170, "number_of_timesteps":3170000, "per_episode_reward":-705.74, "episode_reward_trend_value": 0.008102241242705885, "biggest_recent_change": 0.17666343163011788 },
{"total_number_of_episodes":3180, "number_of_timesteps":3180000, "per_episode_reward":-705.78, "episode_reward_trend_value": 0.005997653706212228, "biggest_recent_change": 0.17666343163011788 },
{"total_number_of_episodes":3190, "number_of_timesteps":3190000, "per_episode_reward":-705.65, "episode_reward_trend_value": 0.006486675969160337, "biggest_recent_change": 0.17666343163011788 },
{"total_number_of_episodes":3200, "number_of_timesteps":3200000, "per_episode_reward":-705.77, "episode_reward_trend_value": 0.003227657641145419, "biggest_recent_change": 0.1700910955321433 },
{"total_number_of_episodes":3210, "number_of_timesteps":3210000, "per_episode_reward":-705.60, "episode_reward_trend_value": 0.004356575825441825, "biggest_recent_change": 0.1700910955321433 },
{"total_number_of_episodes":3220, "number_of_timesteps":3220000, "per_episode_reward":-705.61, "episode_reward_trend_value": 0.0023420343032259754, "biggest_recent_change": 0.16775494285479908 },
{"total_number_of_episodes":3230, "number_of_timesteps":3230000, "per_episode_reward":-705.72, "episode_reward_trend_value": 0.0008630544014017182, "biggest_recent_change": 0.16775494285479908 },
{"total_number_of_episodes":3240, "number_of_timesteps":3240000, "per_episode_reward":-705.74, "episode_reward_trend_value": -0.00032730112946081945, "biggest_recent_change": 0.16775494285479908 },
{"total_number_of_episodes":3250, "number_of_timesteps":3250000, "per_episode_reward":-705.69, "episode_reward_trend_value": -0.00014957219076702836, "biggest_recent_change": 0.16775494285479908 },
{"total_number_of_episodes":3260, "number_of_timesteps":3260000, "per_episode_reward":-705.70, "episode_reward_trend_value": 0.00048553358642998824, "biggest_recent_change": 0.16775494285479908 },
{"total_number_of_episodes":3270, "number_of_timesteps":3270000, "per_episode_reward":-705.70, "episode_reward_trend_value": 0.0008794387647210695, "biggest_recent_change": 0.16775494285479908 },
{"total_number_of_episodes":3280, "number_of_timesteps":3280000, "per_episode_reward":-705.74, "episode_reward_trend_value": -0.0009650615684929816, "biggest_recent_change": 0.16775494285479908 },
{"total_number_of_episodes":3290, "number_of_timesteps":3290000, "per_episode_reward":-705.90, "episode_reward_trend_value": -0.0014065464966506625, "biggest_recent_change": 0.16775494285479908 },
{"total_number_of_episodes":3300, "number_of_timesteps":3300000, "per_episode_reward":-705.95, "episode_reward_trend_value": -0.003835608333873905, "biggest_recent_change": 0.156381861425416 },
{"total_number_of_episodes":3310, "number_of_timesteps":3310000, "per_episode_reward":-705.91, "episode_reward_trend_value": -0.003346802009603936, "biggest_recent_change": 0.156381861425416 },
{"total_number_of_episodes":3320, "number_of_timesteps":3320000, "per_episode_reward":-705.78, "episode_reward_trend_value": -0.0007336588909235312, "biggest_recent_change": 0.156381861425416 },
{"total_number_of_episodes":3330, "number_of_timesteps":3330000, "per_episode_reward":-705.83, "episode_reward_trend_value": -0.001028888628786717, "biggest_recent_change": 0.156381861425416 },
{"total_number_of_episodes":3340, "number_of_timesteps":3340000, "per_episode_reward":-706.06, "episode_reward_trend_value": -0.004050698381747174, "biggest_recent_change": 0.22364753823205774 },
{"total_number_of_episodes":3350, "number_of_timesteps":3350000, "per_episode_reward":-706.03, "episode_reward_trend_value": -0.003736573573145405, "biggest_recent_change": 0.22364753823205774 },
{"total_number_of_episodes":3360, "number_of_timesteps":3360000, "per_episode_reward":-706.10, "episode_reward_trend_value": -0.0044226489150269626, "biggest_recent_change": 0.22364753823205774 },
{"total_number_of_episodes":3370, "number_of_timesteps":3370000, "per_episode_reward":-706.08, "episode_reward_trend_value": -0.0038100968460349173, "biggest_recent_change": 0.22364753823205774 },
{"total_number_of_episodes":3380, "number_of_timesteps":3380000, "per_episode_reward":-706.23, "episode_reward_trend_value": -0.003766934220023662, "biggest_recent_change": 0.22364753823205774 },
{"total_number_of_episodes":3390, "number_of_timesteps":3390000, "per_episode_reward":-706.18, "episode_reward_trend_value": -0.002553029226039093, "biggest_recent_change": 0.22364753823205774 },
{"total_number_of_episodes":3400, "number_of_timesteps":3400000, "per_episode_reward":-706.13, "episode_reward_trend_value": -0.002356580728053359, "biggest_recent_change": 0.22364753823205774 },
{"total_number_of_episodes":3410, "number_of_timesteps":3410000, "per_episode_reward":-706.28, "episode_reward_trend_value": -0.0055489866662671574, "biggest_recent_change": 0.22364753823205774 },
{"total_number_of_episodes":3420, "number_of_timesteps":3420000, "per_episode_reward":-706.17, "episode_reward_trend_value": -0.0037754259173640196, "biggest_recent_change": 0.22364753823205774 },
{"total_number_of_episodes":3430, "number_of_timesteps":3430000, "per_episode_reward":-706.06, "episode_reward_trend_value": -6.220680550465355e-05, "biggest_recent_change": 0.1566066774224737 },
{"total_number_of_episodes":3440, "number_of_timesteps":3440000, "per_episode_reward":-706.09, "episode_reward_trend_value": -0.000621613776757436, "biggest_recent_change": 0.1566066774224737 },
{"total_number_of_episodes":3450, "number_of_timesteps":3450000, "per_episode_reward":-706.23, "episode_reward_trend_value": -0.0014266785974895255, "biggest_recent_change": 0.1566066774224737 },
{"total_number_of_episodes":3460, "number_of_timesteps":3460000, "per_episode_reward":-706.21, "episode_reward_trend_value": -0.0013786910753386414, "biggest_recent_change": 0.1566066774224737 },
{"total_number_of_episodes":3470, "number_of_timesteps":3470000, "per_episode_reward":-706.07, "episode_reward_trend_value": 0.0018234081971665243, "biggest_recent_change": 0.1566066774224737 },
{"total_number_of_episodes":3480, "number_of_timesteps":3480000, "per_episode_reward":-706.10, "episode_reward_trend_value": 0.0008142986267216656, "biggest_recent_change": 0.1566066774224737 },
{"total_number_of_episodes":3490, "number_of_timesteps":3490000, "per_episode_reward":-706.16, "episode_reward_trend_value": -0.0004316596752333781, "biggest_recent_change": 0.1566066774224737 },
{"total_number_of_episodes":3500, "number_of_timesteps":3500000, "per_episode_reward":-706.28, "episode_reward_trend_value": 3.8615204754124634e-05, "biggest_recent_change": 0.13792131545051234 },
{"total_number_of_episodes":3510, "number_of_timesteps":3510000, "per_episode_reward":-706.19, "episode_reward_trend_value": -0.0001798173280715471, "biggest_recent_change": 0.13792131545051234 },
{"total_number_of_episodes":3520, "number_of_timesteps":3520000, "per_episode_reward":-706.20, "episode_reward_trend_value": -0.001540448955323528, "biggest_recent_change": 0.13792131545051234 },
{"total_number_of_episodes":3530, "number_of_timesteps":3530000, "per_episode_reward":-706.19, "episode_reward_trend_value": -0.001083998539113256, "biggest_recent_change": 0.13792131545051234 },
{"total_number_of_episodes":3540, "number_of_timesteps":3540000, "per_episode_reward":-706.15, "episode_reward_trend_value": 0.0008469549293977253, "biggest_recent_change": 0.1356917094410619 },
{"total_number_of_episodes":3550, "number_of_timesteps":3550000, "per_episode_reward":-706.06, "episode_reward_trend_value": 0.001640240365689907, "biggest_recent_change": 0.1356917094410619 },
{"total_number_of_episodes":3560, "number_of_timesteps":3560000, "per_episode_reward":-705.99, "episode_reward_trend_value": 0.000840908735098209, "biggest_recent_change": 0.11428193822359844 },
{"total_number_of_episodes":3570, "number_of_timesteps":3570000, "per_episode_reward":-706.23, "episode_reward_trend_value": -0.0013887994710178948, "biggest_recent_change": 0.2331027729271682 },
{"total_number_of_episodes":3580, "number_of_timesteps":3580000, "per_episode_reward":-706.11, "episode_reward_trend_value": 0.000549257789250785, "biggest_recent_change": 0.2331027729271682 },
{"total_number_of_episodes":3590, "number_of_timesteps":3590000, "per_episode_reward":-706.27, "episode_reward_trend_value": 0.00010129501351785721, "biggest_recent_change": 0.2331027729271682 },
{"total_number_of_episodes":3600, "number_of_timesteps":3600000, "per_episode_reward":-706.06, "episode_reward_trend_value": 0.0014806410701503916, "biggest_recent_change": 0.2331027729271682 },
{"total_number_of_episodes":3610, "number_of_timesteps":3610000, "per_episode_reward":-706.13, "episode_reward_trend_value": 0.000812572776197208, "biggest_recent_change": 0.2331027729271682 },
{"total_number_of_episodes":3620, "number_of_timesteps":3620000, "per_episode_reward":-705.95, "episode_reward_trend_value": 0.0025858457860067573, "biggest_recent_change": 0.2331027729271682 },
{"total_number_of_episodes":3630, "number_of_timesteps":3630000, "per_episode_reward":-705.94, "episode_reward_trend_value": 0.0023596146091196916, "biggest_recent_change": 0.2331027729271682 },
{"total_number_of_episodes":3640, "number_of_timesteps":3640000, "per_episode_reward":-705.81, "episode_reward_trend_value": 0.0027974113117289158, "biggest_recent_change": 0.2331027729271682 },
{"total_number_of_episodes":3650, "number_of_timesteps":3650000, "per_episode_reward":-705.97, "episode_reward_trend_value": 0.00027951869420677315, "biggest_recent_change": 0.2331027729271682 },
{"total_number_of_episodes":3660, "number_of_timesteps":3660000, "per_episode_reward":-705.90, "episode_reward_trend_value": 0.0036051555816432788, "biggest_recent_change": 0.21200431666204622 },
{"total_number_of_episodes":3670, "number_of_timesteps":3670000, "per_episode_reward":-705.94, "episode_reward_trend_value": 0.0019695406723801373, "biggest_recent_change": 0.21200431666204622 },
{"total_number_of_episodes":3680, "number_of_timesteps":3680000, "per_episode_reward":-705.94, "episode_reward_trend_value": 0.003649759089748588, "biggest_recent_change": 0.21200431666204622 },
{"total_number_of_episodes":3690, "number_of_timesteps":3690000, "per_episode_reward":-706.00, "episode_reward_trend_value": 0.0006465734745587624, "biggest_recent_change": 0.17706687421809875 },
{"total_number_of_episodes":3700, "number_of_timesteps":3700000, "per_episode_reward":-705.96, "episode_reward_trend_value": 0.0019192026844797913, "biggest_recent_change": 0.17706687421809875 },
{"total_number_of_episodes":3710, "number_of_timesteps":3710000, "per_episode_reward":-705.89, "episode_reward_trend_value": 0.0007404926642897812, "biggest_recent_change": 0.16285847288918376 },
{"total_number_of_episodes":3720, "number_of_timesteps":3720000, "per_episode_reward":-705.98, "episode_reward_trend_value": -0.0004818228375875656, "biggest_recent_change": 0.16285847288918376 },
{"total_number_of_episodes":3730, "number_of_timesteps":3730000, "per_episode_reward":-705.94, "episode_reward_trend_value": -0.001536745563640175, "biggest_recent_change": 0.16285847288918376 },
{"total_number_of_episodes":3740, "number_of_timesteps":3740000, "per_episode_reward":-705.84, "episode_reward_trend_value": 0.0014402772567387325, "biggest_recent_change": 0.10507358094491792 },
{"total_number_of_episodes":3750, "number_of_timesteps":3750000, "per_episode_reward":-705.94, "episode_reward_trend_value": -0.0003628295225793307, "biggest_recent_change": 0.10507358094491792 },
{"total_number_of_episodes":3760, "number_of_timesteps":3760000, "per_episode_reward":-706.10, "episode_reward_trend_value": -0.0018345964158937831, "biggest_recent_change": 0.16692016344802596 },
{"total_number_of_episodes":3770, "number_of_timesteps":3770000, "per_episode_reward":-705.94, "episode_reward_trend_value": 5.228792820288516e-05, "biggest_recent_change": 0.16692016344802596 },
{"total_number_of_episodes":3780, "number_of_timesteps":3780000, "per_episode_reward":-706.08, "episode_reward_trend_value": -0.0008854792307829484, "biggest_recent_change": 0.16692016344802596 },
{"total_number_of_episodes":3790, "number_of_timesteps":3790000, "per_episode_reward":-706.29, "episode_reward_trend_value": -0.0037442859919956896, "biggest_recent_change": 0.2147967906894337 },
{"total_number_of_episodes":3800, "number_of_timesteps":3800000, "per_episode_reward":-706.23, "episode_reward_trend_value": -0.0038449319294373814, "biggest_recent_change": 0.2147967906894337 },
{"total_number_of_episodes":3810, "number_of_timesteps":3810000, "per_episode_reward":-706.15, "episode_reward_trend_value": -0.0019312796731911577, "biggest_recent_change": 0.2147967906894337 },
{"total_number_of_episodes":3820, "number_of_timesteps":3820000, "per_episode_reward":-706.12, "episode_reward_trend_value": -0.001911939807609997, "biggest_recent_change": 0.2147967906894337 },
{"total_number_of_episodes":3830, "number_of_timesteps":3830000, "per_episode_reward":-706.27, "episode_reward_trend_value": -0.004812641940831478, "biggest_recent_change": 0.2147967906894337 },
{"total_number_of_episodes":3840, "number_of_timesteps":3840000, "per_episode_reward":-706.42, "episode_reward_trend_value": -0.005393895751727643, "biggest_recent_change": 0.2147967906894337 },
{"total_number_of_episodes":3850, "number_of_timesteps":3850000, "per_episode_reward":-706.37, "episode_reward_trend_value": -0.0029324671864009765, "biggest_recent_change": 0.2147967906894337 },
{"total_number_of_episodes":3860, "number_of_timesteps":3860000, "per_episode_reward":-706.15, "episode_reward_trend_value": -0.002339932013761528, "biggest_recent_change": 0.21976882602984915 },
{"total_number_of_episodes":3870, "number_of_timesteps":3870000, "per_episode_reward":-706.05, "episode_reward_trend_value": 0.00034014752403916445, "biggest_recent_change": 0.21976882602984915 },
{"total_number_of_episodes":3880, "number_of_timesteps":3880000, "per_episode_reward":-706.22, "episode_reward_trend_value": 0.0008489685602815674, "biggest_recent_change": 0.21976882602984915 },
{"total_number_of_episodes":3890, "number_of_timesteps":3890000, "per_episode_reward":-706.33, "episode_reward_trend_value": -0.0011401138468828422, "biggest_recent_change": 0.21976882602984915 },
{"total_number_of_episodes":3900, "number_of_timesteps":3900000, "per_episode_reward":-706.11, "episode_reward_trend_value": 0.0005273805731728014, "biggest_recent_change": 0.22779849649384687 },
{"total_number_of_episodes":3910, "number_of_timesteps":3910000, "per_episode_reward":-706.20, "episode_reward_trend_value": -0.0008864011065308356, "biggest_recent_change": 0.22779849649384687 },
{"total_number_of_episodes":3920, "number_of_timesteps":3920000, "per_episode_reward":-706.38, "episode_reward_trend_value": -0.00113954319627712, "biggest_recent_change": 0.22779849649384687 },
{"total_number_of_episodes":3930, "number_of_timesteps":3930000, "per_episode_reward":-706.31, "episode_reward_trend_value": 0.0012146752661540935, "biggest_recent_change": 0.22779849649384687 },
{"total_number_of_episodes":3940, "number_of_timesteps":3940000, "per_episode_reward":-706.11, "episode_reward_trend_value": 0.0028901943977264937, "biggest_recent_change": 0.22779849649384687 },
{"total_number_of_episodes":3950, "number_of_timesteps":3950000, "per_episode_reward":-705.97, "episode_reward_trend_value": 0.001928783671393022, "biggest_recent_change": 0.22779849649384687 },
{"total_number_of_episodes":3960, "number_of_timesteps":3960000, "per_episode_reward":-706.12, "episode_reward_trend_value": -0.0007793437299457967, "biggest_recent_change": 0.22779849649384687 },
{"total_number_of_episodes":3970, "number_of_timesteps":3970000, "per_episode_reward":-705.92, "episode_reward_trend_value": 0.0033477389712579173, "biggest_recent_change": 0.22779849649384687 },
{"total_number_of_episodes":3980, "number_of_timesteps":3980000, "per_episode_reward":-705.91, "episode_reward_trend_value": 0.0047212695458281615, "biggest_recent_change": 0.22779849649384687 },
{"total_number_of_episodes":3990, "number_of_timesteps":3990000, "per_episode_reward":-705.99, "episode_reward_trend_value": 0.0013494842609917315, "biggest_recent_change": 0.20540512927289 },
{"total_number_of_episodes":4000, "number_of_timesteps":4000000, "per_episode_reward":-705.92, "episode_reward_trend_value": 0.0030697239262293704, "biggest_recent_change": 0.20540512927289 },
{"total_number_of_episodes":4010, "number_of_timesteps":4010000, "per_episode_reward":-705.94, "episode_reward_trend_value": 0.004871235595181942, "biggest_recent_change": 0.20540512927289 },
{"total_number_of_episodes":4020, "number_of_timesteps":4020000, "per_episode_reward":-705.91, "episode_reward_trend_value": 0.004494326283996644, "biggest_recent_change": 0.20540512927289 },
{"total_number_of_episodes":4030, "number_of_timesteps":4030000, "per_episode_reward":-705.86, "episode_reward_trend_value": 0.0027479328412103616, "biggest_recent_change": 0.20243454568071684 },
{"total_number_of_episodes":4040, "number_of_timesteps":4040000, "per_episode_reward":-705.95, "episode_reward_trend_value": 0.0002308909618275518, "biggest_recent_change": 0.20243454568071684 },
{"total_number_of_episodes":4050, "number_of_timesteps":4050000, "per_episode_reward":-706.11, "episode_reward_trend_value": 0.0001443247549850134, "biggest_recent_change": 0.20243454568071684 },
{"total_number_of_episodes":4060, "number_of_timesteps":4060000, "per_episode_reward":-705.98, "episode_reward_trend_value": -0.0006565313821207989, "biggest_recent_change": 0.15299669934802296 },
{"total_number_of_episodes":4070, "number_of_timesteps":4070000, "per_episode_reward":-705.86, "episode_reward_trend_value": 0.0005922153291989061, "biggest_recent_change": 0.15299669934802296 },
{"total_number_of_episodes":4080, "number_of_timesteps":4080000, "per_episode_reward":-706.00, "episode_reward_trend_value": -0.00012805670887574807, "biggest_recent_change": 0.15299669934802296 },
{"total_number_of_episodes":4090, "number_of_timesteps":4090000, "per_episode_reward":-705.99, "episode_reward_trend_value": -0.0007911812532888184, "biggest_recent_change": 0.15299669934802296 },
{"total_number_of_episodes":4100, "number_of_timesteps":4100000, "per_episode_reward":-706.09, "episode_reward_trend_value": -0.0017554211774634294, "biggest_recent_change": 0.15299669934802296 },
{"total_number_of_episodes":4110, "number_of_timesteps":4110000, "per_episode_reward":-706.22, "episode_reward_trend_value": -0.0034782805741365845, "biggest_recent_change": 0.15299669934802296 },
{"total_number_of_episodes":4120, "number_of_timesteps":4120000, "per_episode_reward":-706.43, "episode_reward_trend_value": -0.006333477265075595, "biggest_recent_change": 0.2087379827623863 },
{"total_number_of_episodes":4130, "number_of_timesteps":4130000, "per_episode_reward":-706.19, "episode_reward_trend_value": -0.002627301061544232, "biggest_recent_change": 0.24026394983320642 },
{"total_number_of_episodes":4140, "number_of_timesteps":4140000, "per_episode_reward":-706.47, "episode_reward_trend_value": -0.00406158521087213, "biggest_recent_change": 0.28208227278753384 },
{"total_number_of_episodes":4150, "number_of_timesteps":4150000, "per_episode_reward":-706.25, "episode_reward_trend_value": -0.0030698018700213146, "biggest_recent_change": 0.28208227278753384 },
{"total_number_of_episodes":4160, "number_of_timesteps":4160000, "per_episode_reward":-706.40, "episode_reward_trend_value": -0.005996003249884375, "biggest_recent_change": 0.28208227278753384 },
{"total_number_of_episodes":4170, "number_of_timesteps":4170000, "per_episode_reward":-706.23, "episode_reward_trend_value": -0.002602878006367367, "biggest_recent_change": 0.28208227278753384 },








Process Process-208:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-202:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":4180, "number_of_timesteps":4180000, "per_episode_reward":-706.28, "episode_reward_trend_value": -0.003196152286382888, "biggest_recent_change": 0.28208227278753384 },
{"total_number_of_episodes":4190, "number_of_timesteps":4190000, "per_episode_reward":-706.10, "episode_reward_trend_value": -7.786294753234162e-05, "biggest_recent_change": 0.28208227278753384 },
{"total_number_of_episodes":4200, "number_of_timesteps":4200000, "per_episode_reward":-706.23, "episode_reward_trend_value": -0.00015517750242300584, "biggest_recent_change": 0.28208227278753384 },
{"total_number_of_episodes":4210, "number_of_timesteps":4210000, "per_episode_reward":-706.49, "episode_reward_trend_value": -0.0007008818035337249, "biggest_recent_change": 0.28208227278753384 },
{"total_number_of_episodes":4220, "number_of_timesteps":4220000, "per_episode_reward":-706.33, "episode_reward_trend_value": -0.0015149930838245181, "biggest_recent_change": 0.28208227278753384 },
{"total_number_of_episodes":4230, "number_of_timesteps":4230000, "per_episode_reward":-706.22, "episode_reward_trend_value": 0.0027686634971941103, "biggest_recent_change": 0.257851369862351 },
{"total_number_of_episodes":4240, "number_of_timesteps":4240000, "per_episode_reward":-706.28, "episode_reward_trend_value": -0.0003369098569300554, "biggest_recent_change": 0.257851369862351 },
{"total_number_of_episodes":4250, "number_of_timesteps":4250000, "per_episode_reward":-706.35, "episode_reward_trend_value": 0.0004577572054308298, "biggest_recent_change": 0.257851369862351 },
{"total_number_of_episodes":4260, "number_of_timesteps":4260000, "per_episode_reward":-706.30, "episode_reward_trend_value": -0.0007191116304284151, "biggest_recent_change": 0.257851369862351 },
{"total_number_of_episodes":4270, "number_of_timesteps":4270000, "per_episode_reward":-706.19, "episode_reward_trend_value": 0.0009935826997624039, "biggest_recent_change": 0.257851369862351 },
{"total_number_of_episodes":4280, "number_of_timesteps":4280000, "per_episode_reward":-706.23, "episode_reward_trend_value": -0.0013692054113436016, "biggest_recent_change": 0.257851369862351 },
{"total_number_of_episodes":4290, "number_of_timesteps":4290000, "per_episode_reward":-706.25, "episode_reward_trend_value": -0.00020175053473899928, "biggest_recent_change": 0.257851369862351 },
{"total_number_of_episodes":4300, "number_of_timesteps":4300000, "per_episode_reward":-706.33, "episode_reward_trend_value": 0.0018343743215837095, "biggest_recent_change": 0.16699393460703504 },
{"total_number_of_episodes":4310, "number_of_timesteps":4310000, "per_episode_reward":-706.28, "episode_reward_trend_value": 0.00046218544711109646, "biggest_recent_change": 0.10588360372219086 },
{"total_number_of_episodes":4320, "number_of_timesteps":4320000, "per_episode_reward":-706.27, "episode_reward_trend_value": -0.0005844962189093571, "biggest_recent_change": 0.10588360372219086 },
{"total_number_of_episodes":4330, "number_of_timesteps":4330000, "per_episode_reward":-706.37, "episode_reward_trend_value": -0.0009479422040852923, "biggest_recent_change": 0.10588360372219086 },
{"total_number_of_episodes":4340, "number_of_timesteps":4340000, "per_episode_reward":-706.43, "episode_reward_trend_value": -0.0008644745794312156, "biggest_recent_change": 0.10588360372219086 },
{"total_number_of_episodes":4350, "number_of_timesteps":4350000, "per_episode_reward":-706.27, "episode_reward_trend_value": 0.00028233590444036357, "biggest_recent_change": 0.16218935766949016 },
{"total_number_of_episodes":4360, "number_of_timesteps":4360000, "per_episode_reward":-706.20, "episode_reward_trend_value": -0.00012310352051119684, "biggest_recent_change": 0.16218935766949016 },
{"total_number_of_episodes":4370, "number_of_timesteps":4370000, "per_episode_reward":-706.35, "episode_reward_trend_value": -0.0013440241849694858, "biggest_recent_change": 0.16218935766949016 },
{"total_number_of_episodes":4380, "number_of_timesteps":4380000, "per_episode_reward":-706.25, "episode_reward_trend_value": 6.896201041399763e-05, "biggest_recent_change": 0.16218935766949016 },
{"total_number_of_episodes":4390, "number_of_timesteps":4390000, "per_episode_reward":-706.40, "episode_reward_trend_value": -0.000846085239600648, "biggest_recent_change": 0.16218935766949016 },
{"total_number_of_episodes":4400, "number_of_timesteps":4400000, "per_episode_reward":-706.27, "episode_reward_trend_value": 0.00016451943241438334, "biggest_recent_change": 0.16218935766949016 },
{"total_number_of_episodes":4410, "number_of_timesteps":4410000, "per_episode_reward":-706.31, "episode_reward_trend_value": -0.00038055760203153314, "biggest_recent_change": 0.16218935766949016 },
{"total_number_of_episodes":4420, "number_of_timesteps":4420000, "per_episode_reward":-706.28, "episode_reward_trend_value": 0.0009761510168914356, "biggest_recent_change": 0.16218935766949016 },
{"total_number_of_episodes":4430, "number_of_timesteps":4430000, "per_episode_reward":-706.36, "episode_reward_trend_value": 0.0008024841519588537, "biggest_recent_change": 0.16218935766949016 },
{"total_number_of_episodes":4440, "number_of_timesteps":4440000, "per_episode_reward":-706.45, "episode_reward_trend_value": -0.0020447849719453697, "biggest_recent_change": 0.15695438529462535 },
{"total_number_of_episodes":4450, "number_of_timesteps":4450000, "per_episode_reward":-706.49, "episode_reward_trend_value": -0.0032071457495034235, "biggest_recent_change": 0.15695438529462535 },
{"total_number_of_episodes":4460, "number_of_timesteps":4460000, "per_episode_reward":-706.43, "episode_reward_trend_value": -0.0009379382920555044, "biggest_recent_change": 0.15695438529462535 },
{"total_number_of_episodes":4470, "number_of_timesteps":4470000, "per_episode_reward":-706.38, "episode_reward_trend_value": -0.0014319551149987496, "biggest_recent_change": 0.15695438529462535 },
{"total_number_of_episodes":4480, "number_of_timesteps":4480000, "per_episode_reward":-706.42, "episode_reward_trend_value": -0.0002323122216555223, "biggest_recent_change": 0.1344513563858527 },
{"total_number_of_episodes":4490, "number_of_timesteps":4490000, "per_episode_reward":-706.54, "episode_reward_trend_value": -0.0030010322973112124, "biggest_recent_change": 0.11473345042315941 },
{"total_number_of_episodes":4500, "number_of_timesteps":4500000, "per_episode_reward":-706.52, "episode_reward_trend_value": -0.002345687383117618, "biggest_recent_change": 0.11473345042315941 },
{"total_number_of_episodes":4510, "number_of_timesteps":4510000, "per_episode_reward":-706.56, "episode_reward_trend_value": -0.0031133252299481583, "biggest_recent_change": 0.11473345042315941 },
{"total_number_of_episodes":4520, "number_of_timesteps":4520000, "per_episode_reward":-706.48, "episode_reward_trend_value": -0.0013370003603654368, "biggest_recent_change": 0.11473345042315941 },
{"total_number_of_episodes":4530, "number_of_timesteps":4530000, "per_episode_reward":-706.54, "episode_reward_trend_value": -0.0009222796840466193, "biggest_recent_change": 0.11473345042315941 },
{"total_number_of_episodes":4540, "number_of_timesteps":4540000, "per_episode_reward":-706.44, "episode_reward_trend_value": 0.0005219063250099982, "biggest_recent_change": 0.11473345042315941 },
{"total_number_of_episodes":4550, "number_of_timesteps":4550000, "per_episode_reward":-706.46, "episode_reward_trend_value": -0.00036207294592713877, "biggest_recent_change": 0.11473345042315941 },
{"total_number_of_episodes":4560, "number_of_timesteps":4560000, "per_episode_reward":-706.55, "episode_reward_trend_value": -0.0019791506049866964, "biggest_recent_change": 0.11473345042315941 },
{"total_number_of_episodes":4570, "number_of_timesteps":4570000, "per_episode_reward":-706.58, "episode_reward_trend_value": -0.0016768874770112536, "biggest_recent_change": 0.11473345042315941 },
Hit early stopping because biggest_recent_change: 0.09475832631142111 < 0.1
{"total_number_of_episodes":4580, "number_of_timesteps":4580000, "per_episode_reward":-706.55, "episode_reward_trend_value": -0.00012101859085128631, "biggest_recent_change": 0.09475832631142111 },
[32m[I 2022-10-23 17:24:43,355][0m Trial 20 finished with value: -706.5661819546524 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 2, 'permaban_threshold': 0}. Best is trial 20 with value: -706.5661819546524.[0m








Process Process-216:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-220:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-214:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-215:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-213:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-212:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-790.86, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-781.69, "episode_reward_trend_value": 0.9172218128169334, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-786.20, "episode_reward_trend_value": 0.23275258863803855, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-794.48, "episode_reward_trend_value": -0.12054847029565582, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-799.02, "episode_reward_trend_value": -0.20396436866268744, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-808.02, "episode_reward_trend_value": -0.34326228124386493, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-809.11, "episode_reward_trend_value": -0.30417856567708934, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-805.38, "episode_reward_trend_value": -0.20743339951550296, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-802.66, "episode_reward_trend_value": -0.14745146711147186, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-805.94, "episode_reward_trend_value": -0.1675981175579472, "biggest_recent_change": 9.172218128169334 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-804.87, "episode_reward_trend_value": -0.25755989791869194, "biggest_recent_change": 9.004539315685747 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-805.60, "episode_reward_trend_value": -0.21549824775164045, "biggest_recent_change": 9.004539315685747 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-803.81, "episode_reward_trend_value": -0.10374468157898364, "biggest_recent_change": 9.004539315685747 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-802.75, "episode_reward_trend_value": -0.041488878441191444, "biggest_recent_change": 9.004539315685747 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-805.39, "episode_reward_trend_value": 0.02920225744487779, "biggest_recent_change": 3.730375974540152 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-807.55, "episode_reward_trend_value": 0.017387296968638565, "biggest_recent_change": 3.730375974540152 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-803.51, "episode_reward_trend_value": 0.020799133901527968, "biggest_recent_change": 4.037441298500198 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-804.23, "episode_reward_trend_value": -0.017544409883937028, "biggest_recent_change": 4.037441298500198 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-805.23, "episode_reward_trend_value": 0.00789952717605047, "biggest_recent_change": 4.037441298500198 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-803.43, "episode_reward_trend_value": 0.015994453612084826, "biggest_recent_change": 4.037441298500198 },
Hit early stopping because per_episode_reward: -803.4284386523861 < -800
[32m[I 2022-10-23 17:25:04,567][0m Trial 21 finished with value: -802.7761305259194 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 2, 'permaban_threshold': 0}. Best is trial 20 with value: -706.5661819546524.[0m




Process Process-226:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-224:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-230:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-223:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-784.38, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-788.33, "episode_reward_trend_value": -0.3957212083708669, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-796.66, "episode_reward_trend_value": -0.6142824628149584, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-795.98, "episode_reward_trend_value": -0.3868467897858636, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-799.85, "episode_reward_trend_value": -0.3868475561218787, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-805.59, "episode_reward_trend_value": -0.4243602966873278, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-809.03, "episode_reward_trend_value": -0.4108242719652101, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-808.84, "episode_reward_trend_value": -0.34947712686053883, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-805.81, "episode_reward_trend_value": -0.2679622055364604, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-809.88, "episode_reward_trend_value": -0.28334239956505425, "biggest_recent_change": 8.328437172590498 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-803.30, "episode_reward_trend_value": -0.16624980115753943, "biggest_recent_change": 8.328437172590498 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-800.72, "episode_reward_trend_value": -0.04508097675208344, "biggest_recent_change": 6.581121772967663 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-796.70, "episode_reward_trend_value": -0.007940387795307894, "biggest_recent_change": 6.581121772967663 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-797.40, "episode_reward_trend_value": 0.02716977795147386, "biggest_recent_change": 6.581121772967663 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-796.03, "episode_reward_trend_value": 0.10621948718488232, "biggest_recent_change": 6.581121772967663 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-798.07, "episode_reward_trend_value": 0.12172494680926826, "biggest_recent_change": 6.581121772967663 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-794.22, "episode_reward_trend_value": 0.16246397336354776, "biggest_recent_change": 6.581121772967663 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-789.61, "episode_reward_trend_value": 0.18001931586465061, "biggest_recent_change": 6.581121772967663 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-792.51, "episode_reward_trend_value": 0.19296119737206885, "biggest_recent_change": 6.581121772967663 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-794.44, "episode_reward_trend_value": 0.09838704247376881, "biggest_recent_change": 4.6064032624201445 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-795.72, "episode_reward_trend_value": 0.05557720181665723, "biggest_recent_change": 4.6064032624201445 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-795.84, "episode_reward_trend_value": 0.009519676669616376, "biggest_recent_change": 4.6064032624201445 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-798.92, "episode_reward_trend_value": -0.016883518540915552, "biggest_recent_change": 4.6064032624201445 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-800.29, "episode_reward_trend_value": -0.047284528590125774, "biggest_recent_change": 4.6064032624201445 },
Hit early stopping because per_episode_reward: -800.2898037692943 < -800




Process Process-228:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-221:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:25:29,519][0m Trial 22 finished with value: -799.7502855062887 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 1, 'permaban_threshold': 1}. Best is trial 20 with value: -706.5661819546524.[0m








Process Process-235:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-232:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-236:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-231:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-804.71, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-789.62, "episode_reward_trend_value": 1.509422703207872, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-806.80, "episode_reward_trend_value": -0.10451396878060563, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-794.38, "episode_reward_trend_value": 0.3444858034383932, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-807.93, "episode_reward_trend_value": -0.08057475645996134, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-808.33, "episode_reward_trend_value": -0.07236218305570219, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-803.76, "episode_reward_trend_value": 0.015894415362093163, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-804.93, "episode_reward_trend_value": -0.0031597726273113426, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-800.23, "episode_reward_trend_value": 0.05600876369155543, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-794.49, "episode_reward_trend_value": 0.11360534889444882, "biggest_recent_change": 17.184506407690833 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-794.92, "episode_reward_trend_value": -0.058895196788776305, "biggest_recent_change": 17.184506407690833 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-796.75, "episode_reward_trend_value": 0.11170978252330314, "biggest_recent_change": 13.557564361550249 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-795.31, "episode_reward_trend_value": -0.01033999634385307, "biggest_recent_change": 13.557564361550249 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-793.97, "episode_reward_trend_value": 0.15520826136658772, "biggest_recent_change": 5.74378030517596 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-796.60, "episode_reward_trend_value": 0.1303728984511824, "biggest_recent_change": 5.74378030517596 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-797.58, "episode_reward_trend_value": 0.06869526155266056, "biggest_recent_change": 5.74378030517596 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-799.07, "episode_reward_trend_value": 0.06518645493520252, "biggest_recent_change": 5.74378030517596 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-796.64, "episode_reward_trend_value": 0.039893253105434366, "biggest_recent_change": 5.74378030517596 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-798.17, "episode_reward_trend_value": -0.04093431160802841, "biggest_recent_change": 2.6303015567731336 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-799.63, "episode_reward_trend_value": -0.05239809427757412, "biggest_recent_change": 2.6303015567731336 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-798.60, "episode_reward_trend_value": -0.020596207887983078, "biggest_recent_change": 2.6303015567731336 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-799.99, "episode_reward_trend_value": -0.052040841099638635, "biggest_recent_change": 2.6303015567731336 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-800.61, "episode_reward_trend_value": -0.07385519816432028, "biggest_recent_change": 2.6303015567731336 },
Hit early stopping because per_episode_reward: -800.6128972008644 < -800
[32m[I 2022-10-23 17:25:53,572][0m Trial 23 finished with value: -800.1274870528013 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 2, 'permaban_threshold': 0}. Best is trial 20 with value: -706.5661819546524.[0m








Process Process-242:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-245:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-243:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-248:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-783.31, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-789.90, "episode_reward_trend_value": -0.6586833948231743, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-789.39, "episode_reward_trend_value": -0.30405661888671603, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-793.44, "episode_reward_trend_value": -0.33760422479354396, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-793.03, "episode_reward_trend_value": -0.24286176392617734, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-788.96, "episode_reward_trend_value": -0.11292090409260028, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-796.55, "episode_reward_trend_value": -0.22061862723268746, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-794.61, "episode_reward_trend_value": -0.16137120562430027, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-802.67, "episode_reward_trend_value": -0.24197674414452877, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-804.80, "episode_reward_trend_value": -0.23874626633561066, "biggest_recent_change": 8.062155137861282 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-803.89, "episode_reward_trend_value": -0.15541918088723225, "biggest_recent_change": 8.062155137861282 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-800.60, "episode_reward_trend_value": -0.12450140607588511, "biggest_recent_change": 8.062155137861282 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-800.66, "episode_reward_trend_value": -0.08019015130292549, "biggest_recent_change": 8.062155137861282 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-795.44, "episode_reward_trend_value": -0.02678831705571333, "biggest_recent_change": 8.062155137861282 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-793.29, "episode_reward_trend_value": -0.048147624107467535, "biggest_recent_change": 8.062155137861282 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-791.82, "episode_reward_trend_value": 0.05258866222326206, "biggest_recent_change": 8.062155137861282 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-788.92, "episode_reward_trend_value": 0.06323979370050666, "biggest_recent_change": 8.062155137861282 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-791.07, "episode_reward_trend_value": 0.12889083239104795, "biggest_recent_change": 5.219821269008321 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-792.25, "episode_reward_trend_value": 0.1394811221193916, "biggest_recent_change": 5.219821269008321 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-793.09, "episode_reward_trend_value": 0.11992933065651716, "biggest_recent_change": 5.219821269008321 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-795.20, "episode_reward_trend_value": 0.059939024240372875, "biggest_recent_change": 5.219821269008321 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-798.90, "episode_reward_trend_value": 0.01956488224465147, "biggest_recent_change": 5.219821269008321 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-800.53, "episode_reward_trend_value": -0.05658368581325198, "biggest_recent_change": 3.692654216120559 },
Hit early stopping because per_episode_reward: -800.5306189386504 < -800




Process Process-247:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-241:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:26:17,533][0m Trial 24 finished with value: -798.6992634417444 and parameters: {'learning_rate': 0, 'variance_scaling_factor': 3, 'permaban_threshold': 4}. Best is trial 20 with value: -706.5661819546524.[0m








Process Process-254:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
Process Process-252:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-255:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-256:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-779.33, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-772.25, "episode_reward_trend_value": 0.7074412695011005, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-784.84, "episode_reward_trend_value": -0.2756622258566779, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-789.61, "episode_reward_trend_value": -0.34270842077132785, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-797.65, "episode_reward_trend_value": -0.458158544257455, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-784.82, "episode_reward_trend_value": -0.10979980977093873, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-795.97, "episode_reward_trend_value": -0.27736166483973645, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-795.35, "episode_reward_trend_value": -0.22884752300500394, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-797.24, "episode_reward_trend_value": -0.22386608432137506, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-800.67, "episode_reward_trend_value": -0.237095394376777, "biggest_recent_change": 12.836351281751263 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-803.14, "episode_reward_trend_value": -0.34317645293888166, "biggest_recent_change": 12.836351281751263 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-802.99, "episode_reward_trend_value": -0.20166620508404853, "biggest_recent_change": 12.836351281751263 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-799.71, "episode_reward_trend_value": -0.11227512663057015, "biggest_recent_change": 12.836351281751263 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-802.42, "episode_reward_trend_value": -0.05297337114064299, "biggest_recent_change": 12.836351281751263 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-800.71, "episode_reward_trend_value": -0.17657968460641138, "biggest_recent_change": 11.151709401837252 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-802.23, "episode_reward_trend_value": -0.06950669051412382, "biggest_recent_change": 3.429298748199926 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-797.88, "episode_reward_trend_value": -0.02811633550257208, "biggest_recent_change": 4.347505231073569 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-797.44, "episode_reward_trend_value": -0.0022991705037838984, "biggest_recent_change": 4.347505231073569 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-796.92, "episode_reward_trend_value": 0.04164782522101025, "biggest_recent_change": 4.347505231073569 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-798.54, "episode_reward_trend_value": 0.05115001725485198, "biggest_recent_change": 4.347505231073569 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-798.50, "episode_reward_trend_value": 0.04991880931061006, "biggest_recent_change": 4.347505231073569 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-799.58, "episode_reward_trend_value": 0.0014940920162555612, "biggest_recent_change": 4.347505231073569 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-801.32, "episode_reward_trend_value": 0.012202211775405026, "biggest_recent_change": 4.347505231073569 },
Hit early stopping because per_episode_reward: -801.323843928473 < -800




Process Process-251:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError




Process Process-260:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 17:26:41,595][0m Trial 25 finished with value: -800.1293978335714 and parameters: {'learning_rate': 1, 'variance_scaling_factor': 2, 'permaban_threshold': 0}. Best is trial 20 with value: -706.5661819546524.[0m
