[32m[I 2022-10-23 16:31:16,817][0m A new study created in memory with name: no-name-2d400d1b-d739-4d58-bf72-2c0da76e00a3[0m





Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-826.33, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-815.04, "episode_reward_trend_value": 1.1293008058872034, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-803.89, "episode_reward_trend_value": 1.1220198479097803, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-791.68, "episode_reward_trend_value": 1.1550711991261893, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-799.64, "episode_reward_trend_value": 0.6671782239225393, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-804.74, "episode_reward_trend_value": 0.43184233194317817, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-801.93, "episode_reward_trend_value": 0.40667373151406994, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-799.49, "episode_reward_trend_value": 0.3834640913271528, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-794.81, "episode_reward_trend_value": 0.39402318732501695, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-793.05, "episode_reward_trend_value": 0.3697641941911204, "biggest_recent_change": 12.21173901559007 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-794.90, "episode_reward_trend_value": 0.2237778054443677, "biggest_recent_change": 12.21173901559007 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-794.78, "episode_reward_trend_value": 0.10119341437094817, "biggest_recent_change": 12.21173901559007 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-794.37, "episode_reward_trend_value": -0.02989297283849055, "biggest_recent_change": 7.965007016884101 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-794.22, "episode_reward_trend_value": 0.06025126038902928, "biggest_recent_change": 5.095012359742668 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-795.12, "episode_reward_trend_value": 0.10691120830071366, "biggest_recent_change": 4.679368593100662 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-794.96, "episode_reward_trend_value": 0.07739251216831286, "biggest_recent_change": 4.679368593100662 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-793.27, "episode_reward_trend_value": 0.0690393512870489, "biggest_recent_change": 4.679368593100662 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-792.56, "episode_reward_trend_value": 0.02498942848238054, "biggest_recent_change": 1.8457669283357063 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-794.84, "episode_reward_trend_value": -0.019834275517716073, "biggest_recent_change": 2.2772108688092203 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-795.64, "episode_reward_trend_value": -0.008246324505586876, "biggest_recent_change": 2.2772108688092203 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-796.44, "episode_reward_trend_value": -0.018440333066536772, "biggest_recent_change": 2.2772108688092203 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-798.80, "episode_reward_trend_value": -0.049213484893262674, "biggest_recent_change": 2.3556194976647475 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-797.18, "episode_reward_trend_value": -0.03290034592840432, "biggest_recent_change": 2.3556194976647475 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-797.59, "episode_reward_trend_value": -0.02745764945681104, "biggest_recent_change": 2.3556194976647475 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-796.46, "episode_reward_trend_value": -0.016589483207141913, "biggest_recent_change": 2.3556194976647475 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-796.60, "episode_reward_trend_value": -0.03697878296621487, "biggest_recent_change": 2.3556194976647475 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-797.60, "episode_reward_trend_value": -0.056007112208816226, "biggest_recent_change": 2.3556194976647475 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-800.70, "episode_reward_trend_value": -0.06519544205437014, "biggest_recent_change": 3.104160554909072 },
Hit early stopping because per_episode_reward: -800.7039754389215 < -800




Process Process-8:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 591, in run_func
    f()
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 560, in f
    train_loop(
  File "/home/jeffhykin/repos/bizav2/pfrl/experiments/train_agent_async.py", line 169, in train_loop
    update_barrier.wait()   # Wait for all agents to contribute their gradients to global model, the sync_updates() to step it's optimizer
  File "/usr/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/usr/lib/python3.8/threading.py", line 659, in _wait
    raise BrokenBarrierError
threading.BrokenBarrierError
[32m[I 2022-10-23 16:31:45,543][0m Trial 0 finished with value: -799.3561887872445 and parameters: {'learning_rate': 3, 'variance_scaling_factor': 5, 'permaban_threshold': 4}. Best is trial 0 with value: -799.3561887872445.[0m
{"total_number_of_episodes":20, "number_of_timesteps":20000, "per_episode_reward":-754.18, "episode_reward_trend_value": 0.0, "biggest_recent_change": nan },
{"total_number_of_episodes":30, "number_of_timesteps":30000, "per_episode_reward":-790.49, "episode_reward_trend_value": -3.630656455800886, "biggest_recent_change": nan },
{"total_number_of_episodes":40, "number_of_timesteps":40000, "per_episode_reward":-789.96, "episode_reward_trend_value": -1.7890600424212892, "biggest_recent_change": nan },
{"total_number_of_episodes":50, "number_of_timesteps":50000, "per_episode_reward":-799.72, "episode_reward_trend_value": -1.5180674227996822, "biggest_recent_change": nan },
{"total_number_of_episodes":60, "number_of_timesteps":60000, "per_episode_reward":-794.41, "episode_reward_trend_value": -1.0057160072456326, "biggest_recent_change": nan },
{"total_number_of_episodes":70, "number_of_timesteps":70000, "per_episode_reward":-799.81, "episode_reward_trend_value": -0.9125752625288579, "biggest_recent_change": nan },
{"total_number_of_episodes":80, "number_of_timesteps":80000, "per_episode_reward":-793.96, "episode_reward_trend_value": -0.6629925748326514, "biggest_recent_change": nan },
{"total_number_of_episodes":90, "number_of_timesteps":90000, "per_episode_reward":-796.67, "episode_reward_trend_value": -0.6070054251247387, "biggest_recent_change": nan },
{"total_number_of_episodes":100, "number_of_timesteps":100000, "per_episode_reward":-792.71, "episode_reward_trend_value": -0.48166455093522276, "biggest_recent_change": nan },
{"total_number_of_episodes":110, "number_of_timesteps":110000, "per_episode_reward":-798.59, "episode_reward_trend_value": -0.493508295835751, "biggest_recent_change": 36.30656455800886 },
{"total_number_of_episodes":120, "number_of_timesteps":120000, "per_episode_reward":-803.31, "episode_reward_trend_value": -0.14252980905841922, "biggest_recent_change": 9.760821835564684 },
{"total_number_of_episodes":130, "number_of_timesteps":130000, "per_episode_reward":-807.98, "episode_reward_trend_value": -0.20023659503319777, "biggest_recent_change": 9.760821835564684 },
{"total_number_of_episodes":140, "number_of_timesteps":140000, "per_episode_reward":-799.11, "episode_reward_trend_value": 0.006765120427345057, "biggest_recent_change": 8.86933255588417 },
{"total_number_of_episodes":150, "number_of_timesteps":150000, "per_episode_reward":-794.11, "episode_reward_trend_value": 0.0032965592498878015, "biggest_recent_change": 8.86933255588417 },
{"total_number_of_episodes":160, "number_of_timesteps":160000, "per_episode_reward":-793.97, "episode_reward_trend_value": 0.06481464064642321, "biggest_recent_change": 8.86933255588417 },
{"total_number_of_episodes":170, "number_of_timesteps":170000, "per_episode_reward":-792.78, "episode_reward_trend_value": 0.013128456612835432, "biggest_recent_change": 8.86933255588417 },
{"total_number_of_episodes":180, "number_of_timesteps":180000, "per_episode_reward":-795.07, "episode_reward_trend_value": 0.017816075993410625, "biggest_recent_change": 8.86933255588417 },
{"total_number_of_episodes":190, "number_of_timesteps":190000, "per_episode_reward":-793.86, "episode_reward_trend_value": -0.012708130080300684, "biggest_recent_change": 8.86933255588417 },
{"total_number_of_episodes":200, "number_of_timesteps":200000, "per_episode_reward":-795.18, "episode_reward_trend_value": 0.037966303538529346, "biggest_recent_change": 8.86933255588417 },
{"total_number_of_episodes":210, "number_of_timesteps":210000, "per_episode_reward":-796.56, "episode_reward_trend_value": 0.07500710144542685, "biggest_recent_change": 8.86933255588417 },
{"total_number_of_episodes":220, "number_of_timesteps":220000, "per_episode_reward":-795.03, "episode_reward_trend_value": 0.14393069542016343, "biggest_recent_change": 8.86933255588417 },
{"total_number_of_episodes":230, "number_of_timesteps":230000, "per_episode_reward":-793.52, "episode_reward_trend_value": 0.06209728847564722, "biggest_recent_change": 5.001211888194007 },
{"total_number_of_episodes":240, "number_of_timesteps":240000, "per_episode_reward":-793.55, "episode_reward_trend_value": 0.006181666740178571, "biggest_recent_change": 2.288939524520856 },
{"total_number_of_episodes":250, "number_of_timesteps":250000, "per_episode_reward":-795.39, "episode_reward_trend_value": -0.015766418913696928, "biggest_recent_change": 2.288939524520856 },
{"total_number_of_episodes":260, "number_of_timesteps":260000, "per_episode_reward":-793.54, "episode_reward_trend_value": -0.008501241696677021, "biggest_recent_change": 2.288939524520856 },
{"total_number_of_episodes":270, "number_of_timesteps":270000, "per_episode_reward":-792.65, "episode_reward_trend_value": 0.026869497563457873, "biggest_recent_change": 1.8513180229927002 },
{"total_number_of_episodes":280, "number_of_timesteps":280000, "per_episode_reward":-792.70, "episode_reward_trend_value": 0.012818358371575616, "biggest_recent_change": 1.8513180229927002 },
{"total_number_of_episodes":290, "number_of_timesteps":290000, "per_episode_reward":-790.91, "episode_reward_trend_value": 0.047365932744494584, "biggest_recent_change": 1.8513180229927002 },
{"total_number_of_episodes":300, "number_of_timesteps":300000, "per_episode_reward":-793.03, "episode_reward_trend_value": 0.03921583091990517, "biggest_recent_change": 2.1183381006412674 },
{"total_number_of_episodes":310, "number_of_timesteps":310000, "per_episode_reward":-792.81, "episode_reward_trend_value": 0.024663841545613095, "biggest_recent_change": 2.1183381006412674 },
{"total_number_of_episodes":320, "number_of_timesteps":320000, "per_episode_reward":-792.25, "episode_reward_trend_value": 0.014175779514130227, "biggest_recent_change": 2.1183381006412674 },
{"total_number_of_episodes":330, "number_of_timesteps":330000, "per_episode_reward":-791.87, "episode_reward_trend_value": 0.018733782846618398, "biggest_recent_change": 2.1183381006412674 },
{"total_number_of_episodes":340, "number_of_timesteps":340000, "per_episode_reward":-791.40, "episode_reward_trend_value": 0.04441950485313934, "biggest_recent_change": 2.1183381006412674 },
{"total_number_of_episodes":350, "number_of_timesteps":350000, "per_episode_reward":-792.76, "episode_reward_trend_value": 0.00870829461450992, "biggest_recent_change": 2.1183381006412674 },
{"total_number_of_episodes":360, "number_of_timesteps":360000, "per_episode_reward":-792.75, "episode_reward_trend_value": -0.001143509822334939, "biggest_recent_change": 2.1183381006412674 },
{"total_number_of_episodes":370, "number_of_timesteps":370000, "per_episode_reward":-792.38, "episode_reward_trend_value": 0.0036069333306916937, "biggest_recent_change": 2.1183381006412674 },
{"total_number_of_episodes":380, "number_of_timesteps":380000, "per_episode_reward":-791.51, "episode_reward_trend_value": -0.006622429789957904, "biggest_recent_change": 2.1183381006412674 },
{"total_number_of_episodes":390, "number_of_timesteps":390000, "per_episode_reward":-791.83, "episode_reward_trend_value": 0.013346535819161748, "biggest_recent_change": 1.3626908984839474 },
{"total_number_of_episodes":400, "number_of_timesteps":400000, "per_episode_reward":-791.04, "episode_reward_trend_value": 0.0196407067901608, "biggest_recent_change": 1.3626908984839474 },
{"total_number_of_episodes":410, "number_of_timesteps":410000, "per_episode_reward":-791.01, "episode_reward_trend_value": 0.013787079588889306, "biggest_recent_change": 1.3626908984839474 },
{"total_number_of_episodes":420, "number_of_timesteps":420000, "per_episode_reward":-790.72, "episode_reward_trend_value": 0.012752329327078213, "biggest_recent_change": 1.3626908984839474 },
{"total_number_of_episodes":430, "number_of_timesteps":430000, "per_episode_reward":-788.53, "episode_reward_trend_value": 0.031853688311245305, "biggest_recent_change": 2.192014069383731 },
{"total_number_of_episodes":440, "number_of_timesteps":440000, "per_episode_reward":-788.71, "episode_reward_trend_value": 0.04497709233332014, "biggest_recent_change": 2.192014069383731 },
